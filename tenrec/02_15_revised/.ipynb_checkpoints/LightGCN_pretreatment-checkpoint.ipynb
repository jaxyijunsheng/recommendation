{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import random\n",
    "import pickle\n",
    "max_lenth=100\n",
    "neg_num=4\n",
    "test_neg_num=100\n",
    "\n",
    "with open(\"D:/datasets/ml/ml-1m/all_ratings.pkl\",'rb') as f:\n",
    "    all_data=pickle.load(f)\n",
    "\n",
    "train_set=[]\n",
    "val_set=[]\n",
    "test_set=[]\n",
    "for u in all_data.keys():\n",
    "    slice_data=all_data[u][0:max_lenth]\n",
    "    \n",
    "    test_data=choice(slice_data)\n",
    "    slice_data.remove(test_data)\n",
    "    \n",
    "    val_data=choice(slice_data)\n",
    "    slice_data.remove(val_data)\n",
    "    \n",
    "    train_data=slice_data\n",
    "    for i in train_data:\n",
    "        train_set.append([u,i])\n",
    "    val_set.append([u,val_data])\n",
    "    test_set.append([u,test_data])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [1, 10], [1, 11], [1, 12], [1, 13], [1, 14], [1, 15], [1, 16], [1, 18], [1, 19], [1, 20], [1, 21], [1, 22], [1, 23], [1, 24], [1, 25], [1, 26], [1, 27], [1, 28], [1, 29], [1, 30], [1, 31], [1, 32], [1, 33], [1, 34], [1, 35], [1, 36], [1, 37], [1, 38], [1, 39], [1, 40], [1, 41], [1, 42], [1, 43], [1, 44], [1, 45], [1, 46], [1, 47], [1, 49], [1, 50], [1, 51], [1, 52], [1, 53], [2, 23], [2, 25], [2, 26], [2, 35], [2, 37], [2, 39], [2, 50], [2, 54], [2, 55], [2, 56], [2, 57], [2, 58], [2, 59], [2, 60], [2, 61], [2, 62], [2, 63], [2, 64], [2, 65], [2, 66], [2, 67], [2, 68], [2, 69], [2, 70], [2, 71], [2, 72], [2, 73], [2, 74], [2, 75], [2, 76], [2, 77], [2, 78], [2, 79], [2, 80], [2, 81], [2, 83], [2, 84], [2, 85], [2, 86], [2, 87], [2, 88], [2, 89], [2, 90], [2, 91], [2, 92], [2, 93], [2, 94], [2, 95], [2, 96]]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 91, 101, 130, 163, 175, 204, 226, 245, 252, 292, 303, 329, 388, 392, 397, 403, 422, 423, 438, 442, 487, 534, 574, 577, 653, 694, 710, 723, 724, 754, 794, 798, 800, 838, 856, 904, 911, 940, 943, 1110, 1139, 1160, 1162, 1216, 1302, 1333, 1337, 1353, 1362, 1373, 1380, 1383, 1389, 1397, 1408, 1412, 1538, 1540, 1594, 1651, 1685, 1686, 1766, 1772, 2061, 2188, 2235, 2236, 2283, 2778, 3165]\n"
     ]
    }
   ],
   "source": [
    "print(all_data[1156])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 3271\n"
     ]
    }
   ],
   "source": [
    "users=[]\n",
    "items=[]\n",
    "for l in train_set:\n",
    "    uid=int(l[0])\n",
    "    item=int(l[1])\n",
    "    users.append(uid)\n",
    "    items.append(item)\n",
    "for l in val_set:\n",
    "    uid=int(l[0])\n",
    "    item=int(l[1])\n",
    "    users.append(uid)\n",
    "    items.append(item)\n",
    "for l in test_set:\n",
    "    uid=int(l[0])\n",
    "    item=int(l[1])\n",
    "    users.append(uid)\n",
    "    items.append(item)\n",
    "users=list(set(users))\n",
    "items=list(set(items))\n",
    "print(len(users),len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_num=max(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "R = sp.dok_matrix((len(users),items_num), dtype=np.float32)\n",
    "for i in train_set:\n",
    "    R[i[0]-1, i[1]-1] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already create adjacency matrix (9456, 9456)\n",
      "already create adjacency matrix (9456, 9456)\n",
      "already create adjacency matrix (9456, 9456)\n",
      "already create adjacency matrix (9456, 9456)\n",
      "already create adjacency matrix (9456, 9456)\n",
      "generate single-normalized adjacency matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junyachen\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in power\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate single-normalized adjacency matrix.\n"
     ]
    }
   ],
   "source": [
    "adj_mat = sp.dok_matrix((len(users)+items_num,len(users)+items_num), dtype=np.float32)\n",
    "adj_mat = adj_mat.tolil()\n",
    "R = R.tolil()\n",
    "for i in range(5):\n",
    "    adj_mat[int(len(users)*i/5.0):int(len(users)*(i+1.0)/5), len(users):] =\\\n",
    "    R[int(len(users)*i/5.0):int(len(users)*(i+1.0)/5)]\n",
    "    adj_mat[len(users):,int(len(users)*i/5.0):int(len(users)*(i+1.0)/5)] =\\\n",
    "    R[int(len(users)*i/5.0):int(len(users)*(i+1.0)/5)].T\n",
    "    adj_mat = adj_mat.todok()\n",
    "    print('already create adjacency matrix', adj_mat.shape)\n",
    "    def normalized_adj_single(adj):\n",
    "        rowsum = np.array(adj.sum(1))\n",
    "        d_inv = np.power(rowsum, -1).flatten()\n",
    "        d_inv[np.isinf(d_inv)] = 0.\n",
    "        d_mat_inv = sp.diags(d_inv)\n",
    "        norm_adj = d_mat_inv.dot(adj)\n",
    "        print('generate single-normalized adjacency matrix.')\n",
    "        return norm_adj.tocoo()\n",
    "    def check_adj_if_equal(adj):\n",
    "        dense_A = np.array(adj.todense())\n",
    "        degree = np.sum(dense_A, axis=1, keepdims=False)\n",
    "        temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
    "        print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n",
    "        return temp\n",
    "        \n",
    "norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "mean_adj_mat = normalized_adj_single(adj_mat)\n",
    "adj_mat, norm_adj_mat, mean_adj_mat=adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n",
    "sp.save_npz('D:/datasets/ml/ml-1m/' + '/ml-1m_s_adj_mat.npz', adj_mat)\n",
    "sp.save_npz('D:/datasets/ml/ml-1m/' + '/ml-1m_s_norm_adj_mat.npz', norm_adj_mat)\n",
    "sp.save_npz('D:/datasets/ml/ml-1m/' + '/ml-1m_s_mean_adj_mat.npz', mean_adj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load adj matrix (9456, 9456)\n"
     ]
    }
   ],
   "source": [
    "adj_mat = sp.load_npz('D:/datasets/ml/ml-1m/' + '/ml-1m_s_adj_mat.npz')\n",
    "norm_adj_mat = sp.load_npz('D:/datasets/ml/ml-1m/' + '/ml-1m_s_norm_adj_mat.npz')\n",
    "mean_adj_mat = sp.load_npz('D:/datasets/ml/ml-1m/' + '/ml-1m_s_mean_adj_mat.npz')\n",
    "print('already load adj matrix', adj_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junyachen\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in power\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate pre adjacency matrix.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pre_adj_mat = sp.load_npz('D:/datasets/ml/ml-1m/' + '/ml-1m_s_pre_adj_mat.npz')\n",
    "except Exception:\n",
    "    adj_mat=adj_mat\n",
    "    rowsum = np.array(adj_mat.sum(1))\n",
    "    d_inv = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "    norm_adj = d_mat_inv.dot(adj_mat)\n",
    "    norm_adj = norm_adj.dot(d_mat_inv)\n",
    "    print('generate pre adjacency matrix.')\n",
    "    pre_adj_mat = norm_adj.tocsr()\n",
    "    sp.save_npz('D:/datasets/ml/ml-1m/' + '/ml-1m_s_pre_adj_mat.npz.npz', norm_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict={}\n",
    "val_dict={}\n",
    "test_dict={}\n",
    "\n",
    "new_train_set=[]\n",
    "new_val_set=[]\n",
    "new_test_set=[]\n",
    "\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(val_set)\n",
    "random.shuffle(test_set)\n",
    "\n",
    "for i in train_set:\n",
    "    neg_item = [random.randint(1, items_num) for _ in range(neg_num)]\n",
    "    new_train_set.append((i[0],i[1],neg_item))\n",
    "\n",
    "for i in val_set:\n",
    "    neg_item = [random.randint(1, items_num) for _ in range(neg_num)]\n",
    "    new_val_set.append((i[0],i[1],neg_item))\n",
    "\n",
    "for i in test_set:\n",
    "    neg_item = [random.randint(1, items_num) for _ in range(test_neg_num)]\n",
    "    new_test_set.append((i[0],i[1],neg_item))\n",
    "    \n",
    "users, pos_items,neg_items = zip(*new_train_set)\n",
    "train_dict = {'users': np.array(users), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\n",
    "\n",
    "users, pos_items,neg_items = zip(*new_val_set)\n",
    "val_dict = {'users': np.array(users), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\n",
    "\n",
    "users, pos_items,neg_items = zip(*new_test_set)\n",
    "test_dict = {'users': np.array(users), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/datasets/ml/ml-1m/ml-1m_gcn_train_data.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dict, f, -1)\n",
    "with open('D:/datasets/ml/ml-1m/ml-1m_gcn_test_data.pkl', 'wb') as f:\n",
    "    pickle.dump(val_dict, f, -1)\n",
    "with open('D:/datasets/ml/ml-1m/ml-1m_gcn_val_data.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dict, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435214 6040 6040 3416\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set),len(test_set),len(val_set),items_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
