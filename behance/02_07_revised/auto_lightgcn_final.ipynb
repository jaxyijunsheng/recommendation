{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd98390-6cf1-4188-8828-d9bbbeea1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#os.environ['AUTOGRAPH_VERBOSITY'] = '0'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c3b8cca-95eb-4e76-af74-8467884552ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a9ec17-4926-453d-8ca9-9113908b350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/root/autodl-nas/workspace/datasets/ml/ml-1m/all_ratings.pkl', 'rb') as f:    \n",
    "    user_dict = pickle.load(f)\n",
    "    \n",
    "items_num=3416\n",
    "users_num=6040\n",
    "\n",
    "maxlen=100\n",
    "len_seq=50\n",
    "batch_size=512\n",
    "epoch_num=100\n",
    "hidden_size=64\n",
    "keep_rate=0.9\n",
    "layers_num=10\n",
    "interest_num=3\n",
    "neg_num=4\n",
    "test_neg_num=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91fd2c73-3e4f-449e-aef3-88288de30804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def sample(user_dict,maxlen,len_seq):\n",
    "    train_set=[]\n",
    "    train_val_set=[]\n",
    "    val_set=[]\n",
    "    test_set=[]\n",
    "    for u in user_dict.keys():\n",
    "        idx=0\n",
    "        hist=user_dict[u]        \n",
    "        hist=hist[-maxlen:-2]\n",
    "        #print(hist)\n",
    "        for i in range(1,len(hist)):\n",
    "            seq = np.zeros([len_seq], dtype=np.int32)\n",
    "            #print(hist[0:i])\n",
    "            seq[max(0,len_seq+idx-1):]=hist[max(0,i-len_seq):i]\n",
    "            idx+=-1\n",
    "            nxt = hist[i]\n",
    "            #print((u,seq,nxt))\n",
    "            train_set.append((u,list(seq),nxt))\n",
    "            #print(seq)\n",
    "        train_val_set.append((u,list(seq),nxt))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-2:-2]\n",
    "        nxt = user_dict[u][-2]\n",
    "        val_set.append((u,list(seq),nxt))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-1:-1]\n",
    "        nxt = user_dict[u][-1]\n",
    "        test_set.append((u,list(seq),nxt))\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    return train_set,test_set,val_set,train_val_set\n",
    "            \n",
    "def non_zero_sample(user_dict,maxlen,len_seq):\n",
    "    train_set=[]\n",
    "    test_set=[]\n",
    "    val_set=[]\n",
    "    train_val_set=[]\n",
    "    for u in user_dict.keys():\n",
    "        idx=0\n",
    "        hist=user_dict[u]        \n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        #print(np.shape(seq[-maxlen:]))\n",
    "        while(len(hist)<maxlen):\n",
    "            hist.insert(0,0)\n",
    "        #print(len(hist),hist)\n",
    "        hist=hist[-maxlen:-2]\n",
    "        for i in range(len_seq,len(hist)):\n",
    "            seq = np.zeros([len_seq], dtype=np.int32)\n",
    "            #rint(hist[max(0,i-len_seq):i])\n",
    "            seq=hist[max(0,i-len_seq):i]\n",
    "            idx+=-1\n",
    "            nxt = hist[i]\n",
    "            #print((u,seq,nxt))\n",
    "            neg_item = [random.randint(1, items_num) for _ in range(neg_num)]\n",
    "            train_set.append((u,list(seq),nxt,neg_item))\n",
    "            #print(seq)\n",
    "        #print(np.shape(hist[0:len(hist)-1]))\n",
    "        train_val_set.append((u,list(seq),nxt))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-2:-2]\n",
    "        nxt = user_dict[u][-2]\n",
    "        neg_item = [random.randint(1, items_num) for _ in range(neg_num)]\n",
    "        val_set.append((u,list(seq),nxt,neg_item))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-1:-1]\n",
    "        nxt = user_dict[u][-1]\n",
    "        neg_item = [random.randint(1, items_num) for _ in range(test_neg_num)]\n",
    "        test_set.append((u,list(seq),nxt,neg_item))\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    return train_set,test_set ,val_set,train_val_set   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "904dd44f-cae8-4e03-a8f7-a76fbffae7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_set,test_set,val_set,train_val_set=non_zero_sample(user_dict,maxlen,len_seq)\\nprint(len(train_set),len(test_set),len(val_set),len(train_val_set))\\nusers_num=len(test_set)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_set,test_set,val_set,train_val_set=non_zero_sample(user_dict,maxlen,len_seq)\n",
    "print(len(train_set),len(test_set),len(val_set),len(train_val_set))\n",
    "users_num=len(test_set)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881ce4c1-a8b0-4c59-98db-5c6df6ae46a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"users, click_seqs, pos_items,neg_items = zip(*train_set)\\ntrain_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\\nusers, click_seqs, pos_items,neg_items = zip(*test_set)\\ntest_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''users, click_seqs, pos_items,neg_items = zip(*train_set)\n",
    "train_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\n",
    "users, click_seqs, pos_items,neg_items = zip(*test_set)\n",
    "test_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f884a5a-ab90-4a53-a85e-d7cbc2b1262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/autodl-nas/workspace/datasets/ml/ml-1m/gcn/ml-1m_gcn_train_data.pkl', 'rb') as f:\n",
    "    train_data=pickle.load(f)\n",
    "with open('/root/autodl-nas/workspace/datasets/ml/ml-1m/gcn/ml-1m_gcn_test_data.pkl', 'rb') as f:\n",
    "    test_data=pickle.load(f)\n",
    "with open('/root/autodl-nas/workspace/datasets/ml/ml-1m/gcn/ml-1m_gcn_val_data.pkl', 'rb') as f:\n",
    "    val_data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32879739-778f-4a8b-aabe-f2b701d2253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(435214, 4) (6040, 100) (6040, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_data['neg_item']),np.shape(test_data['neg_item']),np.shape(val_data['neg_item']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e8a5b5a-0fa3-40c3-913e-94da35381eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for i in range(len(train_data['click_seq'])):\\n    if i%500000==0:\\n        print(i)\\n    if list(test_data['click_seq'][0])==list(train_data['click_seq'][i]):\\n        print(test_data['click_seq'][0],test_data['pos_item'][0],train_data['click_seq'][i],train_data['pos_item'][i])\\nfor i in range(len(train_data['click_seq'])):\\n    if i%500000==0:\\n        print(i)\\n    if list(val_data['click_seq'][0])==list(train_data['click_seq'][i]):\\n        print(val_data['click_seq'][0],val_data['pos_item'][0],train_data['click_seq'][i],train_data['pos_item'][i])\\nprint('pass')\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in range(len(train_data['click_seq'])):\n",
    "    if i%500000==0:\n",
    "        print(i)\n",
    "    if list(test_data['click_seq'][0])==list(train_data['click_seq'][i]):\n",
    "        print(test_data['click_seq'][0],test_data['pos_item'][0],train_data['click_seq'][i],train_data['pos_item'][i])\n",
    "for i in range(len(train_data['click_seq'])):\n",
    "    if i%500000==0:\n",
    "        print(i)\n",
    "    if list(val_data['click_seq'][0])==list(train_data['click_seq'][i]):\n",
    "        print(val_data['click_seq'][0],val_data['pos_item'][0],train_data['click_seq'][i],train_data['pos_item'][i])\n",
    "print('pass')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df82d54-78b8-48b8-84b9-ad4896e2236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Attention Mechanism Function.\n",
    "    Args:\n",
    "        :param q: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param k: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param v: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param mask: A 3d/4d tensor with shape of (None, ..., seq_len, 1)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mat_qk = tf.matmul(q, k, transpose_b=True)  # (None, seq_len, seq_len)\n",
    "    # Scaled\n",
    "    dk = tf.cast(k.shape[-1], dtype=tf.float32)\n",
    "    scaled_att_logits = mat_qk / tf.sqrt(dk)\n",
    "\n",
    "    paddings = tf.ones_like(scaled_att_logits) * (-2 ** 32 + 1)  # (None, seq_len, seq_len)\n",
    "    if mask!=None:\n",
    "        outputs = tf.where(tf.equal(mask, tf.zeros_like(mask)), paddings, scaled_att_logits)  # (None, seq_len, seq_len)\n",
    "    else:\n",
    "        outputs=scaled_att_logits\n",
    "    # softmax\n",
    "    outputs = tf.nn.softmax(logits=outputs)  # (None, seq_len, seq_len)\n",
    "    outputs = tf.matmul(outputs, v)  # (None, seq_len, dim)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def split_heads(x, seq_len, num_heads, depth):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    Args:\n",
    "        :param x: A Tensor with shape of [batch_size, seq_len, num_heads * depth]\n",
    "        :param seq_len: A scalar(int).\n",
    "        :param num_heads: A scalar(int).\n",
    "        :param depth: A scalar(int).\n",
    "    :return: A tensor with shape of [batch_size, num_heads, seq_len, depth]\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (-1, seq_len, num_heads, depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1382056-672c-4683-a46a-dca40cd582b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"Multi Head Attention Mechanism.\n",
    "        Args:\n",
    "            :param d_model: A scalar. The self-attention hidden size.\n",
    "            :param num_heads: A scalar. Number of heads. If num_heads == 1, the layer is a single self-attention layer.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model, activation=None)\n",
    "        self.wk = tf.keras.layers.Dense(d_model, activation=None)\n",
    "        self.wv = tf.keras.layers.Dense(d_model, activation=None)\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "        q = self.wq(q)  # (None, seq_len, d_model)\n",
    "        k = self.wk(k)  # (None, seq_len, d_model)\n",
    "        v = self.wv(v)  # (None, seq_len, d_model)\n",
    "        # split d_model into num_heads * depth\n",
    "        seq_len, d_model = q.shape[1], q.shape[2]\n",
    "        q = split_heads(q, seq_len, self.num_heads, q.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        k = split_heads(k, seq_len, self.num_heads, k.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        v = split_heads(v, seq_len, self.num_heads, v.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        # mask\n",
    "        if mask!=None:\n",
    "            mask = tf.tile(tf.expand_dims(mask, axis=1), [1, self.num_heads, 1, 1])  # (None, num_heads, seq_len, 1)\n",
    "        # attention\n",
    "        scaled_attention = scaled_dot_product_attention(q, k, v, mask)  # (None, num_heads, seq_len, d_model // num_heads)\n",
    "        # reshape\n",
    "        outputs = tf.reshape(tf.transpose(scaled_attention, [0, 2, 1, 3]), [-1, seq_len, d_model])  # (None, seq_len, d_model)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class FFN(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_unit, d_model):\n",
    "        \"\"\"Feed Forward Network.\n",
    "        Args:\n",
    "            :param hidden_unit: A scalar.\n",
    "            :param d_model: A scalar.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(FFN, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(filters=hidden_unit, kernel_size=1, activation='relu', use_bias=True)\n",
    "        self.conv2 = tf.keras.layers.Conv1D(filters=d_model, kernel_size=1, activation=None, use_bias=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        output = self.conv2(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads=1, ffn_hidden_unit=128, dropout=0., layer_norm_eps=1e-6):\n",
    "        \"\"\"Encoder Layer.\n",
    "        Args:\n",
    "            :param d_model: A scalar. The self-attention hidden size.\n",
    "            :param num_heads: A scalar. Number of heads.\n",
    "            :param ffn_hidden_unit: A scalar. Number of hidden unit in FFN\n",
    "            :param dropout: A scalar. Number of dropout.\n",
    "            :param layer_norm_eps: A scalar. Small float added to variance to avoid dividing by zero.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FFN(ffn_hidden_unit, d_model)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, mask= inputs\n",
    "        # self-attention\n",
    "        att_out = self.mha(x, x, x, mask)  # (None, seq_len, d_model)\n",
    "        att_out = self.dropout1(att_out)\n",
    "        # residual add\n",
    "        out1 = self.layernorm1(x + att_out)  # (None, seq_len, d_model)\n",
    "        # ffn\n",
    "        ffn_out = self.ffn(out1)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        # residual add\n",
    "        out2 = self.layernorm2(out1 + ffn_out)  # (None, seq_len, d_model)\n",
    "        return out2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79f08c80-c250-4a31-b567-684105020b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load adj matrix (9456, 9456)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "adj_mat = sp.load_npz('/root/autodl-nas/workspace/datasets/ml/ml-1m/gcn/' + '/ml-1m_s_adj_mat.npz')\n",
    "norm_adj_mat = sp.load_npz('/root/autodl-nas/workspace/datasets/ml/ml-1m/gcn/' + '/ml-1m_s_norm_adj_mat.npz')\n",
    "mean_adj_mat = sp.load_npz('/root/autodl-nas/workspace/datasets/ml/ml-1m/gcn/' + '/ml-1m_s_mean_adj_mat.npz')\n",
    "print('already load adj matrix', adj_mat.shape)\n",
    "try:\n",
    "    pre_adj_mat = sp.load_npz('/root/autodl-nas/workspace/datasets/ml/ml-1m/gcn/' + '/ml-1m_s_pre_adj_mat.npz')\n",
    "except Exception:\n",
    "    adj_mat=adj_mat\n",
    "    rowsum = np.array(adj_mat.sum(1))\n",
    "    d_inv = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "    norm_adj = d_mat_inv.dot(adj_mat)\n",
    "    norm_adj = norm_adj.dot(d_mat_inv)\n",
    "    print('generate pre adjacency matrix.')\n",
    "    pre_adj_mat = norm_adj.tocsr()\n",
    "    sp.save_npz('/root/autodl-nas/workspace/datasets/ml/ml-1m/gcn/' + '/ml-1m_s_pre_adj_mat.npz', norm_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4205de4c-9a0e-4893-ba33-e07afb13e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_adj=norm_adj_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc9876f8-60e2-412d-913a-28a1305fa9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lightgcn_layer(tf.keras.layers.Layer):\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"Convert a scipy sparse matrix to tf.SparseTensor.\n",
    "        Returns:\n",
    "            tf.SparseTensor: SparseTensor after conversion.\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "    \n",
    "    def __init__(self,hidden_size=64, dropout=0., embed_reg=1e-6):\n",
    "        super(lightgcn_layer, self).__init__()\n",
    "        self.norm_adj=norm_adj\n",
    "        self.item_embedding_pool = tf.keras.layers.Embedding(items_num,hidden_size,input_length=items_num,\n",
    "                                                       embeddings_initializer='random_normal',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n",
    "        self.user_embedding_pool = tf.keras.layers.Embedding(users_num,hidden_size,input_length=users_num,\n",
    "                                                       embeddings_initializer='random_normal',\n",
    "                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n",
    "    def call(self, inputs):\n",
    "        self.item_embedding=self.item_embedding_pool(tf.range(items_num))\n",
    "        self.user_embedding=self.user_embedding_pool(tf.range(users_num))\n",
    "        \n",
    "        A_hat = self._convert_sp_mat_to_sp_tensor(self.norm_adj)\n",
    "        \n",
    "        ego_embeddings = tf.concat(\n",
    "            [self.item_embedding, self.user_embedding], axis=0\n",
    "        )\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(0, layers_num):\n",
    "            ego_embeddings = tf.sparse.sparse_dense_matmul(A_hat, ego_embeddings)\n",
    "            all_embeddings += [ego_embeddings]\n",
    "\n",
    "        all_embeddings = tf.stack(all_embeddings, 1)\n",
    "        all_embeddings = tf.reduce_mean(\n",
    "            input_tensor=all_embeddings, axis=1, keepdims=False\n",
    "        )\n",
    "        \n",
    "        self.ua_embeddings, self.ia_embeddings = tf.split(\n",
    "            all_embeddings, [items_num, users_num], 0\n",
    "        )\n",
    "        u_g_embeddings = tf.gather(\n",
    "            params=self.ua_embeddings, indices=tf.reshape(inputs['users']-1,[1,-1])\n",
    "        )\n",
    "        pos_i_g_embeddings = tf.gather(\n",
    "            params=self.ia_embeddings, indices=tf.reshape(inputs['pos_item']-1,[1,-1])\n",
    "        )\n",
    "        neg_i_g_embeddings = tf.gather(\n",
    "            params=self.ia_embeddings, indices=tf.reshape(inputs['neg_item']-1,[tf.shape(inputs['neg_item'])[1],-1])\n",
    "        )\n",
    "        return u_g_embeddings,pos_i_g_embeddings,neg_i_g_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49fb9d25-6709-4545-9211-898760d749eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lightgcn(tf.keras.models.Model):\n",
    "    \n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"Convert a scipy sparse matrix to tf.SparseTensor.\n",
    "        Returns:\n",
    "            tf.SparseTensor: SparseTensor after conversion.\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(lightgcn, self).__init__()\n",
    "        blocks=1\n",
    "        embed_reg=0.\n",
    "        layer_norm_eps=1e-6\n",
    "        num_heads=1\n",
    "        use_l2norm=False\n",
    "        \n",
    "        self.norm_adj=norm_adj\n",
    "        self.len_seq=len_seq\n",
    "        '''\n",
    "        self.item_embedding = tf.Variable(initial_value= tf.random.normal([items_num, hidden_size], stddev=0.35),trainable=True,\n",
    "                      name=\"self.item_embedding\")\n",
    "        self.user_embedding = tf.Variable(initial_value= tf.random.normal([users_num, hidden_size], stddev=0.35),trainable=True,\n",
    "                      name=\"self.user_embedding\")\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        self.item_embedding_pool = tf.keras.layers.Embedding(items_num,hidden_size,input_length=items_num,\n",
    "                                                       embeddings_initializer='random_normal',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n",
    "        self.user_embedding_pool = tf.keras.layers.Embedding(users_num,hidden_size,input_length=users_num,\n",
    "                                                       embeddings_initializer='random_normal',\n",
    "                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n",
    "        \n",
    "        self.item_embedding=self.item_embedding_pool(tf.range(items_num))\n",
    "        self.user_embedding=self.user_embedding_pool(tf.range(users_num))\n",
    "        \n",
    "        A_hat = self._convert_sp_mat_to_sp_tensor(self.norm_adj)\n",
    "        \n",
    "        ego_embeddings = tf.concat(\n",
    "            [self.item_embedding, self.user_embedding], axis=0\n",
    "        )\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(0, layers_num):\n",
    "            ego_embeddings = tf.sparse.sparse_dense_matmul(A_hat, ego_embeddings)\n",
    "            all_embeddings += [ego_embeddings]\n",
    "\n",
    "        all_embeddings = tf.stack(all_embeddings, 1)\n",
    "        all_embeddings = tf.reduce_mean(\n",
    "            input_tensor=all_embeddings, axis=1, keepdims=False\n",
    "        )\n",
    "        \n",
    "        self.ua_embeddings, self.ia_embeddings = tf.split(\n",
    "            all_embeddings, [items_num, users_num], 0\n",
    "        )\n",
    "     \n",
    "        self.dropout = tf.keras.layers.Dropout(1-keep_rate)\n",
    "        self.encoder_layer = [TransformerEncoder(hidden_size, num_heads, 128,\n",
    "                                                 (1-keep_rate), layer_norm_eps) for _ in range(blocks)]\n",
    "        # norm\n",
    "        self.use_l2norm = use_l2norm'''\n",
    "        self.lg_layer=lightgcn_layer(64,0,0.)\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        '''\n",
    "        u_g_embeddings = tf.gather(\n",
    "            params=self.ua_embeddings, indices=tf.reshape(inputs['users']-1,[1,-1])\n",
    "        )\n",
    "        pos_i_g_embeddings = tf.gather(\n",
    "            params=self.ia_embeddings, indices=tf.reshape(inputs['pos_item']-1,[1,-1])\n",
    "        )\n",
    "        neg_i_g_embeddings = tf.gather(\n",
    "            params=self.ia_embeddings, indices=tf.reshape(inputs['neg_item']-1,[tf.shape(inputs['neg_item'])[1],-1])\n",
    "        )'''\n",
    "        u_g_embeddings,pos_i_g_embeddings,neg_i_g_embeddings=self.lg_layer(inputs)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.u_g_embeddings_pre = tf.nn.embedding_lookup(\n",
    "            params=self.weights[\"user_embedding\"], ids=self.users\n",
    "        )\n",
    "        self.pos_i_g_embeddings_pre = tf.nn.embedding_lookup(\n",
    "            params=self.weights[\"item_embedding\"], ids=self.pos_items\n",
    "        )\n",
    "        self.neg_i_g_embeddings_pre = tf.nn.embedding_lookup(\n",
    "            params=self.weights[\"item_embedding\"], ids=self.neg_items\n",
    "        )'''\n",
    "\n",
    "        pos_score = tf.transpose(tf.reduce_sum(tf.multiply(u_g_embeddings, pos_i_g_embeddings), axis=-1),[1,0])  # (None, 1)\n",
    "        neg_score = tf.transpose(tf.reduce_sum(tf.multiply(u_g_embeddings, neg_i_g_embeddings), axis=-1),[1,0])\n",
    "\n",
    "        logits = tf.nn.softmax(tf.concat([pos_score, neg_score], axis=-1))\n",
    "\n",
    "        loss = tf.reduce_mean(- tf.math.log(logits[:,0]))\n",
    "        \n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def summary(self):\n",
    "        inputs = {\n",
    "            'users': tf.keras.layers.Input(shape=(), dtype=tf.int32),\n",
    "            'pos_item': tf.keras.layers.Input(shape=(), dtype=tf.int32),\n",
    "            'neg_item': tf.keras.layers.Input(shape=(1,), dtype=tf.int32)  # suppose neg_num=1\n",
    "        }\n",
    "        tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab39a04b-df32-46da-93f0-7f2749da22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr(rank, k):\n",
    "    \"\"\"Hit Rate.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: hit rate.\n",
    "    \"\"\"\n",
    "    res = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            res += 1\n",
    "    return res / len(rank)\n",
    "\n",
    "\n",
    "def mrr(rank, k):\n",
    "    \"\"\"Mean Reciprocal Rank.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: mrr.\n",
    "    \"\"\"\n",
    "    mrr = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            mrr += 1 / (r + 1)\n",
    "    return mrr / len(rank)\n",
    "\n",
    "\n",
    "def ndcg(rank, k):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: ndcg.\n",
    "    \"\"\"\n",
    "    res = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            res += 1 / np.log2(r + 2)\n",
    "    return res / len(rank)\n",
    "\n",
    "\n",
    "def eval_rank(pred_y, metric_names, k=10):\n",
    "    \"\"\"Evaluate\n",
    "        Args:\n",
    "            :param pred_y: A ndarray.\n",
    "            :param metric_names: A list like ['hr'].\n",
    "            :param k: A scalar(int).\n",
    "        :return: A result dict such as {'hr':, 'ndcg':, ...}\n",
    "    \"\"\"\n",
    "    rank = pred_y.argsort().argsort()[:, 0]\n",
    "    res_dict = {}\n",
    "    for name in metric_names:\n",
    "        if name == 'hr':\n",
    "            res = hr(rank, k)\n",
    "        elif name == 'ndcg':\n",
    "            res = ndcg(rank, k)\n",
    "        elif name == 'mrr':\n",
    "            res = mrr(rank, k)\n",
    "        else:\n",
    "            break\n",
    "        res_dict[name] = res\n",
    "    return res_dict\n",
    "\n",
    "def eval_pos_neg(model, test_data, metric_names, k=10, batch_size=None):\n",
    "    \"\"\"Evaluate the performance of Top-k recommendation algorithm.\n",
    "    Note: Test data must contain some negative samples(>= k) and one positive samples.\n",
    "    Args:\n",
    "        :param model: A model built-by tensorflow.\n",
    "        :param test_data: A dict.\n",
    "        :param metric_names: A list like ['hr'].\n",
    "        :param k: A scalar(int).\n",
    "        :param batch_size: A scalar(int).\n",
    "    :return: A result dict such as {'hr':, 'ndcg':, ...}\n",
    "    \"\"\"\n",
    "    pred_y = - model.predict(test_data, batch_size)\n",
    "    print(np.shape(pred_y))\n",
    "    return eval_rank(pred_y, metric_names, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79563c16-f603-434c-8279-d817b8e42ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d0933a7-b926-4371-a008-0e992c04d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lightgcn_layer (lightgcn_layer) ((1, None, 64), (1,  605184      input_3[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (1, None, 64)        0           lightgcn_layer[0][0]             \n",
      "                                                                 lightgcn_layer[0][1]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, None, 64)     0           lightgcn_layer[0][0]             \n",
      "                                                                 lightgcn_layer[0][2]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum (TFOpLambda) (1, None)            0           tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_1 (TFOpLambd (None, None)         0           tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose (TFOpLam (None, 1)            0           tf.math.reduce_sum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_1 (TFOpL (None, None)         0           tf.math.reduce_sum_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, None)         0           tf.compat.v1.transpose[0][0]     \n",
      "                                                                 tf.compat.v1.transpose_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.softmax (TFOpLambda)      (None, None)         0           tf.concat[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 605,184\n",
      "Trainable params: 605,184\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 2 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 2 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "426/426 [==============================] - ETA: 0s - loss: 1.5254INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "426/426 [==============================] - 15s 23ms/step - loss: 1.5254 - val_loss: 1.3953\n",
      "(6040, 5)\n",
      "Iteration 1 Fit [15.1 s], Evaluate [2.0 s]: HR = 0.7654, MRR = 0.7654, NDCG = 0.7654\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.2892 - val_loss: 1.2545\n",
      "(6040, 5)\n",
      "Iteration 2 Fit [8.0 s], Evaluate [0.2 s]: HR = 0.7942, MRR = 0.7942, NDCG = 0.7942\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.2012 - val_loss: 1.2117\n",
      "(6040, 5)\n",
      "Iteration 3 Fit [7.8 s], Evaluate [0.2 s]: HR = 0.8053, MRR = 0.8053, NDCG = 0.8053\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.1673 - val_loss: 1.1912\n",
      "(6040, 5)\n",
      "Iteration 4 Fit [7.9 s], Evaluate [0.2 s]: HR = 0.8154, MRR = 0.8154, NDCG = 0.8154\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.1476 - val_loss: 1.1789\n",
      "(6040, 5)\n",
      "Iteration 5 Fit [7.8 s], Evaluate [0.2 s]: HR = 0.8194, MRR = 0.8194, NDCG = 0.8194\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.1354 - val_loss: 1.1703\n",
      "(6040, 5)\n",
      "Iteration 6 Fit [7.8 s], Evaluate [0.2 s]: HR = 0.8232, MRR = 0.8232, NDCG = 0.8232\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.1294 - val_loss: 1.1639\n",
      "(6040, 5)\n",
      "Iteration 7 Fit [7.8 s], Evaluate [0.2 s]: HR = 0.8253, MRR = 0.8253, NDCG = 0.8253\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.1222 - val_loss: 1.1589\n",
      "(6040, 5)\n",
      "Iteration 8 Fit [7.8 s], Evaluate [0.2 s]: HR = 0.8257, MRR = 0.8257, NDCG = 0.8257\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.1164 - val_loss: 1.1550\n",
      "(6040, 5)\n",
      "Iteration 9 Fit [7.7 s], Evaluate [0.2 s]: HR = 0.8278, MRR = 0.8278, NDCG = 0.8278\n",
      "426/426 [==============================] - 7s 18ms/step - loss: 1.1140 - val_loss: 1.1513\n",
      "(6040, 5)\n",
      "Iteration 10 Fit [7.7 s], Evaluate [0.2 s]: HR = 0.8281, MRR = 0.8281, NDCG = 0.8281\n",
      "426/426 [==============================] - 7s 17ms/step - loss: 1.1100 - val_loss: 1.1483\n",
      "(6040, 5)\n",
      "Iteration 11 Fit [7.6 s], Evaluate [0.2 s]: HR = 0.8296, MRR = 0.8296, NDCG = 0.8296\n",
      "426/426 [==============================] - 7s 17ms/step - loss: 1.1077 - val_loss: 1.1462\n",
      "(6040, 5)\n",
      "Iteration 12 Fit [7.6 s], Evaluate [0.2 s]: HR = 0.8296, MRR = 0.8296, NDCG = 0.8296\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.1040 - val_loss: 1.1447\n",
      "(6040, 5)\n",
      "Iteration 13 Fit [7.8 s], Evaluate [0.2 s]: HR = 0.8301, MRR = 0.8301, NDCG = 0.8301\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.1021 - val_loss: 1.1428\n",
      "(6040, 5)\n",
      "Iteration 14 Fit [7.7 s], Evaluate [0.2 s]: HR = 0.8316, MRR = 0.8316, NDCG = 0.8316\n",
      "426/426 [==============================] - 7s 18ms/step - loss: 1.0994 - val_loss: 1.1413\n",
      "(6040, 5)\n",
      "Iteration 15 Fit [7.7 s], Evaluate [0.2 s]: HR = 0.8321, MRR = 0.8321, NDCG = 0.8321\n",
      "426/426 [==============================] - 7s 17ms/step - loss: 1.0990 - val_loss: 1.1397\n",
      "(6040, 5)\n",
      "Iteration 16 Fit [7.7 s], Evaluate [0.2 s]: HR = 0.8310, MRR = 0.8310, NDCG = 0.8310\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.0966 - val_loss: 1.1386\n",
      "(6040, 5)\n",
      "Iteration 17 Fit [7.7 s], Evaluate [0.2 s]: HR = 0.8318, MRR = 0.8318, NDCG = 0.8318\n",
      "426/426 [==============================] - 8s 18ms/step - loss: 1.0960 - val_loss: 1.1375\n",
      "(6040, 5)\n",
      "Iteration 18 Fit [7.7 s], Evaluate [0.2 s]: HR = 0.8316, MRR = 0.8316, NDCG = 0.8316\n",
      "Trigger early stopping\n",
      "(6040, 101)\n",
      "Iteration 18 Fit [7.7 s], Evaluate [2.3 s]: HR = 0.7954, MRR = 0.5885, NDCG = 0.6370\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "batch_size=batch_size*strategy.num_replicas_in_sync\n",
    "with strategy.scope():\n",
    "    model = lightgcn()\n",
    "    model.summary()\n",
    "    optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    model.compile(optimizer=optimizer)\n",
    "best_eval_dict={'hr':0.,'mrr':0.,'ndcg':0.}\n",
    "eval_patience={'hr':0.,'mrr':0.,'ndcg':0.}\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    t1 = time()\n",
    "    model.fit(\n",
    "            x=train_data,\n",
    "            epochs=1,\n",
    "            batch_size=batch_size,\n",
    "        validation_data=val_data)\n",
    "    t2 = time()\n",
    "    eval_dict = eval_pos_neg(model, val_data, ['hr', 'mrr', 'ndcg'], 1, batch_size)\n",
    "    print('Iteration %d Fit [%.1f s], Evaluate [%.1f s]: HR = %.4f, MRR = %.4f, NDCG = %.4f'\n",
    "              % (epoch, t2 - t1, time() - t2, eval_dict['hr'], eval_dict['mrr'], eval_dict['ndcg']))\n",
    "    for i in eval_dict.keys():\n",
    "        if eval_dict[i]>=best_eval_dict[i]:\n",
    "            best_eval_dict[i]=eval_dict[i]\n",
    "        else:\n",
    "            eval_patience[i]+=1\n",
    "    if (np.array(list(eval_patience.values()))>=3).any():\n",
    "        print('Trigger early stopping')\n",
    "        break\n",
    "eval_dict = eval_pos_neg(model, test_data, ['hr', 'mrr', 'ndcg'], 10, batch_size)\n",
    "print('Iteration %d Fit [%.1f s], Evaluate [%.1f s]: HR = %.4f, MRR = %.4f, NDCG = %.4f'\n",
    "              % (epoch, t2 - t1, time() - t2, eval_dict['hr'], eval_dict['mrr'], eval_dict['ndcg']))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79481e70-93d3-41d1-8e5b-558125f5da11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train_model(model,ds_train,ds_valid,epoches):\\n\\n    for epoch in tf.range(1,epoches+1):\\n        model.reset_metrics()\\n        \\n        # 在后期降低学习率\\n        if epoch == 5000:\\n            model.optimizer.lr.assign(model.optimizer.lr/2.0)\\n            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\\n        \\n        for x, y in ds_train:\\n            train_result = model.train_on_batch(x, y)\\n\\n        for x, y in ds_valid:\\n            valid_result = model.test_on_batch(x, y,reset_metrics=False)\\n            \\n        if epoch%100 ==0:\\n            tf.print(\"epoch = \",epoch)\\n            print(\"train:\",dict(zip(model.metrics_names,train_result)))\\n            print(\"valid:\",dict(zip(model.metrics_names,valid_result)))\\n            print(\"\")'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def train_model(model,ds_train,ds_valid,epoches):\n",
    "\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        model.reset_metrics()\n",
    "        \n",
    "        # 在后期降低学习率\n",
    "        if epoch == 5000:\n",
    "            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n",
    "            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n",
    "        \n",
    "        for x, y in ds_train:\n",
    "            train_result = model.train_on_batch(x, y)\n",
    "\n",
    "        for x, y in ds_valid:\n",
    "            valid_result = model.test_on_batch(x, y,reset_metrics=False)\n",
    "            \n",
    "        if epoch%100 ==0:\n",
    "            tf.print(\"epoch = \",epoch)\n",
    "            print(\"train:\",dict(zip(model.metrics_names,train_result)))\n",
    "            print(\"valid:\",dict(zip(model.metrics_names,valid_result)))\n",
    "            print(\"\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a286ada-d7d0-4faf-a82c-43c935a51ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_model(model,ds_train,ds_test,800)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_model(model,ds_train,ds_test,800)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5556bce-de04-45f2-8984-188580027175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nitems_num=3416\\nmaxlen=100\\nlen_seq=50\\nbatch_size=512\\nepoch_num=20\\nhidden_size=64\\nkeep_rate=0.9\\nlayers_num=2\\nnum_interest=1\\nneg_num=4\\ntest_neg_num=100\\n\\n567/567 [==============================] - 15s 15ms/step - loss: 0.3692\\nIteration 1 Fit [15.6 s], Evaluate [0.6 s]: HR = 0.1430, MRR = 0.0510, NDCG = 0.0725\\n567/567 [==============================] - 11s 19ms/step - loss: 0.1491\\nIteration 2 Fit [11.0 s], Evaluate [0.3 s]: HR = 0.1912, MRR = 0.0645, NDCG = 0.0942\\n567/567 [==============================] - 10s 17ms/step - loss: 0.1096\\nIteration 3 Fit [10.0 s], Evaluate [0.3 s]: HR = 0.2171, MRR = 0.0739, NDCG = 0.1074\\n567/567 [==============================] - 10s 18ms/step - loss: 0.0948\\nIteration 4 Fit [10.4 s], Evaluate [0.3 s]: HR = 0.2354, MRR = 0.0829, NDCG = 0.1186\\n567/567 [==============================] - 11s 19ms/step - loss: 0.0859\\nIteration 5 Fit [11.1 s], Evaluate [0.3 s]: HR = 0.2470, MRR = 0.0856, NDCG = 0.1235\\n567/567 [==============================] - 12s 20ms/step - loss: 0.0793\\nIteration 6 Fit [11.8 s], Evaluate [0.3 s]: HR = 0.2502, MRR = 0.0906, NDCG = 0.1281\\n567/567 [==============================] - 12s 21ms/step - loss: 0.0741\\nIteration 7 Fit [12.0 s], Evaluate [0.3 s]: HR = 0.2603, MRR = 0.0971, NDCG = 0.1354\\n567/567 [==============================] - 10s 18ms/step - loss: 0.0702\\nIteration 8 Fit [10.3 s], Evaluate [0.2 s]: HR = 0.2709, MRR = 0.1010, NDCG = 0.1410\\n567/567 [==============================] - 8s 14ms/step - loss: 0.0665\\nIteration 9 Fit [7.9 s], Evaluate [0.3 s]: HR = 0.2667, MRR = 0.1004, NDCG = 0.1395\\n567/567 [==============================] - 9s 16ms/step - loss: 0.0638\\nIteration 10 Fit [9.4 s], Evaluate [0.3 s]: HR = 0.2614, MRR = 0.1013, NDCG = 0.1390\\n567/567 [==============================] - 8s 13ms/step - loss: 0.0613\\nIteration 11 Fit [7.9 s], Evaluate [0.3 s]: HR = 0.2637, MRR = 0.1010, NDCG = 0.1394\\n567/567 [==============================] - 8s 14ms/step - loss: 0.0592\\nIteration 12 Fit [8.1 s], Evaluate [0.3 s]: HR = 0.2601, MRR = 0.0992, NDCG = 0.1372\\n567/567 [==============================] - 8s 13ms/step - loss: 0.0571\\nIteration 13 Fit [7.8 s], Evaluate [0.2 s]: HR = 0.2692, MRR = 0.1030, NDCG = 0.1422\\n567/567 [==============================] - 8s 14ms/step - loss: 0.0556\\nIteration 14 Fit [7.9 s], Evaluate [0.3 s]: HR = 0.2594, MRR = 0.1003, NDCG = 0.1378\\n567/567 [==============================] - 8s 13ms/step - loss: 0.0541\\nIteration 15 Fit [7.7 s], Evaluate [0.2 s]: HR = 0.2669, MRR = 0.1037, NDCG = 0.1422\\n567/567 [==============================] - 8s 13ms/step - loss: nan\\nIteration 16 Fit [7.8 s], Evaluate [0.2 s]: HR = 1.0000, MRR = 1.0000, NDCG = 1.0000\\n567/567 [==============================] - 11s 19ms/step - loss: nan\\nIteration 17 Fit [11.2 s], Evaluate [0.2 s]: HR = 1.0000, MRR = 1.0000, NDCG = 1.0000\\n567/567 [==============================] - 11s 20ms/step - loss: nan\\nIteration 18 Fit [11.8 s], Evaluate [0.2 s]: HR = 1.0000, MRR = 1.0000, NDCG = 1.0000\\n567/567 [==============================] - 10s 17ms/step - loss: nan\\nIteration 19 Fit [9.9 s], Evaluate [0.3 s]: HR = 1.0000, MRR = 1.0000, NDCG = 1.0000\\n567/567 [==============================] - 11s 19ms/step - loss: nan\\nIteration 20 Fit [11.2 s], Evaluate [0.3 s]: HR = 1.0000, MRR = 1.0000, NDCG = 1.0000\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "items_num=3416\n",
    "maxlen=100\n",
    "len_seq=50\n",
    "batch_size=512\n",
    "epoch_num=20\n",
    "hidden_size=64\n",
    "keep_rate=0.9\n",
    "layers_num=2\n",
    "num_interest=1\n",
    "neg_num=4\n",
    "test_neg_num=100\n",
    "\n",
    "567/567 [==============================] - 15s 15ms/step - loss: 0.3692\n",
    "Iteration 1 Fit [15.6 s], Evaluate [0.6 s]: HR = 0.1430, MRR = 0.0510, NDCG = 0.0725\n",
    "567/567 [==============================] - 11s 19ms/step - loss: 0.1491\n",
    "Iteration 2 Fit [11.0 s], Evaluate [0.3 s]: HR = 0.1912, MRR = 0.0645, NDCG = 0.0942\n",
    "567/567 [==============================] - 10s 17ms/step - loss: 0.1096\n",
    "Iteration 3 Fit [10.0 s], Evaluate [0.3 s]: HR = 0.2171, MRR = 0.0739, NDCG = 0.1074\n",
    "567/567 [==============================] - 10s 18ms/step - loss: 0.0948\n",
    "Iteration 4 Fit [10.4 s], Evaluate [0.3 s]: HR = 0.2354, MRR = 0.0829, NDCG = 0.1186\n",
    "567/567 [==============================] - 11s 19ms/step - loss: 0.0859\n",
    "Iteration 5 Fit [11.1 s], Evaluate [0.3 s]: HR = 0.2470, MRR = 0.0856, NDCG = 0.1235\n",
    "567/567 [==============================] - 12s 20ms/step - loss: 0.0793\n",
    "Iteration 6 Fit [11.8 s], Evaluate [0.3 s]: HR = 0.2502, MRR = 0.0906, NDCG = 0.1281\n",
    "567/567 [==============================] - 12s 21ms/step - loss: 0.0741\n",
    "Iteration 7 Fit [12.0 s], Evaluate [0.3 s]: HR = 0.2603, MRR = 0.0971, NDCG = 0.1354\n",
    "567/567 [==============================] - 10s 18ms/step - loss: 0.0702\n",
    "Iteration 8 Fit [10.3 s], Evaluate [0.2 s]: HR = 0.2709, MRR = 0.1010, NDCG = 0.1410\n",
    "567/567 [==============================] - 8s 14ms/step - loss: 0.0665\n",
    "Iteration 9 Fit [7.9 s], Evaluate [0.3 s]: HR = 0.2667, MRR = 0.1004, NDCG = 0.1395\n",
    "567/567 [==============================] - 9s 16ms/step - loss: 0.0638\n",
    "Iteration 10 Fit [9.4 s], Evaluate [0.3 s]: HR = 0.2614, MRR = 0.1013, NDCG = 0.1390\n",
    "567/567 [==============================] - 8s 13ms/step - loss: 0.0613\n",
    "Iteration 11 Fit [7.9 s], Evaluate [0.3 s]: HR = 0.2637, MRR = 0.1010, NDCG = 0.1394\n",
    "567/567 [==============================] - 8s 14ms/step - loss: 0.0592\n",
    "Iteration 12 Fit [8.1 s], Evaluate [0.3 s]: HR = 0.2601, MRR = 0.0992, NDCG = 0.1372\n",
    "567/567 [==============================] - 8s 13ms/step - loss: 0.0571\n",
    "Iteration 13 Fit [7.8 s], Evaluate [0.2 s]: HR = 0.2692, MRR = 0.1030, NDCG = 0.1422\n",
    "567/567 [==============================] - 8s 14ms/step - loss: 0.0556\n",
    "Iteration 14 Fit [7.9 s], Evaluate [0.3 s]: HR = 0.2594, MRR = 0.1003, NDCG = 0.1378\n",
    "567/567 [==============================] - 8s 13ms/step - loss: 0.0541\n",
    "Iteration 15 Fit [7.7 s], Evaluate [0.2 s]: HR = 0.2669, MRR = 0.1037, NDCG = 0.1422\n",
    "567/567 [==============================] - 8s 13ms/step - loss: nan\n",
    "Iteration 16 Fit [7.8 s], Evaluate [0.2 s]: HR = 1.0000, MRR = 1.0000, NDCG = 1.0000\n",
    "567/567 [==============================] - 11s 19ms/step - loss: nan\n",
    "Iteration 17 Fit [11.2 s], Evaluate [0.2 s]: HR = 1.0000, MRR = 1.0000, NDCG = 1.0000\n",
    "567/567 [==============================] - 11s 20ms/step - loss: nan\n",
    "Iteration 18 Fit [11.8 s], Evaluate [0.2 s]: HR = 1.0000, MRR = 1.0000, NDCG = 1.0000\n",
    "567/567 [==============================] - 10s 17ms/step - loss: nan\n",
    "Iteration 19 Fit [9.9 s], Evaluate [0.3 s]: HR = 1.0000, MRR = 1.0000, NDCG = 1.0000\n",
    "567/567 [==============================] - 11s 19ms/step - loss: nan\n",
    "Iteration 20 Fit [11.2 s], Evaluate [0.3 s]: HR = 1.0000, MRR = 1.0000, NDCG = 1.0000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8e66217-f349-4dc4-85fe-a8d250b20e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  9 15:37:30 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:43:00.0 Off |                  N/A |\n",
      "| 43%   49C    P2    89W / 320W |   8485MiB / 10240MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:47:00.0 Off |                  N/A |\n",
      "| 44%   48C    P2   112W / 320W |   8485MiB / 10240MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d3066-cf31-42a9-93cd-09f61bdfbb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
