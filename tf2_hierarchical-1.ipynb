{"cells":[{"cell_type":"code","execution_count":2,"id":"3dd98390-6cf1-4188-8828-d9bbbeea1bd6","metadata":{"id":"3dd98390-6cf1-4188-8828-d9bbbeea1bd6","executionInfo":{"status":"ok","timestamp":1666147022217,"user_tz":-480,"elapsed":485,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"id":"4c3b8cca-95eb-4e76-af74-8467884552ef","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4c3b8cca-95eb-4e76-af74-8467884552ef","executionInfo":{"status":"ok","timestamp":1666147022792,"user_tz":-480,"elapsed":8,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}},"outputId":"498f1488-239e-4190-b99c-99e8f8b099a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.9.2\n"]}],"source":["print(tf.__version__)"]},{"cell_type":"code","execution_count":4,"id":"c58dfbfa-9e1a-4b74-aa4a-58905907a67d","metadata":{"id":"c58dfbfa-9e1a-4b74-aa4a-58905907a67d","executionInfo":{"status":"ok","timestamp":1666147022792,"user_tz":-480,"elapsed":6,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["import pandas as pd\n","import pickle"]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"091gVx5O6rhK","executionInfo":{"status":"ok","timestamp":1666147022792,"user_tz":-480,"elapsed":6,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}},"outputId":"bcf78380-3ea6-4c3d-90b3-bc4b557abb8a"},"id":"091gVx5O6rhK","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K69OSNQW7NHQ","executionInfo":{"status":"ok","timestamp":1666147040190,"user_tz":-480,"elapsed":17401,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}},"outputId":"c800e201-a009-4600-a835-63a47e6c8bd6"},"id":"K69OSNQW7NHQ","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":7,"id":"b4a9ec17-4926-453d-8ca9-9113908b350d","metadata":{"id":"b4a9ec17-4926-453d-8ca9-9113908b350d","executionInfo":{"status":"ok","timestamp":1666147041157,"user_tz":-480,"elapsed":970,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["import pickle\n","with open('/content/drive/My Drive/datasets/ml/ml-1m/all_ratings.pkl', 'rb') as f:    \n","    user_dict = pickle.load(f)\n","items_num=3416\n","maxlen=200\n","len_seq=100\n","batch_size=512\n","epoch_num=20\n","hidden_size=64\n","keep_rate=0.9\n","layers_num=2\n","num_interest=1\n","neg_num=4\n","test_neg_num=100"]},{"cell_type":"code","execution_count":8,"id":"91fd2c73-3e4f-449e-aef3-88288de30804","metadata":{"id":"91fd2c73-3e4f-449e-aef3-88288de30804","executionInfo":{"status":"ok","timestamp":1666147041158,"user_tz":-480,"elapsed":4,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["import random\n","import numpy as np\n","def sample(user_dict,maxlen,len_seq):\n","    train_set=[]\n","    train_val_set=[]\n","    val_set=[]\n","    test_set=[]\n","    for u in user_dict.keys():\n","        idx=0\n","        hist=user_dict[u]        \n","        hist=hist[-maxlen:-2]\n","        #print(hist)\n","        for i in range(1,len(hist)):\n","            seq = np.zeros([len_seq], dtype=np.int32)\n","            #print(hist[0:i])\n","            seq[max(0,len_seq+idx-1):]=hist[max(0,i-len_seq):i]\n","            idx+=-1\n","            nxt = hist[i]\n","            #print((u,seq,nxt))\n","            train_set.append((u,list(seq),nxt))\n","            #print(seq)\n","        train_val_set.append((u,list(seq),nxt))\n","        seq = np.zeros([len_seq], dtype=np.int32)\n","        seq=hist[-len_seq-2:-2]\n","        nxt = user_dict[u][-2]\n","        val_set.append((u,list(seq),nxt))\n","        seq = np.zeros([len_seq], dtype=np.int32)\n","        seq=hist[-len_seq-1:-1]\n","        nxt = user_dict[u][-1]\n","        test_set.append((u,list(seq),nxt))\n","    random.shuffle(train_set)\n","    random.shuffle(test_set)\n","    return train_set,test_set,val_set,train_val_set\n","            \n","def non_zero_sample(user_dict,maxlen,len_seq):\n","    train_set=[]\n","    test_set=[]\n","    val_set=[]\n","    train_val_set=[]\n","    for u in user_dict.keys():\n","        idx=0\n","        hist=user_dict[u]        \n","        seq = np.zeros([len_seq], dtype=np.int32)\n","        #print(np.shape(seq[-maxlen:]))\n","        while(len(hist)<maxlen):\n","            hist.insert(0,0)\n","        #print(len(hist),hist)\n","        hist=hist[-maxlen:-2]\n","        for i in range(len_seq,len(hist)):\n","            seq = np.zeros([len_seq], dtype=np.int32)\n","            #rint(hist[max(0,i-len_seq):i])\n","            seq=hist[max(0,i-len_seq):i]\n","            idx+=-1\n","            nxt = hist[i]\n","            #print((u,seq,nxt))\n","            neg_item = [random.randint(1, items_num) for _ in range(neg_num)]\n","            train_set.append((u,list(seq),nxt,neg_item))\n","            #print(seq)\n","        #print(np.shape(hist[0:len(hist)-1]))\n","        train_val_set.append((u,list(seq),nxt))\n","        seq = np.zeros([len_seq], dtype=np.int32)\n","        seq=hist[-len_seq-2:-2]\n","        nxt = user_dict[u][-2]\n","        neg_item = [random.randint(1, items_num) for _ in range(neg_num)]\n","        val_set.append((u,list(seq),nxt,neg_item))\n","        seq = np.zeros([len_seq], dtype=np.int32)\n","        seq=hist[-len_seq-1:-1]\n","        nxt = user_dict[u][-1]\n","        neg_item = [random.randint(1, items_num) for _ in range(test_neg_num)]\n","        test_set.append((u,list(seq),nxt,neg_item))\n","    random.shuffle(train_set)\n","    random.shuffle(test_set)\n","    return train_set,test_set ,val_set,train_val_set   "]},{"cell_type":"code","execution_count":9,"id":"904dd44f-cae8-4e03-a8f7-a76fbffae7ed","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"904dd44f-cae8-4e03-a8f7-a76fbffae7ed","executionInfo":{"status":"ok","timestamp":1666147051799,"user_tz":-480,"elapsed":10644,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}},"outputId":"ed0c9e6f-4f87-4948-8e74-586ec0c0f5ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["591920 6040 6040 6040\n"]}],"source":["train_set,test_set,val_set,train_val_set=non_zero_sample(user_dict,maxlen,len_seq)\n","print(len(train_set),len(test_set),len(val_set),len(train_val_set))\n","users_num=len(test_set)"]},{"cell_type":"code","execution_count":10,"id":"881ce4c1-a8b0-4c59-98db-5c6df6ae46a1","metadata":{"id":"881ce4c1-a8b0-4c59-98db-5c6df6ae46a1","executionInfo":{"status":"ok","timestamp":1666147060872,"user_tz":-480,"elapsed":7653,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["users, click_seqs, pos_items,neg_items = zip(*train_set)\n","train_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\n","users, click_seqs, pos_items,neg_items = zip(*test_set)\n","test_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}"]},{"cell_type":"code","execution_count":11,"id":"32879739-778f-4a8b-aabe-f2b701d2253b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"32879739-778f-4a8b-aabe-f2b701d2253b","executionInfo":{"status":"ok","timestamp":1666147060873,"user_tz":-480,"elapsed":37,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}},"outputId":"aee1763d-8d01-4cd4-dc71-45ce49bb75d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["(591920, 4)\n"]}],"source":["print(np.shape(train_data['neg_item']))"]},{"cell_type":"code","execution_count":12,"id":"1df82d54-78b8-48b8-84b9-ad4896e2236a","metadata":{"id":"1df82d54-78b8-48b8-84b9-ad4896e2236a","executionInfo":{"status":"ok","timestamp":1666147060873,"user_tz":-480,"elapsed":34,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["def scaled_dot_product_attention(q, k, v, mask):\n","    \"\"\"Attention Mechanism Function.\n","    Args:\n","        :param q: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n","        :param k: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n","        :param v: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n","        :param mask: A 3d/4d tensor with shape of (None, ..., seq_len, 1)\n","    :return:\n","    \"\"\"\n","    mat_qk = tf.matmul(q, k, transpose_b=True)  # (None, seq_len, seq_len)\n","    # Scaled\n","    dk = tf.cast(k.shape[-1], dtype=tf.float32)\n","    scaled_att_logits = mat_qk / tf.sqrt(dk)\n","\n","    paddings = tf.ones_like(scaled_att_logits) * (-2 ** 32 + 1)  # (None, seq_len, seq_len)\n","    if mask!=None:\n","        outputs = tf.where(tf.equal(mask, tf.zeros_like(mask)), paddings, scaled_att_logits)  # (None, seq_len, seq_len)\n","    else:\n","        outputs=scaled_att_logits\n","    # softmax\n","    outputs = tf.nn.softmax(logits=outputs)  # (None, seq_len, seq_len)\n","    outputs = tf.matmul(outputs, v)  # (None, seq_len, dim)\n","\n","    return outputs\n","\n","\n","def split_heads(x, seq_len, num_heads, depth):\n","    \"\"\"Split the last dimension into (num_heads, depth).\n","        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","    Args:\n","        :param x: A Tensor with shape of [batch_size, seq_len, num_heads * depth]\n","        :param seq_len: A scalar(int).\n","        :param num_heads: A scalar(int).\n","        :param depth: A scalar(int).\n","    :return: A tensor with shape of [batch_size, num_heads, seq_len, depth]\n","    \"\"\"\n","    x = tf.reshape(x, (-1, seq_len, num_heads, depth))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])"]},{"cell_type":"code","execution_count":14,"id":"e1382056-672c-4683-a46a-dca40cd582b2","metadata":{"id":"e1382056-672c-4683-a46a-dca40cd582b2","executionInfo":{"status":"ok","timestamp":1666147079765,"user_tz":-480,"elapsed":328,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        \"\"\"Multi Head Attention Mechanism.\n","        Args:\n","            :param d_model: A scalar. The self-attention hidden size.\n","            :param num_heads: A scalar. Number of heads. If num_heads == 1, the layer is a single self-attention layer.\n","        :return:\n","        \"\"\"\n","        super(MultiHeadAttention, self).__init__()\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model, activation=None)\n","        self.wk = tf.keras.layers.Dense(d_model, activation=None)\n","        self.wv = tf.keras.layers.Dense(d_model, activation=None)\n","\n","    def call(self, q, k, v, mask):\n","        q = self.wq(q)  # (None, seq_len, d_model)\n","        k = self.wk(k)  # (None, seq_len, d_model)\n","        v = self.wv(v)  # (None, seq_len, d_model)\n","        # split d_model into num_heads * depth\n","        seq_len, d_model = q.shape[1], q.shape[2]\n","        q = split_heads(q, seq_len, self.num_heads, q.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n","        k = split_heads(k, seq_len, self.num_heads, k.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n","        v = split_heads(v, seq_len, self.num_heads, v.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n","        # mask\n","        if mask!=None:\n","            mask = tf.tile(tf.expand_dims(mask, axis=1), [1, self.num_heads, 1, 1])  # (None, num_heads, seq_len, 1)\n","        # attention\n","        scaled_attention = scaled_dot_product_attention(q, k, v, mask)  # (None, num_heads, seq_len, d_model // num_heads)\n","        # reshape\n","        outputs = tf.reshape(tf.transpose(scaled_attention, [0, 2, 1, 3]), [-1, seq_len, d_model])  # (None, seq_len, d_model)\n","        return outputs\n","\n","\n","class FFN(tf.keras.layers.Layer):\n","    def __init__(self, hidden_unit, d_model):\n","        \"\"\"Feed Forward Network.\n","        Args:\n","            :param hidden_unit: A scalar.\n","            :param d_model: A scalar.\n","        :return:\n","        \"\"\"\n","        super(FFN, self).__init__()\n","        self.conv1 = tf.keras.layers.Conv1D(filters=hidden_unit, kernel_size=1, activation='relu', use_bias=True)\n","        self.conv2 = tf.keras.layers.Conv1D(filters=d_model, kernel_size=1, activation=None, use_bias=True)\n","\n","    def call(self, inputs):\n","        x = self.conv1(inputs)\n","        output = self.conv2(x)\n","        return output\n","\n","\n","class TransformerEncoder(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads=1, ffn_hidden_unit=128, dropout=0., layer_norm_eps=1e-6):\n","        \"\"\"Encoder Layer.\n","        Args:\n","            :param d_model: A scalar. The self-attention hidden size.\n","            :param num_heads: A scalar. Number of heads.\n","            :param ffn_hidden_unit: A scalar. Number of hidden unit in FFN\n","            :param dropout: A scalar. Number of dropout.\n","            :param layer_norm_eps: A scalar. Small float added to variance to avoid dividing by zero.\n","        :return:\n","        \"\"\"\n","        super(TransformerEncoder, self).__init__()\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = FFN(ffn_hidden_unit, d_model)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(dropout)\n","        self.dropout2 = tf.keras.layers.Dropout(dropout)\n","\n","    def call(self, inputs):\n","        x, mask= inputs\n","        # self-attention\n","        att_out = self.mha(x, x, x, mask)  # (None, seq_len, d_model)\n","        att_out = self.dropout1(att_out)\n","        # residual add\n","        out1 = self.layernorm1(x + att_out)  # (None, seq_len, d_model)\n","        # ffn\n","        ffn_out = self.ffn(out1)\n","        ffn_out = self.dropout2(ffn_out)\n","        # residual add\n","        out2 = self.layernorm2(out1 + ffn_out)  # (None, seq_len, d_model)\n","        return out2\n","\n","class hierarchical_interest(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        \"\"\"Multi Head Attention Mechanism.\n","        Args:\n","            :param d_model: A scalar. The self-attention hidden size.\n","            :param num_heads: A scalar. Number of heads. If num_heads == 1, the layer is a single self-attention layer.\n","        :return:\n","        \"\"\"\n","        super(hierarchical_interest, self).__init__()\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.layer_1=tf.keras.layers.Conv1D(self.d_model,10,padding='same',input_shape=[len_seq,hidden_size])\n","        self.layer_2=tf.keras.layers.Conv1D(self.d_model,5,padding='same',input_shape=[len_seq,hidden_size])\n","        self.layer_3=tf.keras.layers.Conv1D(self.d_model,1,padding='same',input_shape=[len_seq,hidden_size])\n","        self.layernorm = tf.keras.layers.LayerNormalization()\n","    def call(self, x):\n","        interest_1 = self.layer_1(x)  # (None, len_seq, d_model)\n","        interest_2 = self.layer_2(x)  # (None, len_seq, d_model)\n","        interest_3 = self.layer_3(x)  # (None, len_seq, d_model)\n","        interest_2=interest_2-scaled_dot_product_attention(interest_2,interest_1,interest_1,None)\n","        interest_3=interest_3-scaled_dot_product_attention(interest_3,interest_1,interest_1,None)\n","        interest_1=self.layernorm(interest_1)\n","        interest_2=self.layernorm(interest_2)\n","        interest_3=self.layernorm(interest_3)\n","        return interest_1,interest_2,interest_3"]},{"cell_type":"code","execution_count":20,"id":"49fb9d25-6709-4545-9211-898760d749eb","metadata":{"id":"49fb9d25-6709-4545-9211-898760d749eb","executionInfo":{"status":"ok","timestamp":1666147141304,"user_tz":-480,"elapsed":322,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["class sasrec(tf.keras.models.Model):\n","    def __init__(self):\n","        super(sasrec, self).__init__()\n","        blocks=1\n","        embed_reg=0.\n","        layer_norm_eps=1e-6\n","        num_heads=1\n","        use_l2norm=False\n","        \n","        self.len_seq=len_seq\n","        self.item_embedding = tf.keras.layers.Embedding(items_num+1,hidden_size,input_length=self.len_seq,\n","                                                       embeddings_initializer='random_normal',\n","                                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n","        self.pos_embedding = tf.keras.layers.Embedding(len_seq,hidden_size,input_length=1,\n","                                                       embeddings_initializer='random_normal',\n","                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n","        self.dropout = tf.keras.layers.Dropout(1-keep_rate)\n","        self.hiera=hierarchical_interest(hidden_size,0)\n","        self.encoder_layer_1 = [TransformerEncoder(hidden_size, num_heads, 128,\n","                                                 (1-keep_rate), layer_norm_eps) for _ in range(blocks)]\n","        self.encoder_layer_2 = [TransformerEncoder(hidden_size, num_heads, 128,\n","                                                 (1-keep_rate), layer_norm_eps) for _ in range(blocks)]\n","        self.encoder_layer_3= [TransformerEncoder(hidden_size, num_heads, 128,\n","                                                 (1-keep_rate), layer_norm_eps) for _ in range(blocks)]\n","                                                 \n","        # norm\n","        self.use_l2norm = use_l2norm\n","        \n","    \n","    def call(self, inputs):\n","        seq_embed=self.item_embedding(inputs['click_seq'])\n","        pos_encoding = tf.expand_dims(self.pos_embedding(tf.range(self.len_seq)), axis=0)  # (1, seq_len, embed_dim)\n","        seq_embed += pos_encoding\n","\n","        seq_embed = self.dropout(seq_embed)\n","        \n","        interest_1,interest_2,interest_3=self.hiera(seq_embed)\n","\n","        att_outputs_1=interest_1\n","        for block in self.encoder_layer_1:\n","            att_outputs_1 = block([att_outputs_1,None])  # (None, seq_len, embed_dim)\n","        \n","        att_outputs_2=interest_2\n","        for block in self.encoder_layer_2:\n","            att_outputs_2= block([att_outputs_2,None])  # (None, seq_len, embed_dim)\n","        \n","        att_outputs_3=interest_3\n","        for block in self.encoder_layer_3:\n","            att_outputs_3 = block([att_outputs_3,None])  # (None, seq_len, embed_dim)\n","        \n","\n","        interest_1 = tf.slice(att_outputs_1, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n","        interest_2 = tf.slice(att_outputs_2, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n","        interest_3 = tf.slice(att_outputs_3, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n","        \n","        mixed_interest=tf.concat([interest_1,interest_2,interest_3],axis=-2)#[b,3,h]\n","        #mixed_interest=interest_emb_3\n","        #mixed_interest=tf.squeeze(mixed_interest,axis=1)\n","        \n","        self.mixed_interest=mixed_interest\n","        \n","        if self.use_l2norm:\n","            pos_emb = tf.math.l2_normalize(pos_emb, axis=-1)\n","            neg_emb = tf.math.l2_normalize(neg_emb, axis=-1)\n","            user_info = tf.math.l2_normalize(user_info, axis=-1)\n","\n","        pos_emb=self.item_embedding(tf.reshape(inputs['pos_item'], [-1, ]))#[b,h]\n","        expanded_target_item=tf.expand_dims(pos_emb,axis=1)#[b,1,h]\n","\n","        alpha=tf.matmul(expanded_target_item,tf.transpose(mixed_interest,[0,2,1]))#[b,1,3]\n","        alpha=tf.nn.softmax(alpha)\n","        \n","\n","        res = tf.matmul(alpha,mixed_interest)#[b,1,h]\n","        pos_score=tf.reduce_sum(tf.multiply(res,expanded_target_item),axis=-1)#[b,1]\n","\n","        neg_emb=self.item_embedding(inputs['neg_item'])#[b,neg_num,h]\n","        alpha=tf.matmul(neg_emb,tf.transpose(mixed_interest,[0,2,1]))#[b,neg_num,3]\n","        alpha=tf.nn.softmax(alpha)\n","        \n","        #[b,t,1,3]*[b,1,3,h]=[b,t,1,h] squeeze=[b,t,h]\n","        res = tf.matmul(alpha,mixed_interest)#[b,neg_num,h]\n","        neg_score=tf.reduce_sum(tf.multiply(res,neg_emb),axis=-1)#[b,neg_num]\n","        \n","        \n","        \n","        \n","        \n","        pos_score = tf.tile(pos_score, [1, neg_score.shape[1]])\n","        \n","        loss = tf.reduce_mean(- tf.math.log(tf.nn.sigmoid(pos_score)) - tf.math.log(1 - tf.nn.sigmoid(neg_score))) / 2\n","        self.add_loss(loss)\n","        logits = tf.concat([pos_score, neg_score], axis=-1)\n","        return logits\n","    \n","    def summary(self):\n","        inputs = {\n","            'click_seq': tf.keras.layers.Input(shape=(self.len_seq,), dtype=tf.int32),\n","            'pos_item': tf.keras.layers.Input(shape=(), dtype=tf.int32),\n","            'neg_item': tf.keras.layers.Input(shape=(1,), dtype=tf.int32)  # suppose neg_num=1\n","        }\n","        tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs)).summary()"]},{"cell_type":"code","execution_count":21,"id":"ef04f38c-df08-4399-bbd0-f47bc1ee3213","metadata":{"id":"ef04f38c-df08-4399-bbd0-f47bc1ee3213","executionInfo":{"status":"ok","timestamp":1666147142592,"user_tz":-480,"elapsed":1,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["tf.keras.backend.clear_session()\n","\n","model = sasrec()\n","#model.summary()\n","optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n","model.compile(optimizer=optimizer)"]},{"cell_type":"code","execution_count":22,"id":"ab39a04b-df32-46da-93f0-7f2749da22d5","metadata":{"id":"ab39a04b-df32-46da-93f0-7f2749da22d5","executionInfo":{"status":"ok","timestamp":1666147143886,"user_tz":-480,"elapsed":2,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["def hr(rank, k):\n","    \"\"\"Hit Rate.\n","    Args:\n","        :param rank: A list.\n","        :param k: A scalar(int).\n","    :return: hit rate.\n","    \"\"\"\n","    res = 0.0\n","    for r in rank:\n","        if r < k:\n","            res += 1\n","    return res / len(rank)\n","\n","\n","def mrr(rank, k):\n","    \"\"\"Mean Reciprocal Rank.\n","    Args:\n","        :param rank: A list.\n","        :param k: A scalar(int).\n","    :return: mrr.\n","    \"\"\"\n","    mrr = 0.0\n","    for r in rank:\n","        if r < k:\n","            mrr += 1 / (r + 1)\n","    return mrr / len(rank)\n","\n","\n","def ndcg(rank, k):\n","    \"\"\"Normalized Discounted Cumulative Gain.\n","    Args:\n","        :param rank: A list.\n","        :param k: A scalar(int).\n","    :return: ndcg.\n","    \"\"\"\n","    res = 0.0\n","    for r in rank:\n","        if r < k:\n","            res += 1 / np.log2(r + 2)\n","    return res / len(rank)\n","\n","\n","def eval_rank(pred_y, metric_names, k=10):\n","    \"\"\"Evaluate\n","        Args:\n","            :param pred_y: A ndarray.\n","            :param metric_names: A list like ['hr'].\n","            :param k: A scalar(int).\n","        :return: A result dict such as {'hr':, 'ndcg':, ...}\n","    \"\"\"\n","    rank = pred_y.argsort().argsort()[:, 0]\n","    res_dict = {}\n","    for name in metric_names:\n","        if name == 'hr':\n","            res = hr(rank, k)\n","        elif name == 'ndcg':\n","            res = ndcg(rank, k)\n","        elif name == 'mrr':\n","            res = mrr(rank, k)\n","        else:\n","            break\n","        res_dict[name] = res\n","    return res_dict\n","\n","def eval_pos_neg(model, test_data, metric_names, k=10, batch_size=None):\n","    \"\"\"Evaluate the performance of Top-k recommendation algorithm.\n","    Note: Test data must contain some negative samples(>= k) and one positive samples.\n","    Args:\n","        :param model: A model built-by tensorflow.\n","        :param test_data: A dict.\n","        :param metric_names: A list like ['hr'].\n","        :param k: A scalar(int).\n","        :param batch_size: A scalar(int).\n","    :return: A result dict such as {'hr':, 'ndcg':, ...}\n","    \"\"\"\n","    pred_y = - model.predict(test_data, batch_size)\n","    return eval_rank(pred_y, metric_names, k)"]},{"cell_type":"code","execution_count":null,"id":"3d0933a7-b926-4371-a008-0e992c04d096","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3d0933a7-b926-4371-a008-0e992c04d096","outputId":"b91891a5-5d41-462b-fa17-174a05dafd46"},"outputs":[{"output_type":"stream","name":"stdout","text":["   2/1157 [..............................] - ETA: 58:59 - loss: 0.6785  "]}],"source":["from time import time\n","for epoch in range(1, epoch_num + 1):\n","    t1 = time()\n","    model.fit(\n","            x=train_data,\n","            epochs=1,\n","            batch_size=batch_size)\n","    t2 = time()\n","    eval_dict = eval_pos_neg(model, test_data, ['hr', 'mrr', 'ndcg'], 10, batch_size)\n","    print('Iteration %d Fit [%.1f s], Evaluate [%.1f s]: HR = %.4f, MRR = %.4f, NDCG = %.4f'\n","              % (epoch, t2 - t1, time() - t2, eval_dict['hr'], eval_dict['mrr'], eval_dict['ndcg']))"]},{"cell_type":"code","execution_count":null,"id":"1a9725cf-0012-418f-bd02-be06cc7ab1ad","metadata":{"id":"1a9725cf-0012-418f-bd02-be06cc7ab1ad","executionInfo":{"status":"aborted","timestamp":1666147060876,"user_tz":-480,"elapsed":18,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["a=tf.constant([[2558, 2559, 2582, 2605, 2608, 2669, 2801, 2835, 2913, 3052]])\n","tf.print(tf.argmax(model(a)[0]))"]},{"cell_type":"code","execution_count":null,"id":"79481e70-93d3-41d1-8e5b-558125f5da11","metadata":{"id":"79481e70-93d3-41d1-8e5b-558125f5da11","executionInfo":{"status":"aborted","timestamp":1666147060876,"user_tz":-480,"elapsed":18,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["'''def train_model(model,ds_train,ds_valid,epoches):\n","\n","    for epoch in tf.range(1,epoches+1):\n","        model.reset_metrics()\n","        \n","        # 在后期降低学习率\n","        if epoch == 5000:\n","            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n","            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n","        \n","        for x, y in ds_train:\n","            train_result = model.train_on_batch(x, y)\n","\n","        for x, y in ds_valid:\n","            valid_result = model.test_on_batch(x, y,reset_metrics=False)\n","            \n","        if epoch%100 ==0:\n","            tf.print(\"epoch = \",epoch)\n","            print(\"train:\",dict(zip(model.metrics_names,train_result)))\n","            print(\"valid:\",dict(zip(model.metrics_names,valid_result)))\n","            print(\"\")'''"]},{"cell_type":"code","execution_count":null,"id":"6a286ada-d7d0-4faf-a82c-43c935a51ff9","metadata":{"id":"6a286ada-d7d0-4faf-a82c-43c935a51ff9","executionInfo":{"status":"aborted","timestamp":1666147060876,"user_tz":-480,"elapsed":18,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":["'''train_model(model,ds_train,ds_test,800)'''"]},{"cell_type":"code","execution_count":null,"id":"f8e66217-f349-4dc4-85fe-a8d250b20e79","metadata":{"id":"f8e66217-f349-4dc4-85fe-a8d250b20e79","executionInfo":{"status":"aborted","timestamp":1666147060877,"user_tz":-480,"elapsed":19,"user":{"displayName":"yijun sheng","userId":"07090048668815296857"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}