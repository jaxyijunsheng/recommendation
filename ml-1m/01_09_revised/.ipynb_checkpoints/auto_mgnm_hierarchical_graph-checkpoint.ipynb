{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd98390-6cf1-4188-8828-d9bbbeea1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#os.environ['AUTOGRAPH_VERBOSITY'] = '0'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23eb670f-5896-440f-9bd5-2b5bc312537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a9ec17-4926-453d-8ca9-9113908b350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/root/autodl-nas/workspace/datasets/ml/ml-1m/all_ratings.pkl', 'rb') as f:  \n",
    "    user_dict = pickle.load(f)\n",
    "items_num=[]\n",
    "for i in user_dict.values():\n",
    "    items_num.append(max(i))\n",
    "items_num=max(items_num)\n",
    "\n",
    "maxlen=124\n",
    "len_seq=10\n",
    "\n",
    "batch_size=512\n",
    "epoch_num=100\n",
    "hidden_size=64\n",
    "keep_rate=0.9\n",
    "layers_num=3\n",
    "num_heads=1\n",
    "interest_num=3\n",
    "kernel_size=4\n",
    "transformerencoder=0\n",
    "multi_dense=0\n",
    "neg_num=5\n",
    "test_neg_num=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91fd2c73-3e4f-449e-aef3-88288de30804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def sample(user_dict,maxlen,len_seq):\n",
    "    train_set=[]\n",
    "    train_val_set=[]\n",
    "    val_set=[]\n",
    "    test_set=[]\n",
    "    for u in user_dict.keys():\n",
    "        idx=0\n",
    "        hist=user_dict[u]        \n",
    "        hist=hist[-maxlen:-2]\n",
    "        #print(hist)\n",
    "        for i in range(1,len(hist)):\n",
    "            seq = np.zeros([len_seq], dtype=np.int32)\n",
    "            #print(hist[0:i])\n",
    "            seq[max(0,len_seq+idx-1):]=hist[max(0,i-len_seq):i]\n",
    "            idx+=-1\n",
    "            nxt = hist[i]\n",
    "            #print((u,seq,nxt))\n",
    "            train_set.append((u,list(seq),nxt))\n",
    "            #print(seq)\n",
    "        train_val_set.append((u,list(seq),nxt))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-2:-2]\n",
    "        nxt = user_dict[u][-2]\n",
    "        val_set.append((u,list(seq),nxt))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-1:-1]\n",
    "        nxt = user_dict[u][-1]\n",
    "        test_set.append((u,list(seq),nxt))\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    return train_set,test_set,val_set,train_val_set\n",
    "            \n",
    "def non_zero_sample(user_dict,maxlen,len_seq):\n",
    "    train_set=[]\n",
    "    test_set=[]\n",
    "    val_set=[]\n",
    "    train_val_set=[]\n",
    "    for u in user_dict.keys():\n",
    "        idx=0\n",
    "        hist=user_dict[u]        \n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        #print(np.shape(seq[-maxlen:]))\n",
    "        while(len(hist)<maxlen):\n",
    "            hist.insert(0,0)\n",
    "        #print(len(hist),hist)\n",
    "        hist=hist[-maxlen:-2]\n",
    "        for i in range(len_seq,len(hist)):\n",
    "            seq = np.zeros([len_seq], dtype=np.int32)\n",
    "            #rint(hist[max(0,i-len_seq):i])\n",
    "            seq=hist[max(0,i-len_seq):i]\n",
    "            idx+=-1\n",
    "            nxt = hist[i]\n",
    "            #print((u,seq,nxt))\n",
    "            neg_item = [random.randint(1, items_num) for _ in range(neg_num)]\n",
    "            train_set.append((u,list(seq),nxt,neg_item))\n",
    "            #print(seq)\n",
    "        #print(np.shape(hist[0:len(hist)-1]))\n",
    "        train_val_set.append((u,list(seq),nxt))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-2:-2]\n",
    "        nxt = user_dict[u][-2]\n",
    "        neg_item = [random.randint(1, items_num) for _ in range(neg_num)]\n",
    "        val_set.append((u,list(seq),nxt,neg_item))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-1:-1]\n",
    "        nxt = user_dict[u][-1]\n",
    "        neg_item = [random.randint(1, items_num) for _ in range(test_neg_num)]\n",
    "        test_set.append((u,list(seq),nxt,neg_item))\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    return train_set,test_set ,val_set,train_val_set   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "904dd44f-cae8-4e03-a8f7-a76fbffae7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_set,test_set,val_set,train_val_set=non_zero_sample(user_dict,maxlen,len_seq)\\nprint(len(train_set),len(test_set),len(val_set),len(train_val_set))\\nusers_num=len(test_set)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_set,test_set,val_set,train_val_set=non_zero_sample(user_dict,maxlen,len_seq)\n",
    "print(len(train_set),len(test_set),len(val_set),len(train_val_set))\n",
    "users_num=len(test_set)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881ce4c1-a8b0-4c59-98db-5c6df6ae46a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"users, click_seqs, pos_items,neg_items = zip(*train_set)\\ntrain_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\\nusers, click_seqs, pos_items,neg_items = zip(*test_set)\\ntest_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\\nusers, click_seqs, pos_items,neg_items = zip(*val_set)\\nval_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''users, click_seqs, pos_items,neg_items = zip(*train_set)\n",
    "train_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\n",
    "users, click_seqs, pos_items,neg_items = zip(*test_set)\n",
    "test_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\n",
    "users, click_seqs, pos_items,neg_items = zip(*val_set)\n",
    "val_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "154cc3c0-4e62-4538-92c7-650f2539743e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open('/root/autodl-tmp/workspace/datasets/kuairec/small_matrix_train_data.pkl', 'wb') as f:\\n    pickle.dump(train_data, f, -1)\\nwith open('/root/autodl-tmp/workspace/datasets/kuairec/small_matrix_test_data.pkl', 'wb') as f:\\n    pickle.dump(test_data, f, -1)\\nwith open('/root/autodl-tmp/workspace/datasets/kuairec/small_matrix_val_data.pkl', 'wb') as f:\\n    pickle.dump(val_data, f, -1)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with open('/root/autodl-tmp/workspace/datasets/kuairec/small_matrix_train_data.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f, -1)\n",
    "with open('/root/autodl-tmp/workspace/datasets/kuairec/small_matrix_test_data.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data, f, -1)\n",
    "with open('/root/autodl-tmp/workspace/datasets/kuairec/small_matrix_val_data.pkl', 'wb') as f:\n",
    "    pickle.dump(val_data, f, -1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4abddbd-3124-41e1-a15c-16c9ae07e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='ml-1m'\n",
    "\n",
    "with open('/root/autodl-nas/workspace/datasets/ml/ml-1m/'+dataset_name+'_train_data.pkl', 'rb') as f:\n",
    "    train_data=pickle.load(f)\n",
    "with open('/root/autodl-nas/workspace/datasets/ml/ml-1m/'+dataset_name+'_test_data.pkl', 'rb') as f:\n",
    "    test_data=pickle.load(f)\n",
    "with open('/root/autodl-nas/workspace/datasets/ml/ml-1m/'+dataset_name+'_val_data.pkl', 'rb') as f:\n",
    "    val_data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32879739-778f-4a8b-aabe-f2b701d2253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(527766, 5) (6040, 1000) (6040, 5)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_data['neg_item']),np.shape(test_data['neg_item']),np.shape(val_data['neg_item']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df82d54-78b8-48b8-84b9-ad4896e2236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Attention Mechanism Function.\n",
    "    Args:\n",
    "        :param q: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param k: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param v: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param mask: A 3d/4d tensor with shape of (None, ..., seq_len, 1)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mat_qk = tf.matmul(q, k, transpose_b=True)  # (None, seq_len, seq_len)\n",
    "    # Scaled\n",
    "    dk = tf.cast(k.shape[-1], dtype=tf.float32)\n",
    "    scaled_att_logits = mat_qk / tf.sqrt(dk)\n",
    "\n",
    "    paddings = tf.ones_like(scaled_att_logits) * (-2 ** 32 + 1)  # (None, seq_len, seq_len)\n",
    "    if mask!=None:\n",
    "        outputs = tf.where(tf.equal(mask, tf.zeros_like(mask)), paddings, scaled_att_logits)  # (None, seq_len, seq_len)\n",
    "    else:\n",
    "        outputs=scaled_att_logits\n",
    "    # softmax\n",
    "    outputs = tf.nn.softmax(logits=outputs)  # (None, seq_len, seq_len)\n",
    "    outputs = tf.matmul(outputs, v)  # (None, seq_len, dim)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def split_heads(x, seq_len, num_heads, depth):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    Args:\n",
    "        :param x: A Tensor with shape of [batch_size, seq_len, num_heads * depth]\n",
    "        :param seq_len: A scalar(int).\n",
    "        :param num_heads: A scalar(int).\n",
    "        :param depth: A scalar(int).\n",
    "    :return: A tensor with shape of [batch_size, num_heads, seq_len, depth]\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (-1, seq_len, num_heads, depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "def normalize_adj_tensor(adj, seq_len):\n",
    "    adj = adj + tf.expand_dims(tf.eye(seq_len), axis=0)\n",
    "    rowsum = tf.reduce_sum(adj, axis=1)\n",
    "    d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "    candidate_a = tf.zeros_like(d_inv_sqrt)\n",
    "    d_inv_sqrt = tf.where(tf.math.is_inf(d_inv_sqrt), candidate_a, d_inv_sqrt)\n",
    "    d_mat_inv_sqrt = tf.linalg.diag(d_inv_sqrt)\n",
    "    norm_adg = tf.matmul(d_mat_inv_sqrt, adj)\n",
    "    return norm_adg\n",
    "\n",
    "\n",
    "def contrastive_label(seq_embed,att):\n",
    "    s_item_emd,s_item_index=tf.math.top_k(att, k)\n",
    "    s_item_index=tf.reshape(s_item_index,[-1,1])\n",
    "    h_index=tf.sort(tf.tile(tf.reshape(tf.range(tf.shape(seq_embed)[0]),[-1,1]),[interest_num*k,1]),axis=0)\n",
    "    index = tf.stack([h_index, s_item_index])\n",
    "    index=tf.reshape(index,[-1,k,2])\n",
    "    all_user_label=tf.gather_nd(seq_embed, index)\n",
    "    all_user_label=tf.reshape(all_user_label,[-1,interest_num,k,hidden_size])\n",
    "    all_user_label=tf.reduce_mean(all_user_label,axis=-2)\n",
    "    return all_user_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1382056-672c-4683-a46a-dca40cd582b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"Multi Head Attention Mechanism.\n",
    "        Args:\n",
    "            :param d_model: A scalar. The self-attention hidden size.\n",
    "            :param num_heads: A scalar. Number of heads. If num_heads == 1, the layer is a single self-attention layer.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model, activation=None)\n",
    "        self.wk = tf.keras.layers.Dense(d_model, activation=None)\n",
    "        self.wv = tf.keras.layers.Dense(d_model, activation=None)\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "        q = self.wq(q)  # (None, seq_len, d_model)\n",
    "        k = self.wk(k)  # (None, seq_len, d_model)\n",
    "        v = self.wv(v)  # (None, seq_len, d_model)\n",
    "        # split d_model into num_heads * depth\n",
    "        seq_len, d_model = q.shape[1], q.shape[2]\n",
    "        q = split_heads(q, seq_len, self.num_heads, q.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        k = split_heads(k, seq_len, self.num_heads, k.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        v = split_heads(v, seq_len, self.num_heads, v.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        # mask\n",
    "        if mask!=None:\n",
    "            mask = tf.tile(tf.expand_dims(mask, axis=1), [1, self.num_heads, 1, 1])  # (None, num_heads, seq_len, 1)\n",
    "        # attention\n",
    "        scaled_attention = scaled_dot_product_attention(q, k, v, mask)  # (None, num_heads, seq_len, d_model // num_heads)\n",
    "        # reshape\n",
    "        outputs = tf.reshape(tf.transpose(scaled_attention, [0, 2, 1, 3]), [-1, seq_len, d_model])  # (None, seq_len, d_model)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class FFN(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_unit, d_model):\n",
    "        \"\"\"Feed Forward Network.\n",
    "        Args:\n",
    "            :param hidden_unit: A scalar.\n",
    "            :param d_model: A scalar.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(FFN, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(filters=hidden_unit, kernel_size=1, activation='relu', use_bias=True)\n",
    "        self.conv2 = tf.keras.layers.Conv1D(filters=d_model, kernel_size=1, activation=None, use_bias=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        output = self.conv2(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads=1, ffn_hidden_unit=128, dropout=0., layer_norm_eps=1e-6):\n",
    "        \"\"\"Encoder Layer.\n",
    "        Args:\n",
    "            :param d_model: A scalar. The self-attention hidden size.\n",
    "            :param num_heads: A scalar. Number of heads.\n",
    "            :param ffn_hidden_unit: A scalar. Number of hidden unit in FFN\n",
    "            :param dropout: A scalar. Number of dropout.\n",
    "            :param layer_norm_eps: A scalar. Small float added to variance to avoid dividing by zero.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FFN(ffn_hidden_unit, d_model)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, mask= inputs\n",
    "        # self-attention\n",
    "        att_out = self.mha(x, x, x, mask)  # (None, seq_len, d_model)\n",
    "        att_out = self.dropout1(att_out)\n",
    "        # residual add\n",
    "        out1 = self.layernorm1(x + att_out)  # (None, seq_len, d_model)\n",
    "        # ffn\n",
    "        ffn_out = self.ffn(out1)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        # residual add\n",
    "        out2 = self.layernorm2(out1 + ffn_out)  # (None, seq_len, d_model)\n",
    "        return out2\n",
    "\n",
    "class gcn_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size, num_layer):\n",
    "        super(gcn_layer, self).__init__()\n",
    "        #weights_size_list = [hidden_size,64,64,64]\n",
    "        #all_weights = {}\n",
    "        self.num_layer=num_layer\n",
    "        self.conv_dense=tf.keras.layers.Dense(64, activation=tf.keras.layers.LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    def call(self,A, x):\n",
    "    # gcn has three layers\n",
    "        all_embeddings =[x]\n",
    "        for k in range(self.num_layer):\n",
    "            embeddings = tf.matmul(A, x)\n",
    "            embeddings = self.conv_dense(embeddings)\n",
    "            all_embeddings.append(embeddings)\n",
    "        return all_embeddings\n",
    "\n",
    "class light_gcn_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size, num_layer):\n",
    "        super(gcn_layer, self).__init__()\n",
    "        #weights_size_list = [hidden_size,64,64,64]\n",
    "        #all_weights = {}\n",
    "        self.num_layer=num_layer\n",
    "        self.conv_dense=tf.keras.layers.Dense(64, activation=tf.keras.layers.LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    def call(self,A, x):\n",
    "    # gcn has three layers\n",
    "        all_embeddings =[x]\n",
    "        for k in range(self.num_layer):\n",
    "            embeddings = tf.matmul(A, x)\n",
    "            #embeddings = self.conv_dense(embeddings)\n",
    "            all_embeddings.append(embeddings)\n",
    "        return all_embeddings    \n",
    "    \n",
    "class hierarchical_interest(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(hierarchical_interest, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.adj=tf.linalg.band_part(tf.ones((layers_num,layers_num)),-1,0)-1\n",
    "        self.layer_infor={}\n",
    "        for i in range(0,layers_num+1):\n",
    "            self.layer_infor[str(i)]=tf.keras.layers.Dense(self.d_model, activation=tf.keras.layers.LeakyReLU(alpha=0.01))\n",
    "        \n",
    "        self.conv_layer_dict={}\n",
    "        for i in range(0,layers_num-1):\n",
    "            self.conv_layer_dict['conv_layer_'+str(i)]=tf.nn.softmax(tf.Variable(initial_value=tf.random.truncated_normal([int((layers_num-1-i)*len_seq/(layers_num-1)),1,1,1],mean=0.5,stddev=1),name='w_'+str(i)),axis=0)\n",
    "\n",
    "        self.conv_layer_dict['conv_layer_'+str(layers_num-1)]=tf.nn.softmax(tf.Variable(initial_value=tf.random.truncated_normal([1,1,1,1],mean=0.5,stddev=1),name='w_'+str(layers_num-1)),axis=0)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, ori_all_embeddings):\n",
    "        \n",
    "        ori_all_embeddings[-1]=tf.nn.conv2d(tf.expand_dims(ori_all_embeddings[-1],-1),self.conv_layer_dict['conv_layer_0'],[1,1,1,1],'SAME')\n",
    "        ori_all_embeddings[-1]=tf.squeeze(ori_all_embeddings[-1],-1)\n",
    "        for i,element in enumerate(ori_all_embeddings):\n",
    "            #print(i)\n",
    "            if i==len(ori_all_embeddings)-1:\n",
    "                break\n",
    "            ori_all_embeddings[i]=tf.nn.conv2d(tf.expand_dims(element,-1),self.conv_layer_dict['conv_layer_'+str(layers_num-1-i)],[1,1,1,1],'SAME')\n",
    "            ori_all_embeddings[i]=tf.squeeze(ori_all_embeddings[i],-1)\n",
    "            ori_all_embeddings[i]=self.layernorm(ori_all_embeddings[i])\n",
    "        \n",
    "            \n",
    "        all_embeddings_tensor=tf.stack(ori_all_embeddings,axis=1)#[b,Layers,l,h]\n",
    "        renew_embeddings=[]\n",
    "        for i in range(len_seq):\n",
    "            layer_seq_embed=tf.gather(all_embeddings_tensor, indices=[i], axis=2)\n",
    "            #print(tf.shape(layer_seq_embed))\n",
    "            layer_seq_embed=tf.squeeze(layer_seq_embed,axis=2)\n",
    "            #layer_seq_embed=tf.squeeze(layer_seq_embed,axis=2)\n",
    "            \n",
    "            adj_l = tf.tile(tf.expand_dims(layer_seq_embed, axis=2), [1, 1, layers_num+1, 1])\n",
    "            adj_r = tf.tile(tf.expand_dims(layer_seq_embed, axis=1), [1, layers_num+1, 1, 1])\n",
    "            adj =tf.nn.leaky_relu(tf.nn.sigmoid(tf.reduce_sum(adj_l * adj_r, axis=-1)),alpha=0.01)\n",
    "            \n",
    "            adj = normalize_adj_tensor(adj, layers_num+1)\n",
    "            \n",
    "            adj=tf.multiply(adj,tf.linalg.band_part(tf.ones((layers_num+1,layers_num+1)),-1,0)-1)\n",
    "            \n",
    "            renew_embeddings.append(self.layernorm(tf.matmul(adj, layer_seq_embed)))\n",
    "        renew_embeddings_tensor=tf.stack(renew_embeddings,axis=2)\n",
    "        all_embeddings=tf.unstack(renew_embeddings_tensor,axis=1)\n",
    "        bef_sim=[]\n",
    "        aft_sim=[]\n",
    "        dif=[]\n",
    "        for i,ele in enumerate(all_embeddings):\n",
    "            sim=tf.matmul(ele,tf.transpose(ele,[0,2,1]))\n",
    "            sim=sim*(1-tf.linalg.band_part(tf.ones((len_seq,len_seq)),-1,0))\n",
    "            bef_sim.append(sim)\n",
    "            \n",
    "        for i,ele in enumerate(all_embeddings):\n",
    "            all_embeddings[i]=self.layer_infor[str(i)](all_embeddings[i])\n",
    "        for i,ele in enumerate(all_embeddings):\n",
    "            sim=tf.matmul(ele,tf.transpose(ele,[0,2,1]))\n",
    "            sim=sim*(1-tf.linalg.band_part(tf.ones((len_seq,len_seq)),-1,0))\n",
    "            aft_sim.append(sim)\n",
    "        for i,ele in enumerate(bef_sim):\n",
    "            dif.append(tf.reduce_mean(tf.nn.relu(bef_sim[i])-tf.nn.relu(aft_sim[i])))\n",
    "        all_embeddings.extend(ori_all_embeddings)\n",
    "        \n",
    "        return all_embeddings,tf.reduce_mean(dif)\n",
    "\n",
    "class CapsuleNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, seq_len, bilinear_type=0, num_interest=4, stop_grad=True):\n",
    "        super(CapsuleNetwork, self).__init__()\n",
    "        self.bilinear_type = bilinear_type\n",
    "        self.seq_len = seq_len\n",
    "        self.num_interest = num_interest\n",
    "        self.embed_dim = embed_dim\n",
    "        self.bi_dense=tf.keras.layers.Dense(hidden_size * self.num_interest, activation=None,use_bias=False)\n",
    "        self.lstm=tf.keras.layers.LSTM(self.embed_dim*2, unit_forget_bias=1.0,return_sequences=True)\n",
    "        self.stop_grad = stop_grad\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.bilinear_type >= 2:\n",
    "            self.w = self.add_weight(\n",
    "                shape=[1, self.seq_len, self.num_interest * self.embed_dim, self.embed_dim],\n",
    "                initializer='random_normal',\n",
    "                name='weights'\n",
    "            )\n",
    "\n",
    "    def call(self, hist_emb, mask):\n",
    "        if self.bilinear_type == 0:\n",
    "            hist_emb_hat = tf.tile(hist_emb, [1, 1, self.num_interest])  # (None, seq_len, num_inter * embed_dim)\n",
    "        elif self.bilinear_type == 1:\n",
    "            #outputs = self.lstm(hist_emb)\n",
    "            #output = tf.concat(outputs, axis=0)\n",
    "            #output = tf.reshape(output, (-1, self.seq_len, self.embed_dim*2))\n",
    "            #output = tf.layers.dense(output, self.embed_dim*self.num_interest, activation=None, use_bias=False)\n",
    "            hist_emb_hat = self.bi_dense(hist_emb)\n",
    "        else:\n",
    "            u = tf.expand_dims(hist_emb, axis=2)  # (None, seq_len, 1, embed_dim)\n",
    "            hist_emb_hat = tf.reduce_sum(self.w * u, axis=3)  # (None, seq_len, num_inter * embed_dim)\n",
    "        hist_emb_hat = tf.reshape(hist_emb_hat, [-1, self.seq_len, self.num_interest, self.embed_dim])\n",
    "        hist_emb_hat = tf.transpose(hist_emb_hat, [0, 2, 1, 3])  # (None, num_inter, seq_len, embed_dim)\n",
    "        hist_emb_hat = tf.reshape(hist_emb_hat, [-1, self.num_interest, self.seq_len, self.embed_dim])\n",
    "        if self.stop_grad:\n",
    "            hist_emb_iter = tf.stop_gradient(hist_emb_hat)\n",
    "        else:\n",
    "            hist_emb_iter = hist_emb_hat  # (None, num_inter, seq_len, embed_dim)\n",
    "\n",
    "        if self.bilinear_type > 0:\n",
    "            '''self.capsule_weight = self.add_weight(\n",
    "                shape=[tf.shape(hist_emb_hat)[0], self.num_interest, self.seq_len],\n",
    "                initializer=tf.zeros_initializer())'''\n",
    "            self.capsule_weight = tf.random.truncated_normal(\n",
    "                shape=[tf.shape(hist_emb_hat)[0], self.num_interest, self.seq_len],\n",
    "                stddev=1.0)\n",
    "            \n",
    "        else:\n",
    "            self.capsule_weight = tf.random.truncated_normal(\n",
    "                shape=[tf.shape(hist_emb_hat)[0], self.num_interest, self.seq_len],\n",
    "                stddev=1.0)\n",
    "        tf.stop_gradient(self.capsule_weight)\n",
    "\n",
    "        for i in range(3):\n",
    "            att_mask = tf.tile(tf.expand_dims(mask, axis=1), [1, self.num_interest, 1])  # (None, num_inter, seq_len)\n",
    "            paddings = tf.zeros_like(att_mask)\n",
    "\n",
    "            capsule_softmax_weight = tf.nn.softmax(self.capsule_weight, axis=1)  # (None, num_inter, seq_len)\n",
    "            capsule_softmax_weight = tf.where(tf.equal(att_mask, 0), paddings, capsule_softmax_weight)\n",
    "            capsule_softmax_weight = tf.expand_dims(capsule_softmax_weight, 2)  # (None, num_inter, 1, seq_len)\n",
    "\n",
    "            if i < 2:\n",
    "                interest_capsule = tf.matmul(capsule_softmax_weight, hist_emb_iter)  # (None, num_inter, 1, embed_dim)\n",
    "                cap_norm = tf.reduce_sum(tf.square(interest_capsule), -1, keepdims=True)\n",
    "                scalar_factor = cap_norm / (1 + cap_norm) / tf.sqrt(cap_norm + 1e-9)\n",
    "                interest_capsule = scalar_factor * interest_capsule\n",
    "\n",
    "                delta_weight = tf.matmul(hist_emb_iter, tf.transpose(interest_capsule, [0, 1, 3, 2]))\n",
    "                delta_weight = tf.reshape(delta_weight, [-1, self.num_interest, self.seq_len])\n",
    "                self.capsule_weight = self.capsule_weight + delta_weight\n",
    "            else:\n",
    "                interest_capsule = tf.matmul(capsule_softmax_weight, hist_emb_hat)\n",
    "                cap_norm = tf.reduce_sum(tf.square(interest_capsule), -1, True)\n",
    "                scalar_factor = cap_norm / (1 + cap_norm) / tf.sqrt(cap_norm + 1e-9)\n",
    "                interest_capsule = scalar_factor * interest_capsule\n",
    "\n",
    "        interest_capsule = tf.reshape(interest_capsule, [-1, self.num_interest, self.embed_dim])\n",
    "        return interest_capsule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49fb9d25-6709-4545-9211-898760d749eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mgnm(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(mgnm, self).__init__()\n",
    "        blocks=1\n",
    "        embed_reg=0.\n",
    "        layer_norm_eps=1e-6\n",
    "        num_heads=1\n",
    "        use_l2norm=False\n",
    "        \n",
    "        self.len_seq=len_seq\n",
    "        self.item_embedding = tf.keras.layers.Embedding(items_num+1,hidden_size,input_length=self.len_seq,\n",
    "                                                       embeddings_initializer='random_normal',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(len_seq,hidden_size,input_length=1,\n",
    "                                                       embeddings_initializer='random_normal',\n",
    "                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n",
    "        self.dropout = tf.keras.layers.Dropout(1-keep_rate)\n",
    "        \n",
    "        self.hiera=hierarchical_interest(hidden_size,0)\n",
    "        self.encoder_layer_dict={}\n",
    "        for i in range(0,2*(layers_num+1)):\n",
    "            self.encoder_layer_dict['encoder_layer_'+str(i)]=CapsuleNetwork(hidden_size, len_seq, 1, interest_num, False)\n",
    "        \n",
    "        self.multi_interest_dict={}\n",
    "        for i in range(0,2*(layers_num+1)):\n",
    "            self.multi_interest_dict['multi_interest_layer_'+str(i)+'_w1']=tf.keras.layers.Dense(hidden_size * 4, activation='tanh')\n",
    "            self.multi_interest_dict['multi_interest_layer_'+str(i)+'_w2']=tf.keras.layers.Dense(interest_num, activation=None)\n",
    "        \n",
    "        self.multi_interest_weight={}\n",
    "        for i in range(0,2*(layers_num+1)):\n",
    "            self.multi_interest_weight['multi_interest_weight_'+str(i)]=tf.nn.sigmoid(tf.Variable(initial_value=tf.random.truncated_normal([interest_num,len_seq],mean=0.5,stddev=1),\n",
    "                                                                                                  name='mul_w_'+str(i)))\n",
    "            print(tf.shape(self.multi_interest_weight['multi_interest_weight_'+str(i)]))\n",
    "        self.gru_encoder={}\n",
    "        for i in range(0,2*(layers_num+1)):\n",
    "            self.gru_encoder['gru_encoder'+str(i)] = tf.keras.layers.GRU(hidden_size)\n",
    "        \n",
    "        self.gcn=gcn_layer(hidden_size, layers_num)\n",
    "        self.score_weight=tf.keras.layers.Dense(1, activation=None)\n",
    "        # norm\n",
    "        self.use_l2norm = use_l2norm\n",
    "        \n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        seq_embed=self.item_embedding(inputs['click_seq'])\n",
    "        \n",
    "        adj_l = tf.tile(tf.expand_dims(seq_embed, axis=2), [1, 1, len_seq, 1])\n",
    "        adj_r = tf.tile(tf.expand_dims(seq_embed, axis=1), [1, len_seq, 1, 1])\n",
    "        adj = tf.nn.sigmoid(tf.reduce_sum(adj_l * adj_r, axis=-1))\n",
    "        self.adj_l1 = tf.norm(adj, ord=1)\n",
    "        \n",
    "        if 1:\n",
    "            adj = normalize_adj_tensor(adj, len_seq) \n",
    "        \n",
    "        all_embeddings=self.gcn(adj,seq_embed)\n",
    "        \n",
    "        all_embeddings,dif=self.hiera(all_embeddings)\n",
    "        \n",
    "        pos_encoding = tf.expand_dims(self.pos_embedding(tf.range(self.len_seq)), axis=0)  # (1, seq_len, embed_dim)\n",
    "        att_outputs_dict={}\n",
    "        for i,element in enumerate(all_embeddings):\n",
    "            #print(i,'/',layers_num)\n",
    "            #att_outputs_dict['embed_'+str(i)]=tf.math.reduce_mean(element,axis=1,keepdims=True)\n",
    "            att_outputs_dict['embed_'+str(i)]=element+pos_encoding\n",
    "            \n",
    "            att_outputs_dict['embed_'+str(i)] = self.dropout(att_outputs_dict['embed_'+str(i)])\n",
    "            print(tf.shape(att_outputs_dict['embed_'+str(i)]))\n",
    "        \n",
    "        for i in range(0,2*(layers_num+1)):\n",
    "            att_outputs_dict['embed_'+str(i)]=tf.expand_dims(att_outputs_dict['embed_'+str(i)],axis=1)#[b,1,l,h]\n",
    "            print(tf.shape(att_outputs_dict['embed_'+str(i)]))\n",
    "            print(tf.shape(self.multi_interest_weight['multi_interest_weight_'+str(i)]))\n",
    "            self.multi_interest_weight['multi_interest_weight_'+str(i)]=tf.expand_dims(self.multi_interest_weight['multi_interest_weight_'+str(i)],axis=0)#[1,k,l]\n",
    "            self.multi_interest_weight['multi_interest_weight_'+str(i)]=tf.expand_dims(self.multi_interest_weight['multi_interest_weight_'+str(i)],axis=-1)#[1,k,l,1]\n",
    "            print(tf.shape(self.multi_interest_weight['multi_interest_weight_'+str(i)]))\n",
    "            att_outputs_dict['embed_'+str(i)]=tf.multiply(att_outputs_dict['embed_'+str(i)],self.multi_interest_weight['multi_interest_weight_'+str(i)])#[b,k,l,h]\n",
    "            print(tf.shape(att_outputs_dict['embed_'+str(i)]))\n",
    "            att_outputs_dict['embed_'+str(i)]=tf.unstack(att_outputs_dict['embed_'+str(i)],axis=1)#k*[b,l,h]\n",
    "\n",
    "            \n",
    "        '''    \n",
    "        self.layer_interest_dict={}\n",
    "        for i in range(0,2*(layers_num+1)):\n",
    "            self.layer_interest_dict['layer_'+str(i)+'_interest']=[]\n",
    "            for j in range(len(att_outputs_dict['embed_'+str(i)])):\n",
    "                print(j,tf.shape(att_outputs_dict['embed_'+str(i)][j]))\n",
    "                interest=self.gru_encoder['gru_encoder'+str(i)](att_outputs_dict['embed_'+str(i)][j])\n",
    "                #print(tf.shape(interest))\n",
    "                self.layer_interest_dict['layer_'+str(i)+'_interest'].append(interest)#[b,h]\n",
    "                \n",
    "            self.layer_interest_dict['layer_'+str(i)+'_interest']=tf.stack(self.layer_interest_dict['layer_'+str(i)+'_interest'],1)#[b,k,h]\n",
    "            self.layer_interest_dict['layer_'+str(i)+'_interest']=tf.reshape(self.layer_interest_dict['layer_'+str(i)+'_interest'],\n",
    "                                                                                 [-1,1,interest_num,hidden_size])\n",
    "            \n",
    "        '''\n",
    "        mixed_interest=tf.stack(att_outputs_dict['embed_'+str(0)],axis=1)\n",
    "        #mixed_interest=tf.concat([self.layer_interest_dict['layer_'+str(i)+'_interest'] for i in range(0,2*(layers_num+1))],axis=1)#[b,l,n,h]\n",
    "        '''\n",
    "        pos_emb=self.item_embedding(tf.reshape(inputs['pos_item'], [-1, ]))#[b,h]\n",
    "        neg_emb=self.item_embedding(inputs['neg_item'])#[b,neg_num,h]\n",
    "        \n",
    "        pos_score=tf.zeros([tf.shape(pos_emb)[0],1])\n",
    "        neg_score=tf.zeros([tf.shape(neg_emb)[0],tf.shape(neg_emb)[1]])\n",
    "\n",
    "        for i in range(0,layers_num+1):\n",
    "            pos_score+=tf.reduce_max(tf.reduce_sum(tf.multiply(tf.squeeze(self.layer_interest_dict['layer_'+str(i)+'_interest'],axis=1),tf.expand_dims(pos_emb,axis=1))),keepdims=True)\n",
    "        \n",
    "        for i in range(0,layers_num+1):\n",
    "            neg_score+=tf.reduce_max(tf.matmul(neg_emb,tf.transpose(tf.squeeze(self.layer_interest_dict['layer_'+str(i)+'_interest'],axis=1),[0,2,1])))\n",
    "        print(tf.shape(pos_score),tf.shape(neg_score))\n",
    "        '''\n",
    "        \n",
    "        #print(tf.shape(mixed_interest))\n",
    "        if self.use_l2norm:\n",
    "            pos_emb = tf.math.l2_normalize(pos_emb, axis=-1)\n",
    "            neg_emb = tf.math.l2_normalize(neg_emb, axis=-1)\n",
    "            mixed_interest = tf.math.l2_normalize(mixed_interest, axis=-1)\n",
    "\n",
    "        \n",
    "        pos_emb=self.item_embedding(tf.reshape(inputs['pos_item'], [-1, ]))#[b,h]\n",
    "        expanded_target_item=tf.expand_dims(tf.expand_dims(pos_emb,axis=1),axis=1)#[b,1,1,h]\n",
    "        alpha=tf.reduce_max(tf.matmul(expanded_target_item,tf.transpose(mixed_interest,[0,1,3,2])),axis=-1)#[b,l,1]\n",
    "        \n",
    "        pos_score=tf.squeeze(self.score_weight(tf.transpose(alpha,[0,2,1])),axis=-1)#[b,1]\n",
    "            \n",
    "        neg_emb=self.item_embedding(inputs['neg_item'])#[b,neg_num,h]\n",
    "        expaned_neg_emb=tf.expand_dims(neg_emb,axis=1)#[b,1,neg_num,h]\n",
    "        alpha=tf.reduce_max(tf.matmul(expaned_neg_emb,tf.transpose(mixed_interest,[0,1,3,2])),axis=-1)#[b,l,neg_num]\n",
    "        \n",
    "        neg_score=tf.squeeze(self.score_weight(tf.transpose(alpha,[0,2,1])),axis=-1)#[b,neg_num]\n",
    "        \n",
    "        logits = tf.nn.softmax(tf.concat([pos_score, neg_score], axis=-1))\n",
    "        #pos_score = tf.tile(pos_score, [1, neg_score.shape[1]])\n",
    "        \n",
    "        loss = tf.reduce_mean(- tf.math.log(logits)[:,0])+tf.nn.relu(dif)\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def summary(self):\n",
    "        inputs = {\n",
    "            'click_seq': tf.keras.layers.Input(shape=(self.len_seq,), dtype=tf.int32),\n",
    "            'pos_item': tf.keras.layers.Input(shape=(), dtype=tf.int32),\n",
    "            'neg_item': tf.keras.layers.Input(shape=(1,), dtype=tf.int32)  # suppose neg_num=1\n",
    "        }\n",
    "        tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab39a04b-df32-46da-93f0-7f2749da22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr(rank, k):\n",
    "    \"\"\"Hit Rate.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: hit rate.\n",
    "    \"\"\"\n",
    "    res = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            res += 1\n",
    "    return res / len(rank)\n",
    "\n",
    "\n",
    "def mrr(rank, k):\n",
    "    \"\"\"Mean Reciprocal Rank.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: mrr.\n",
    "    \"\"\"\n",
    "    mrr = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            mrr += 1 / (r + 1)\n",
    "    return mrr / len(rank)\n",
    "\n",
    "\n",
    "def ndcg(rank, k):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: ndcg.\n",
    "    \"\"\"\n",
    "    res = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            res += 1 / np.log2(r + 2)\n",
    "    return res / len(rank)\n",
    "\n",
    "\n",
    "def eval_rank(pred_y, metric_names, k=10):\n",
    "    \"\"\"Evaluate\n",
    "        Args:\n",
    "            :param pred_y: A ndarray.\n",
    "            :param metric_names: A list like ['hr'].\n",
    "            :param k: A scalar(int).\n",
    "        :return: A result dict such as {'hr':, 'ndcg':, ...}\n",
    "    \"\"\"\n",
    "    rank = pred_y.argsort().argsort()[:, 0]\n",
    "    res_dict = {}\n",
    "    for name in metric_names:\n",
    "        if name == 'hr':\n",
    "            res = hr(rank, k)\n",
    "        elif name == 'ndcg':\n",
    "            res = ndcg(rank, k)\n",
    "        elif name == 'mrr':\n",
    "            res = mrr(rank, k)\n",
    "        else:\n",
    "            break\n",
    "        res_dict[name] = res\n",
    "    return res_dict\n",
    "\n",
    "def eval_pos_neg(model, test_data, metric_names, k=10, batch_size=None):\n",
    "    \"\"\"Evaluate the performance of Top-k recommendation algorithm.\n",
    "    Note: Test data must contain some negative samples(>= k) and one positive samples.\n",
    "    Args:\n",
    "        :param model: A model built-by tensorflow.\n",
    "        :param test_data: A dict.\n",
    "        :param metric_names: A list like ['hr'].\n",
    "        :param k: A scalar(int).\n",
    "        :param batch_size: A scalar(int).\n",
    "    :return: A result dict such as {'hr':, 'ndcg':, ...}\n",
    "    \"\"\"\n",
    "    pred_y = - model.predict(test_data, batch_size)\n",
    "    print(np.shape(pred_y))\n",
    "    return eval_rank(pred_y, metric_names, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2232c-3d17-4785-8889-ac02a07aec67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8da1e27b-4427-4a95-a808-b7f45dd7c729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\\nfrom time import time\\nstrategy = tf.distribute.MirroredStrategy()\\nbatch_size=batch_size*strategy.num_replicas_in_sync\\nwith strategy.scope():\\n    model = mgnm()\\n    model.summary()\\n    optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\\n    model.compile(optimizer=optimizer)\\n\\nmodel.fit(\\n            x=train_data,\\n            epochs=100,\\n            batch_size=batch_size,\\n        validation_data=val_data,\\n    callbacks=[callback])\\nt2=time()\\neval_dict = eval_pos_neg(model, test_data, ['hr', 'mrr', 'ndcg'], 10, batch_size)\\nprint('Evaluate [%.1f s]: HR = %.4f, MRR = %.4f, NDCG = %.4f'\\n              % (time() - t2,eval_dict['hr'], eval_dict['mrr'], eval_dict['ndcg']))\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "from time import time\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "batch_size=batch_size*strategy.num_replicas_in_sync\n",
    "with strategy.scope():\n",
    "    model = mgnm()\n",
    "    model.summary()\n",
    "    optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    model.compile(optimizer=optimizer)\n",
    "\n",
    "model.fit(\n",
    "            x=train_data,\n",
    "            epochs=100,\n",
    "            batch_size=batch_size,\n",
    "        validation_data=val_data,\n",
    "    callbacks=[callback])\n",
    "t2=time()\n",
    "eval_dict = eval_pos_neg(model, test_data, ['hr', 'mrr', 'ndcg'], 10, batch_size)\n",
    "print('Evaluate [%.1f s]: HR = %.4f, MRR = %.4f, NDCG = %.4f'\n",
    "              % (time() - t2,eval_dict['hr'], eval_dict['mrr'], eval_dict['ndcg']))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0933a7-b926-4371-a008-0e992c04d096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "WARNING:tensorflow:From /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 10, 64], name='tf.compat.v1.shape/Shape:0', description=\"created by layer 'tf.compat.v1.shape'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 10, 64], name='tf.compat.v1.shape_1/Shape:0', description=\"created by layer 'tf.compat.v1.shape_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 10, 64], name='tf.compat.v1.shape_2/Shape:0', description=\"created by layer 'tf.compat.v1.shape_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 10, 64], name='tf.compat.v1.shape_3/Shape:0', description=\"created by layer 'tf.compat.v1.shape_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 10, 64], name='tf.compat.v1.shape_4/Shape:0', description=\"created by layer 'tf.compat.v1.shape_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 10, 64], name='tf.compat.v1.shape_5/Shape:0', description=\"created by layer 'tf.compat.v1.shape_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 10, 64], name='tf.compat.v1.shape_6/Shape:0', description=\"created by layer 'tf.compat.v1.shape_6'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 10, 64], name='tf.compat.v1.shape_7/Shape:0', description=\"created by layer 'tf.compat.v1.shape_7'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 1, 10, 64], name='tf.compat.v1.shape_8/Shape:0', description=\"created by layer 'tf.compat.v1.shape_8'\")\n",
      "tf.Tensor([ 3 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 1  3 10  1], shape=(4,), dtype=int32)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 3, 10, 64], name='tf.compat.v1.shape_9/Shape:0', description=\"created by layer 'tf.compat.v1.shape_9'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 1, 10, 64], name='tf.compat.v1.shape_10/Shape:0', description=\"created by layer 'tf.compat.v1.shape_10'\")\n",
      "tf.Tensor([ 3 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 1  3 10  1], shape=(4,), dtype=int32)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 3, 10, 64], name='tf.compat.v1.shape_11/Shape:0', description=\"created by layer 'tf.compat.v1.shape_11'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 1, 10, 64], name='tf.compat.v1.shape_12/Shape:0', description=\"created by layer 'tf.compat.v1.shape_12'\")\n",
      "tf.Tensor([ 3 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 1  3 10  1], shape=(4,), dtype=int32)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 3, 10, 64], name='tf.compat.v1.shape_13/Shape:0', description=\"created by layer 'tf.compat.v1.shape_13'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 1, 10, 64], name='tf.compat.v1.shape_14/Shape:0', description=\"created by layer 'tf.compat.v1.shape_14'\")\n",
      "tf.Tensor([ 3 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 1  3 10  1], shape=(4,), dtype=int32)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 3, 10, 64], name='tf.compat.v1.shape_15/Shape:0', description=\"created by layer 'tf.compat.v1.shape_15'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 1, 10, 64], name='tf.compat.v1.shape_16/Shape:0', description=\"created by layer 'tf.compat.v1.shape_16'\")\n",
      "tf.Tensor([ 3 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 1  3 10  1], shape=(4,), dtype=int32)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 3, 10, 64], name='tf.compat.v1.shape_17/Shape:0', description=\"created by layer 'tf.compat.v1.shape_17'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 1, 10, 64], name='tf.compat.v1.shape_18/Shape:0', description=\"created by layer 'tf.compat.v1.shape_18'\")\n",
      "tf.Tensor([ 3 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 1  3 10  1], shape=(4,), dtype=int32)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 3, 10, 64], name='tf.compat.v1.shape_19/Shape:0', description=\"created by layer 'tf.compat.v1.shape_19'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 1, 10, 64], name='tf.compat.v1.shape_20/Shape:0', description=\"created by layer 'tf.compat.v1.shape_20'\")\n",
      "tf.Tensor([ 3 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 1  3 10  1], shape=(4,), dtype=int32)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 3, 10, 64], name='tf.compat.v1.shape_21/Shape:0', description=\"created by layer 'tf.compat.v1.shape_21'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 1, 10, 64], name='tf.compat.v1.shape_22/Shape:0', description=\"created by layer 'tf.compat.v1.shape_22'\")\n",
      "tf.Tensor([ 3 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 1  3 10  1], shape=(4,), dtype=int32)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 3, 10, 64], name='tf.compat.v1.shape_23/Shape:0', description=\"created by layer 'tf.compat.v1.shape_23'\")\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape (TFOpLambda)         (None,)              0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           multiple             218688      input_1[0][0]                    \n",
      "                                                                 tf.reshape[0][0]                 \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (None, 10, 1, 64)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (None, 1, 10, 64)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.tile (TFOpLambda)            (None, 10, 10, 64)   0           tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.tile_1 (TFOpLambda)          (None, 10, 10, 64)   0           tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 10, 10, 64)   0           tf.tile[0][0]                    \n",
      "                                                                 tf.tile_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum (TFOpLambda) (None, 10, 10)       0           tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sigmoid (TFOpLambda)    (None, 10, 10)       0           tf.math.reduce_sum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 10, 10)       0           tf.math.sigmoid[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_1 (TFOpLambd (None, 10)           0           tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow (TFOpLambda)        (None, 10)           0           tf.math.reduce_sum_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.is_inf (TFOpLambda)     (None, 10)           0           tf.math.pow[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.zeros_like (TFOpLambda)      (None, 10)           0           tf.math.pow[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.where (TFOpLambda)           (None, 10)           0           tf.math.is_inf[0][0]             \n",
      "                                                                 tf.zeros_like[0][0]              \n",
      "                                                                 tf.math.pow[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.diag (TFOpLambda)     (None, 10, 10)       0           tf.where[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.matmul (TFOpLambda)   (None, 10, 10)       0           tf.linalg.diag[0][0]             \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "gcn_layer (gcn_layer)           [(None, 10, 64), (No 4160        tf.linalg.matmul[0][0]           \n",
      "                                                                 embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hierarchical_interest (hierarch ([(None, 10, 64), (N 16768       gcn_layer[0][0]                  \n",
      "                                                                 gcn_layer[0][1]                  \n",
      "                                                                 gcn_layer[0][2]                  \n",
      "                                                                 gcn_layer[0][3]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 10, 64)       0           hierarchical_interest[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 10, 64)       0           tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_2 (TFOpLambda)   (None, 1, 10, 64)    0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, 3, 10, 64)    0           tf.expand_dims_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.unstack (TFOpLambda)         [(None, 10, 64), (No 0           tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_10 (TFOpLambda)  (None, 1, 64)        0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.stack (TFOpLambda)           (None, 3, 10, 64)    0           tf.unstack[0][0]                 \n",
      "                                                                 tf.unstack[0][1]                 \n",
      "                                                                 tf.unstack[0][2]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_11 (TFOpLambda)  (None, 1, 1, 64)     0           tf.expand_dims_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose (TFOpLam (None, 3, 64, 10)    0           tf.stack[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_12 (TFOpLambda)  (None, 1, 1, 64)     0           embedding[2][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_2 (TFOpL (None, 3, 64, 10)    0           tf.stack[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.matmul_1 (TFOpLambda) (None, 3, 1, 10)     0           tf.expand_dims_11[0][0]          \n",
      "                                                                 tf.compat.v1.transpose[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.matmul_2 (TFOpLambda) (None, 3, 1, 10)     0           tf.expand_dims_12[0][0]          \n",
      "                                                                 tf.compat.v1.transpose_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max (TFOpLambda) (None, 3, 1)         0           tf.linalg.matmul_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_max_1 (TFOpLambd (None, 3, 1)         0           tf.linalg.matmul_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_1 (TFOpL (None, 1, 3)         0           tf.math.reduce_max[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_3 (TFOpL (None, 1, 3)         0           tf.math.reduce_max_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1, 1)         4           tf.compat.v1.transpose_1[0][0]   \n",
      "                                                                 tf.compat.v1.transpose_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze (TFOpLambd (None, 1)            0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze_1 (TFOpLam (None, 1)            0           dense_29[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 2)            0           tf.compat.v1.squeeze[0][0]       \n",
      "                                                                 tf.compat.v1.squeeze_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.softmax (TFOpLambda)      (None, 2)            0           tf.concat[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 239,620\n",
      "Trainable params: 239,620\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Tensor(\"mgnm/Shape:0\", shape=(3,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_1:0\", shape=(3,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_2:0\", shape=(3,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_3:0\", shape=(3,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_4:0\", shape=(3,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_5:0\", shape=(3,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_6:0\", shape=(3,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_7:0\", shape=(3,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_8:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_9:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_10:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_11:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_12:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_13:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_14:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_15:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_16:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_17:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_18:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_19:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_20:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_21:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_22:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_23:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_24:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_25:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_26:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_27:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_28:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_29:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_30:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_31:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_32:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_33:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_34:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_35:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_36:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_37:0\", shape=(4,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_38:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"mgnm/Shape_39:0\", shape=(6,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "INFO:tensorflow:Error reported to Coordinator: in user code:\n",
      "\n",
      "    /tmp/ipykernel_25695/1188701180.py:121 call  *\n",
      "        alpha=tf.reduce_max(tf.matmul(expanded_target_item,tf.transpose(mixed_interest,[0,1,3,2])),axis=-1)#[b,l,1]\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n",
      "        return target(*args, **kwargs)\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2227 transpose_v2\n",
      "        return transpose(a=a, perm=perm, name=name, conjugate=conjugate)\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n",
      "        return target(*args, **kwargs)\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2308 transpose\n",
      "        return transpose_fn(a, perm, name=name)\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:11652 transpose\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n",
      "        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n",
      "        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3557 _create_op_internal\n",
      "        ret = Operation(\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n",
      "        self._c_op = _create_c_op(self._graph, node_def, inputs,\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n",
      "        raise ValueError(str(e))\n",
      "\n",
      "    ValueError: Dimension must be 6 but is 4 for '{{node mgnm/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](mgnm/stack, mgnm/transpose/perm)' with input shapes: [1,1,3,10,10,64], [4].\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 334, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 695, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "ValueError: in user code:\n",
      "\n",
      "    /tmp/ipykernel_25695/1188701180.py:121 call  *\n",
      "        alpha=tf.reduce_max(tf.matmul(expanded_target_item,tf.transpose(mixed_interest,[0,1,3,2])),axis=-1)#[b,l,1]\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n",
      "        return target(*args, **kwargs)\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2227 transpose_v2\n",
      "        return transpose(a=a, perm=perm, name=name, conjugate=conjugate)\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n",
      "        return target(*args, **kwargs)\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2308 transpose\n",
      "        return transpose_fn(a, perm, name=name)\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:11652 transpose\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n",
      "        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n",
      "        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3557 _create_op_internal\n",
      "        ret = Operation(\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n",
      "        self._c_op = _create_c_op(self._graph, node_def, inputs,\n",
      "    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n",
      "        raise ValueError(str(e))\n",
      "\n",
      "    ValueError: Dimension must be 6 but is 4 for '{{node mgnm/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](mgnm/stack, mgnm/transpose/perm)' with input shapes: [1,1,3,10,10,64], [4].\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /tmp/ipykernel_25695/1188701180.py:121 call  *\n        alpha=tf.reduce_max(tf.matmul(expanded_target_item,tf.transpose(mixed_interest,[0,1,3,2])),axis=-1)#[b,l,1]\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2227 transpose_v2\n        return transpose(a=a, perm=perm, name=name, conjugate=conjugate)\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2308 transpose\n        return transpose_fn(a, perm, name=name)\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:11652 transpose\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3557 _create_op_internal\n        ret = Operation(\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimension must be 6 but is 4 for '{{node mgnm/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](mgnm/stack, mgnm/transpose/perm)' with input shapes: [1,1,3,10,10,64], [4].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25695/409313593.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /tmp/ipykernel_25695/1188701180.py:121 call  *\n        alpha=tf.reduce_max(tf.matmul(expanded_target_item,tf.transpose(mixed_interest,[0,1,3,2])),axis=-1)#[b,l,1]\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2227 transpose_v2\n        return transpose(a=a, perm=perm, name=name, conjugate=conjugate)\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2308 transpose\n        return transpose_fn(a, perm, name=name)\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:11652 transpose\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3557 _create_op_internal\n        ret = Operation(\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimension must be 6 but is 4 for '{{node mgnm/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](mgnm/stack, mgnm/transpose/perm)' with input shapes: [1,1,3,10,10,64], [4].\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "batch_size=batch_size*strategy.num_replicas_in_sync\n",
    "with strategy.scope():\n",
    "    model = mgnm()\n",
    "    model.summary()\n",
    "    optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    model.compile(optimizer=optimizer)\n",
    "best_eval_dict={'hr':0.,'mrr':0.,'ndcg':0.}\n",
    "eval_patience={'hr':0.,'mrr':0.,'ndcg':0.}\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    t1 = time()\n",
    "    model.fit(\n",
    "            x=train_data,\n",
    "            epochs=1,\n",
    "            batch_size=batch_size,\n",
    "        validation_data=val_data)\n",
    "    t2 = time()\n",
    "    eval_dict = eval_pos_neg(model, val_data, ['hr', 'mrr', 'ndcg'], 1, batch_size)\n",
    "    print('Iteration %d Fit [%.1f s], Evaluate [%.1f s]: HR = %.4f, MRR = %.4f, NDCG = %.4f'\n",
    "              % (epoch, t2 - t1, time() - t2, eval_dict['hr'], eval_dict['mrr'], eval_dict['ndcg']))\n",
    "    for i in eval_dict.keys():\n",
    "        if eval_dict[i]>=best_eval_dict[i]:\n",
    "            best_eval_dict[i]=eval_dict[i]\n",
    "        else:\n",
    "            print(i,'drop 1 time')\n",
    "            eval_patience[i]+=1\n",
    "    if (np.array(list(eval_patience.values()))>=3).any():\n",
    "        print('Trigger early stopping')\n",
    "        break\n",
    "eval_dict = eval_pos_neg(model, test_data, ['hr', 'mrr', 'ndcg'], 10, batch_size)\n",
    "print('Iteration %d Fit [%.1f s], Evaluate [%.1f s]: HR = %.4f, MRR = %.4f, NDCG = %.4f'\n",
    "              % (epoch, t2 - t1, time() - t2, eval_dict['hr'], eval_dict['mrr'], eval_dict['ndcg']))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584bb961-d3f6-408e-96aa-9ab37eb19a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K=5 Iteration 6 Fit [20.1 s], Evaluate [3.5 s]: HR = 0.1530, MRR = 0.0543, NDCG = 0.0772\n",
    "#K=4 Iteration 6 Fit [20.3 s], Evaluate [3.5 s]: HR = 0.1591, MRR = 0.0600, NDCG = 0.0830\n",
    "#K=3 Iteration 6 Fit [21.1 s], Evaluate [3.5 s]: HR = 0.1561, MRR = 0.0584, NDCG = 0.0811\n",
    "#K=2 Iteration 9 Fit [20.2 s], Evaluate [3.5 s]: HR = 0.1503, MRR = 0.0568, NDCG = 0.0784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c812613-51c5-4c39-9500-8d103d4b5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K=3 Iteration 7 Fit [27.0 s], Evaluate [4.8 s]: HR = 0.1699, MRR = 0.0704, NDCG = 0.0935\n",
    "#K=3 Iteration 7 Fit [27.7 s], Evaluate [4.9 s]: HR = 0.1737, MRR = 0.0673, NDCG = 0.0921\n",
    "#K=3 without hip Iteration 9 Fit [25.5 s], Evaluate [4.7 s]: HR = 0.1603, MRR = 0.0612, NDCG = 0.0842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bdc01-2b60-41ab-80f2-3a04e765b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxlen=1000\n",
    "#len_seq=100\n",
    "#batch_size=512\n",
    "#epoch_num=100\n",
    "#hidden_size=64\n",
    "#keep_rate=0.9\n",
    "#layers_num=10\n",
    "#num_heads=1\n",
    "#interest_num=3\n",
    "#kernel_size=4\n",
    "#transformerencoder=0\n",
    "#neg_num=4\n",
    "#test_neg_num=100\n",
    "#Iteration 10 Fit [58.3 s], Evaluate [0.3 s]: HR = 0.2941, MRR = 0.0970, NDCG = 0.1424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79481e70-93d3-41d1-8e5b-558125f5da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def train_model(model,ds_train,ds_valid,epoches):\n",
    "\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        model.reset_metrics()\n",
    "        \n",
    "        # 在后期降低学习率\n",
    "        if epoch == 5000:\n",
    "            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n",
    "            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n",
    "        \n",
    "        for x, y in ds_train:\n",
    "            train_result = model.train_on_batch(x, y)\n",
    "\n",
    "        for x, y in ds_valid:\n",
    "            valid_result = model.test_on_batch(x, y,reset_metrics=False)\n",
    "            \n",
    "        if epoch%100 ==0:\n",
    "            tf.print(\"epoch = \",epoch)\n",
    "            print(\"train:\",dict(zip(model.metrics_names,train_result)))\n",
    "            print(\"valid:\",dict(zip(model.metrics_names,valid_result)))\n",
    "            print(\"\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a286ada-d7d0-4faf-a82c-43c935a51ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_model(model,ds_train,ds_test,800)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c088fc-21ff-419e-9f3e-8eab811eb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "params = tf.constant([[[0, 1.0, 2.0],\n",
    "                      [10.0, 11.0, 12.0],\n",
    "                      [20.0, 21.0, 22.0],\n",
    "                      [30.0, 31.0, 32.0]],\n",
    "                      \n",
    "                      [[0, 1.0, 2.0],\n",
    "                      [10.0, 11.0, 12.0],\n",
    "                      [20.0, 21.0, 22.0],\n",
    "                      [30.0, 31.0, 32.0]]])\n",
    "tf.gather(params, indices=[1], axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408a013-4001-418d-ae02-fcf0d0a6c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "a=tf.constant(np.random.random((5,3,4)))\n",
    "c=tf.unstack(a,axis=0)\n",
    "for i in c:\n",
    "    print(i)\n",
    "    print(np.corrcoef(i,i)[0:5,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13b04d-6f23-4429-a3d2-45eeed18329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef([[0.50142582 0.02288794 0.9867883  0.06646458]\n",
    " [0.71259711 0.73245141 0.96514089 0.75292946]\n",
    " [0.41503649 0.73406449 0.47345004 0.33112608]],[[0.50142582 0.02288794 0.9867883  0.06646458]\n",
    " [0.71259711 0.73245141 0.96514089 0.75292946]\n",
    " [0.41503649 0.73406449 0.47345004 0.33112608]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb25921-1c2c-4672-b0e3-53c84d58cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean([[0, 1.0, 2.0],\n",
    "                      [10.0, 11.0, 12.0],\n",
    "                      [20.0, 21.0, 22.0],\n",
    "                      [30.0, 31.0, 32.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e8acf-dc49-42c1-bd34-28494e7e7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "a=tf.constant(np.random.random((5,1,4,2)))\n",
    "b=tf.constant(np.random.random((1,3,4,1)))\n",
    "print(tf.shape(a*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73811876-3e18-4267-b144-7d12d9c1928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=tf.unstack(a*b,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925514c4-c2a6-40a5-8c92-a899dc33e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d7b7d-4c02-4300-8595-8eae1d829fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.shape(c[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b276a-d3b1-4e58-b30e-6d1620cbedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.nn.sigmoid(tf.Variable(initial_value=tf.random.truncated_normal([3,10],mean=0.5,stddev=1),name='mul_w_'+str(i)))\n",
    "a=tf.expand_dims(a,axis=0)#[1,k,l]\n",
    "a=tf.expand_dims(a,axis=-1)#[1,k,l,1]\n",
    "b=tf.nn.sigmoid(tf.Variable(initial_value=tf.random.truncated_normal([64,10,8],mean=0.5,stddev=1),name='12'+str(i)))\n",
    "b=tf.expand_dims(b,axis=1)\n",
    "print(tf.shape(a),tf.shape(b*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e6a03-bc2b-4d0c-8957-d24073cfb6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
