{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd98390-6cf1-4188-8828-d9bbbeea1bd6",
   "metadata": {
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1666147022217,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "3dd98390-6cf1-4188-8828-d9bbbeea1bd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 11:10:05.359076: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['AUTOGRAPH_VERBOSITY'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c3b8cca-95eb-4e76-af74-8467884552ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1666147022792,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "4c3b8cca-95eb-4e76-af74-8467884552ef",
    "outputId": "498f1488-239e-4190-b99c-99e8f8b099a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a9ec17-4926-453d-8ca9-9113908b350d",
   "metadata": {
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1666147041157,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "b4a9ec17-4926-453d-8ca9-9113908b350d"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/root/autodl-tmp/workspace/datasets/kuairec/small_matrix.pkl', 'rb') as f:    \n",
    "    user_dict = pickle.load(f)\n",
    "items_num=max(max(user_dict.values()))\n",
    "maxlen=100\n",
    "len_seq=50\n",
    "batch_size=512\n",
    "epoch_num=20\n",
    "hidden_size=64\n",
    "keep_rate=0.9\n",
    "layers_num=2\n",
    "num_interest=1\n",
    "neg_num=4\n",
    "test_neg_num=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91fd2c73-3e4f-449e-aef3-88288de30804",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666147041158,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "91fd2c73-3e4f-449e-aef3-88288de30804"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def sample(user_dict,maxlen,len_seq):\n",
    "    train_set=[]\n",
    "    train_val_set=[]\n",
    "    val_set=[]\n",
    "    test_set=[]\n",
    "    for u in user_dict.keys():\n",
    "        idx=0\n",
    "        hist=user_dict[u]        \n",
    "        hist=hist[-maxlen:-2]\n",
    "        #print(hist)\n",
    "        for i in range(1,len(hist)):\n",
    "            seq = np.zeros([len_seq], dtype=np.int32)\n",
    "            #print(hist[0:i])\n",
    "            seq[max(0,len_seq+idx-1):]=hist[max(0,i-len_seq):i]\n",
    "            idx+=-1\n",
    "            nxt = hist[i]\n",
    "            #print((u,seq,nxt))\n",
    "            train_set.append((u,list(seq),nxt))\n",
    "            #print(seq)\n",
    "        train_val_set.append((u,list(seq),nxt))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-2:-2]\n",
    "        nxt = user_dict[u][-2]\n",
    "        val_set.append((u,list(seq),nxt))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-1:-1]\n",
    "        nxt = user_dict[u][-1]\n",
    "        test_set.append((u,list(seq),nxt))\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    return train_set,test_set,val_set,train_val_set\n",
    "            \n",
    "def non_zero_sample(user_dict,maxlen,len_seq):\n",
    "    train_set=[]\n",
    "    test_set=[]\n",
    "    val_set=[]\n",
    "    train_val_set=[]\n",
    "    for u in user_dict.keys():\n",
    "        idx=0\n",
    "        hist=user_dict[u]        \n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        #print(np.shape(seq[-maxlen:]))\n",
    "        while(len(hist)<maxlen):\n",
    "            hist.insert(0,0)\n",
    "        #print(len(hist),hist)\n",
    "        hist=hist[-maxlen:-2]\n",
    "        for i in range(len_seq,len(hist)):\n",
    "            seq = np.zeros([len_seq], dtype=np.int32)\n",
    "            #rint(hist[max(0,i-len_seq):i])\n",
    "            seq=hist[max(0,i-len_seq):i]\n",
    "            idx+=-1\n",
    "            nxt = hist[i]\n",
    "            #print((u,seq,nxt))\n",
    "            neg_item = [random.randint(1, items_num) for _ in range(neg_num)]\n",
    "            train_set.append((u,list(seq),nxt,neg_item))\n",
    "            #print(seq)\n",
    "        #print(np.shape(hist[0:len(hist)-1]))\n",
    "        train_val_set.append((u,list(seq),nxt))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-2:-2]\n",
    "        nxt = user_dict[u][-2]\n",
    "        neg_item = [random.randint(1, items_num) for _ in range(neg_num)]\n",
    "        val_set.append((u,list(seq),nxt,neg_item))\n",
    "        seq = np.zeros([len_seq], dtype=np.int32)\n",
    "        seq=hist[-len_seq-1:-1]\n",
    "        nxt = user_dict[u][-1]\n",
    "        neg_item = [random.randint(1, items_num) for _ in range(test_neg_num)]\n",
    "        test_set.append((u,list(seq),nxt,neg_item))\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    return train_set,test_set ,val_set,train_val_set   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "904dd44f-cae8-4e03-a8f7-a76fbffae7ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10644,
     "status": "ok",
     "timestamp": 1666147051799,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "904dd44f-cae8-4e03-a8f7-a76fbffae7ed",
    "outputId": "ed0c9e6f-4f87-4948-8e74-586ec0c0f5ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67728 1411 1411 1411\n"
     ]
    }
   ],
   "source": [
    "train_set,test_set,val_set,train_val_set=non_zero_sample(user_dict,maxlen,len_seq)\n",
    "print(len(train_set),len(test_set),len(val_set),len(train_val_set))\n",
    "users_num=len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881ce4c1-a8b0-4c59-98db-5c6df6ae46a1",
   "metadata": {
    "executionInfo": {
     "elapsed": 7653,
     "status": "ok",
     "timestamp": 1666147060872,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "881ce4c1-a8b0-4c59-98db-5c6df6ae46a1"
   },
   "outputs": [],
   "source": [
    "users, click_seqs, pos_items,neg_items = zip(*train_set)\n",
    "train_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}\n",
    "users, click_seqs, pos_items,neg_items = zip(*test_set)\n",
    "test_data = {'click_seq': np.array(click_seqs), 'pos_item': np.array(pos_items),'neg_item':np.array(neg_items)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32879739-778f-4a8b-aabe-f2b701d2253b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1666147060873,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "32879739-778f-4a8b-aabe-f2b701d2253b",
    "outputId": "aee1763d-8d01-4cd4-dc71-45ce49bb75d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67728, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_data['neg_item']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df82d54-78b8-48b8-84b9-ad4896e2236a",
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1666147060873,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "1df82d54-78b8-48b8-84b9-ad4896e2236a"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Attention Mechanism Function.\n",
    "    Args:\n",
    "        :param q: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param k: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param v: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param mask: A 3d/4d tensor with shape of (None, ..., seq_len, 1)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mat_qk = tf.matmul(q, k, transpose_b=True)  # (None, seq_len, seq_len)\n",
    "    # Scaled\n",
    "    dk = tf.cast(k.shape[-1], dtype=tf.float32)\n",
    "    scaled_att_logits = mat_qk / tf.sqrt(dk)\n",
    "\n",
    "    paddings = tf.ones_like(scaled_att_logits) * (-2 ** 32 + 1)  # (None, seq_len, seq_len)\n",
    "    if mask!=None:\n",
    "        outputs = tf.where(tf.equal(mask, tf.zeros_like(mask)), paddings, scaled_att_logits)  # (None, seq_len, seq_len)\n",
    "    else:\n",
    "        outputs=scaled_att_logits\n",
    "    # softmax\n",
    "    outputs = tf.nn.softmax(logits=outputs)  # (None, seq_len, seq_len)\n",
    "    outputs = tf.matmul(outputs, v)  # (None, seq_len, dim)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def split_heads(x, seq_len, num_heads, depth):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    Args:\n",
    "        :param x: A Tensor with shape of [batch_size, seq_len, num_heads * depth]\n",
    "        :param seq_len: A scalar(int).\n",
    "        :param num_heads: A scalar(int).\n",
    "        :param depth: A scalar(int).\n",
    "    :return: A tensor with shape of [batch_size, num_heads, seq_len, depth]\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (-1, seq_len, num_heads, depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1382056-672c-4683-a46a-dca40cd582b2",
   "metadata": {
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1666147079765,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "e1382056-672c-4683-a46a-dca40cd582b2"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"Multi Head Attention Mechanism.\n",
    "        Args:\n",
    "            :param d_model: A scalar. The self-attention hidden size.\n",
    "            :param num_heads: A scalar. Number of heads. If num_heads == 1, the layer is a single self-attention layer.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model, activation=None)\n",
    "        self.wk = tf.keras.layers.Dense(d_model, activation=None)\n",
    "        self.wv = tf.keras.layers.Dense(d_model, activation=None)\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "        q = self.wq(q)  # (None, seq_len, d_model)\n",
    "        k = self.wk(k)  # (None, seq_len, d_model)\n",
    "        v = self.wv(v)  # (None, seq_len, d_model)\n",
    "        # split d_model into num_heads * depth\n",
    "        seq_len, d_model = q.shape[1], q.shape[2]\n",
    "        q = split_heads(q, seq_len, self.num_heads, q.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        k = split_heads(k, seq_len, self.num_heads, k.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        v = split_heads(v, seq_len, self.num_heads, v.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        # mask\n",
    "        if mask!=None:\n",
    "            mask = tf.tile(tf.expand_dims(mask, axis=1), [1, self.num_heads, 1, 1])  # (None, num_heads, seq_len, 1)\n",
    "        # attention\n",
    "        scaled_attention = scaled_dot_product_attention(q, k, v, mask)  # (None, num_heads, seq_len, d_model // num_heads)\n",
    "        # reshape\n",
    "        outputs = tf.reshape(tf.transpose(scaled_attention, [0, 2, 1, 3]), [-1, seq_len, d_model])  # (None, seq_len, d_model)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class FFN(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_unit, d_model):\n",
    "        \"\"\"Feed Forward Network.\n",
    "        Args:\n",
    "            :param hidden_unit: A scalar.\n",
    "            :param d_model: A scalar.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(FFN, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(filters=hidden_unit, kernel_size=1, activation='relu', use_bias=True)\n",
    "        self.conv2 = tf.keras.layers.Conv1D(filters=d_model, kernel_size=1, activation=None, use_bias=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        output = self.conv2(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads=1, ffn_hidden_unit=128, dropout=0., layer_norm_eps=1e-6):\n",
    "        \"\"\"Encoder Layer.\n",
    "        Args:\n",
    "            :param d_model: A scalar. The self-attention hidden size.\n",
    "            :param num_heads: A scalar. Number of heads.\n",
    "            :param ffn_hidden_unit: A scalar. Number of hidden unit in FFN\n",
    "            :param dropout: A scalar. Number of dropout.\n",
    "            :param layer_norm_eps: A scalar. Small float added to variance to avoid dividing by zero.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FFN(ffn_hidden_unit, d_model)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, mask= inputs\n",
    "        # self-attention\n",
    "        att_out = self.mha(x, x, x, mask)  # (None, seq_len, d_model)\n",
    "        att_out = self.dropout1(att_out)\n",
    "        # residual add\n",
    "        out1 = self.layernorm1(x + att_out)  # (None, seq_len, d_model)\n",
    "        # ffn\n",
    "        ffn_out = self.ffn(out1)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        # residual add\n",
    "        out2 = self.layernorm2(out1 + ffn_out)  # (None, seq_len, d_model)\n",
    "        return out2\n",
    "\n",
    "class hierarchical_interest(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"Multi Head Attention Mechanism.\n",
    "        Args:\n",
    "            :param d_model: A scalar. The self-attention hidden size.\n",
    "            :param num_heads: A scalar. Number of heads. If num_heads == 1, the layer is a single self-attention layer.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(hierarchical_interest, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.layer_1=tf.keras.layers.Conv1D(self.d_model,10,padding='same',input_shape=[len_seq,hidden_size])\n",
    "        self.layer_2=tf.keras.layers.Conv1D(self.d_model,5,padding='same',input_shape=[len_seq,hidden_size])\n",
    "        self.layer_3=tf.keras.layers.Conv1D(self.d_model,1,padding='same',input_shape=[len_seq,hidden_size])\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    def call(self, x):\n",
    "        interest_1 = self.layer_1(x)  # (None, len_seq, d_model)\n",
    "        interest_2 = self.layer_2(x)  # (None, len_seq, d_model)\n",
    "        interest_3 = self.layer_3(x)  # (None, len_seq, d_model)\n",
    "        interest_2=interest_2-scaled_dot_product_attention(interest_2,interest_1,interest_1,None)\n",
    "        interest_3=interest_3-scaled_dot_product_attention(interest_3,interest_1,interest_1,None)\n",
    "        interest_1=self.layernorm(interest_1)\n",
    "        interest_2=self.layernorm(interest_2)\n",
    "        interest_3=self.layernorm(interest_3)\n",
    "        return interest_1,interest_2,interest_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49fb9d25-6709-4545-9211-898760d749eb",
   "metadata": {
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1666147141304,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "49fb9d25-6709-4545-9211-898760d749eb"
   },
   "outputs": [],
   "source": [
    "class sasrec(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(sasrec, self).__init__()\n",
    "        blocks=1\n",
    "        embed_reg=0.\n",
    "        layer_norm_eps=1e-6\n",
    "        num_heads=1\n",
    "        use_l2norm=False\n",
    "        \n",
    "        self.len_seq=len_seq\n",
    "        self.item_embedding = tf.keras.layers.Embedding(items_num+1,hidden_size,input_length=self.len_seq,\n",
    "                                                       embeddings_initializer='random_normal',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(len_seq,hidden_size,input_length=1,\n",
    "                                                       embeddings_initializer='random_normal',\n",
    "                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n",
    "        self.dropout = tf.keras.layers.Dropout(1-keep_rate)\n",
    "        self.hiera=hierarchical_interest(hidden_size,0)\n",
    "        self.encoder_layer_1 = [TransformerEncoder(hidden_size, num_heads, 128,\n",
    "                                                 (1-keep_rate), layer_norm_eps) for _ in range(blocks)]\n",
    "        self.encoder_layer_2 = [TransformerEncoder(hidden_size, num_heads, 128,\n",
    "                                                 (1-keep_rate), layer_norm_eps) for _ in range(blocks)]\n",
    "        self.encoder_layer_3= [TransformerEncoder(hidden_size, num_heads, 128,\n",
    "                                                 (1-keep_rate), layer_norm_eps) for _ in range(blocks)]\n",
    "                                                 \n",
    "        # norm\n",
    "        self.use_l2norm = use_l2norm\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        seq_embed=self.item_embedding(inputs['click_seq'])\n",
    "        pos_encoding = tf.expand_dims(self.pos_embedding(tf.range(self.len_seq)), axis=0)  # (1, seq_len, embed_dim)\n",
    "        seq_embed += pos_encoding\n",
    "\n",
    "        seq_embed = self.dropout(seq_embed)\n",
    "        \n",
    "        interest_1,interest_2,interest_3=self.hiera(seq_embed)\n",
    "\n",
    "        att_outputs_1=interest_1\n",
    "        for block in self.encoder_layer_1:\n",
    "            att_outputs_1 = block([att_outputs_1,None])  # (None, seq_len, embed_dim)\n",
    "        \n",
    "        att_outputs_2=interest_2\n",
    "        for block in self.encoder_layer_2:\n",
    "            att_outputs_2= block([att_outputs_2,None])  # (None, seq_len, embed_dim)\n",
    "        \n",
    "        att_outputs_3=interest_3\n",
    "        for block in self.encoder_layer_3:\n",
    "            att_outputs_3 = block([att_outputs_3,None])  # (None, seq_len, embed_dim)\n",
    "        \n",
    "\n",
    "        interest_1 = tf.slice(att_outputs_1, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n",
    "        interest_2 = tf.slice(att_outputs_2, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n",
    "        interest_3 = tf.slice(att_outputs_3, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n",
    "        \n",
    "        mixed_interest=tf.concat([interest_1,interest_2,interest_3],axis=-2)#[b,3,h]\n",
    "        #mixed_interest=interest_emb_3\n",
    "        #mixed_interest=tf.squeeze(mixed_interest,axis=1)\n",
    "        \n",
    "        self.mixed_interest=mixed_interest\n",
    "        \n",
    "        if self.use_l2norm:\n",
    "            pos_emb = tf.math.l2_normalize(pos_emb, axis=-1)\n",
    "            neg_emb = tf.math.l2_normalize(neg_emb, axis=-1)\n",
    "            user_info = tf.math.l2_normalize(user_info, axis=-1)\n",
    "\n",
    "        pos_emb=self.item_embedding(tf.reshape(inputs['pos_item'], [-1, ]))#[b,h]\n",
    "        expanded_target_item=tf.expand_dims(pos_emb,axis=1)#[b,1,h]\n",
    "\n",
    "        alpha=tf.matmul(expanded_target_item,tf.transpose(mixed_interest,[0,2,1]))#[b,1,3]\n",
    "        alpha=tf.nn.softmax(alpha)\n",
    "        \n",
    "\n",
    "        res = tf.matmul(alpha,mixed_interest)#[b,1,h]\n",
    "        pos_score=tf.reduce_sum(tf.multiply(res,expanded_target_item),axis=-1)#[b,1]\n",
    "\n",
    "        neg_emb=self.item_embedding(inputs['neg_item'])#[b,neg_num,h]\n",
    "        alpha=tf.matmul(neg_emb,tf.transpose(mixed_interest,[0,2,1]))#[b,neg_num,3]\n",
    "        alpha=tf.nn.softmax(alpha)\n",
    "        \n",
    "        #[b,t,1,3]*[b,1,3,h]=[b,t,1,h] squeeze=[b,t,h]\n",
    "        res = tf.matmul(alpha,mixed_interest)#[b,neg_num,h]\n",
    "        neg_score=tf.reduce_sum(tf.multiply(res,neg_emb),axis=-1)#[b,neg_num]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        pos_score = tf.tile(pos_score, [1, neg_score.shape[1]])\n",
    "        \n",
    "        loss = tf.reduce_mean(- tf.math.log(tf.nn.sigmoid(pos_score)) - tf.math.log(1 - tf.nn.sigmoid(neg_score))) / 2\n",
    "        self.add_loss(loss)\n",
    "        logits = tf.concat([pos_score, neg_score], axis=-1)\n",
    "        return logits\n",
    "    \n",
    "    def summary(self):\n",
    "        inputs = {\n",
    "            'click_seq': tf.keras.layers.Input(shape=(self.len_seq,), dtype=tf.int32),\n",
    "            'pos_item': tf.keras.layers.Input(shape=(), dtype=tf.int32),\n",
    "            'neg_item': tf.keras.layers.Input(shape=(1,), dtype=tf.int32)  # suppose neg_num=1\n",
    "        }\n",
    "        tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f64d3397-2c3c-4c58-9099-13b9bea1ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr(rank, k):\n",
    "    \"\"\"Hit Rate.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: hit rate.\n",
    "    \"\"\"\n",
    "    res = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            res += 1\n",
    "    return res / len(rank)\n",
    "\n",
    "\n",
    "def mrr(rank, k):\n",
    "    \"\"\"Mean Reciprocal Rank.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: mrr.\n",
    "    \"\"\"\n",
    "    mrr = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            mrr += 1 / (r + 1)\n",
    "    return mrr / len(rank)\n",
    "\n",
    "\n",
    "def ndcg(rank, k):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: ndcg.\n",
    "    \"\"\"\n",
    "    res = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            res += 1 / np.log2(r + 2)\n",
    "    return res / len(rank)\n",
    "\n",
    "\n",
    "def eval_rank(pred_y, metric_names, k=10):\n",
    "    \"\"\"Evaluate\n",
    "        Args:\n",
    "            :param pred_y: A ndarray.\n",
    "            :param metric_names: A list like ['hr'].\n",
    "            :param k: A scalar(int).\n",
    "        :return: A result dict such as {'hr':, 'ndcg':, ...}\n",
    "    \"\"\"\n",
    "    rank = pred_y.argsort().argsort()[:, 0]\n",
    "    res_dict = {}\n",
    "    for name in metric_names:\n",
    "        if name == 'hr':\n",
    "            res = hr(rank, k)\n",
    "        elif name == 'ndcg':\n",
    "            res = ndcg(rank, k)\n",
    "        elif name == 'mrr':\n",
    "            res = mrr(rank, k)\n",
    "        else:\n",
    "            break\n",
    "        res_dict[name] = res\n",
    "    return res_dict\n",
    "\n",
    "def eval_pos_neg(model, test_data, metric_names, k=10, batch_size=None):\n",
    "    \"\"\"Evaluate the performance of Top-k recommendation algorithm.\n",
    "    Note: Test data must contain some negative samples(>= k) and one positive samples.\n",
    "    Args:\n",
    "        :param model: A model built-by tensorflow.\n",
    "        :param test_data: A dict.\n",
    "        :param metric_names: A list like ['hr'].\n",
    "        :param k: A scalar(int).\n",
    "        :param batch_size: A scalar(int).\n",
    "    :return: A result dict such as {'hr':, 'ndcg':, ...}\n",
    "    \"\"\"\n",
    "    pred_y = - model.predict(test_data, batch_size)\n",
    "    return eval_rank(pred_y, metric_names, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef04f38c-df08-4399-bbd0-f47bc1ee3213",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1666147142592,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "ef04f38c-df08-4399-bbd0-f47bc1ee3213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 11:10:08.421893: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-10-20 11:10:08.466650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:3f:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2022-10-20 11:10:08.467115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:47:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2022-10-20 11:10:08.467137: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-10-20 11:10:08.470934: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-10-20 11:10:08.470990: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-10-20 11:10:08.472104: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-10-20 11:10:08.472377: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-10-20 11:10:08.473354: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-10-20 11:10:08.474123: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-10-20 11:10:08.474267: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-10-20 11:10:08.475575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2022-10-20 11:10:08.476081: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-20 11:10:08.672813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:3f:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2022-10-20 11:10:08.673167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:47:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2022-10-20 11:10:08.674370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2022-10-20 11:10:08.674416: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-10-20 11:10:09.505814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-10-20 11:10:09.505840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \n",
      "2022-10-20 11:10:09.505845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N N \n",
      "2022-10-20 11:10:09.505848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   N N \n",
      "2022-10-20 11:10:09.507671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8108 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:3f:00.0, compute capability: 8.6)\n",
      "2022-10-20 11:10:09.508319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8108 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:47:00.0, compute capability: 8.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7f421c596430>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7f421c596430>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7f421c596430>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7f421c596430>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7f421c5969d0>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(67728,) dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7f421c5969d0>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(67728,) dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7f421c5969d0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7f421c5969d0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x7f421c596550>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, ({'click_seq': <tf.Tensor 'args_1:0' shape=(67728, 50) dtype=int64>, 'pos_item': <tf.Tensor 'args_3:0' shape=(67728,) dtype=int64>, 'neg_item': <tf.Tensor 'args_2:0' shape=(67728, 4) dtype=int64>},))\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x7f421c596550>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, ({'click_seq': <tf.Tensor 'args_1:0' shape=(67728, 50) dtype=int64>, 'pos_item': <tf.Tensor 'args_3:0' shape=(67728,) dtype=int64>, 'neg_item': <tf.Tensor 'args_2:0' shape=(67728, 4) dtype=int64>},))\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x7f421c596550>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x7f421c596550>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function normalize_to_dense.<locals>.normalize at 0x7f421c47a040>\n",
      "    args: ({'click_seq': <tf.Tensor 'args_0:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'args_2:0' shape=(None,) dtype=int64>, 'neg_item': <tf.Tensor 'args_1:0' shape=(None, 4) dtype=int64>},)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <function normalize_to_dense.<locals>.normalize at 0x7f421c47a040>\n",
      "    args: ({'click_seq': <tf.Tensor 'args_0:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'args_2:0' shape=(None,) dtype=int64>, 'neg_item': <tf.Tensor 'args_1:0' shape=(None, 4) dtype=int64>},)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function normalize_to_dense.<locals>.normalize at 0x7f421c47a040>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function normalize_to_dense.<locals>.normalize at 0x7f421c47a040>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.train_function at 0x7f421c596dc0>\n",
      "    args: (<tensorflow.python.distribute.input_lib.DistributedIterator object at 0x7f421c473fa0>,)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <function Model.make_train_function.<locals>.train_function at 0x7f421c596dc0>\n",
      "    args: (<tensorflow.python.distribute.input_lib.DistributedIterator object at 0x7f421c473fa0>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:<function Model.make_train_function.<locals>.train_function at 0x7f421c596dc0> is not cached for subkey ConversionOptions[{}]\n",
      "<function Model.make_train_function.<locals>.train_function at 0x7f421c596dc0> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function Model.make_train_function.<locals>.train_function at 0x7f421c596dc0>:\n",
      "\n",
      "def train_function(iterator):\n",
      "  \"\"\"Runs a training execution with one step.\"\"\"\n",
      "  return step_function(self, iterator)\n",
      "\n",
      "\n",
      "Source code of <function Model.make_train_function.<locals>.train_function at 0x7f421c596dc0>:\n",
      "\n",
      "def train_function(iterator):\n",
      "  \"\"\"Runs a training execution with one step.\"\"\"\n",
      "  return step_function(self, iterator)\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function Model.make_train_function.<locals>.train_function at 0x7f421c596dc0>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__train_function(iterator):\n",
      "    'Runs a training execution with one step.'\n",
      "    with ag__.FunctionScope('train_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "Transformed <function Model.make_train_function.<locals>.train_function at 0x7f421c596dc0>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__train_function(iterator):\n",
      "    'Runs a training execution with one step.'\n",
      "    with ag__.FunctionScope('train_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x7f421c3f0c10> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x7f421c3f0c10> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x7f421c3f0c10> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x7f421c3f0c10> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x7f421c3f0c10> with\n",
      "    iterator: <tensorflow.python.distribute.input_lib.DistributedIterator object at 0x7f421c473fa0>\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x7f421c3f0c10> with\n",
      "    iterator: <tensorflow.python.distribute.input_lib.DistributedIterator object at 0x7f421c473fa0>\n",
      "\n",
      "INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.step_function at 0x7f421c596ee0>\n",
      "    args: (<__main__.sasrec object at 0x7f4247a02b80>, <tensorflow.python.distribute.input_lib.DistributedIterator object at 0x7f421c473fa0>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function Model.make_train_function.<locals>.step_function at 0x7f421c596ee0>\n",
      "    args: (<__main__.sasrec object at 0x7f4247a02b80>, <tensorflow.python.distribute.input_lib.DistributedIterator object at 0x7f421c473fa0>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function Model.make_train_function.<locals>.step_function at 0x7f421c596ee0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function Model.make_train_function.<locals>.step_function at 0x7f421c596ee0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x7f421c3f09d0>\n",
      "    args: (({'click_seq': <tf.Tensor 'cond_2/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'cond_2/Identity_2:0' shape=(None,) dtype=int64>, 'neg_item': <tf.Tensor 'cond_2/Identity_1:0' shape=(None, 4) dtype=int64>},),)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x7f421c3f09d0>\n",
      "    args: (({'click_seq': <tf.Tensor 'cond_2/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'cond_2/Identity_2:0' shape=(None,) dtype=int64>, 'neg_item': <tf.Tensor 'cond_2/Identity_1:0' shape=(None, 4) dtype=int64>},),)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x7f421c3f09d0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x7f421c3f09d0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>\n",
      "    args: ({'click_seq': <tf.Tensor 'cond_2/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'neg_item': <tf.Tensor 'cond_2/Identity_1:0' shape=(None, 4) dtype=int64>},)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>\n",
      "    args: ({'click_seq': <tf.Tensor 'cond_2/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'neg_item': <tf.Tensor 'cond_2/Identity_1:0' shape=(None, 4) dtype=int64>},)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c4cac00>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c4cac00>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.sasrec'>: default rule\n",
      "Not allowed: <class '__main__.sasrec'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>: default rule\n",
      "Not allowed: <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>: default rule\n",
      "INFO:tensorflow:<bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>> is not cached for subkey ConversionOptions[{}]\n",
      "<bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>:\n",
      "\n",
      "def call(self, inputs):\n",
      "    seq_embed=self.item_embedding(inputs['click_seq'])\n",
      "    pos_encoding = tf.expand_dims(self.pos_embedding(tf.range(self.len_seq)), axis=0)  # (1, seq_len, embed_dim)\n",
      "    seq_embed += pos_encoding\n",
      "\n",
      "    seq_embed = self.dropout(seq_embed)\n",
      "\n",
      "    interest_1,interest_2,interest_3=self.hiera(seq_embed)\n",
      "\n",
      "    att_outputs_1=interest_1\n",
      "    for block in self.encoder_layer_1:\n",
      "        att_outputs_1 = block([att_outputs_1,None])  # (None, seq_len, embed_dim)\n",
      "\n",
      "    att_outputs_2=interest_2\n",
      "    for block in self.encoder_layer_2:\n",
      "        att_outputs_2= block([att_outputs_2,None])  # (None, seq_len, embed_dim)\n",
      "\n",
      "    att_outputs_3=interest_3\n",
      "    for block in self.encoder_layer_3:\n",
      "        att_outputs_3 = block([att_outputs_3,None])  # (None, seq_len, embed_dim)\n",
      "\n",
      "\n",
      "    interest_1 = tf.slice(att_outputs_1, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n",
      "    interest_2 = tf.slice(att_outputs_2, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n",
      "    interest_3 = tf.slice(att_outputs_3, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n",
      "\n",
      "    mixed_interest=tf.concat([interest_1,interest_2,interest_3],axis=-2)#[b,3,h]\n",
      "    #mixed_interest=interest_emb_3\n",
      "    #mixed_interest=tf.squeeze(mixed_interest,axis=1)\n",
      "\n",
      "    self.mixed_interest=mixed_interest\n",
      "\n",
      "    if self.use_l2norm:\n",
      "        pos_emb = tf.math.l2_normalize(pos_emb, axis=-1)\n",
      "        neg_emb = tf.math.l2_normalize(neg_emb, axis=-1)\n",
      "        user_info = tf.math.l2_normalize(user_info, axis=-1)\n",
      "\n",
      "    pos_emb=self.item_embedding(tf.reshape(inputs['pos_item'], [-1, ]))#[b,h]\n",
      "    expanded_target_item=tf.expand_dims(pos_emb,axis=1)#[b,1,h]\n",
      "\n",
      "    alpha=tf.matmul(expanded_target_item,tf.transpose(mixed_interest,[0,2,1]))#[b,1,3]\n",
      "    alpha=tf.nn.softmax(alpha)\n",
      "\n",
      "\n",
      "    res = tf.matmul(alpha,mixed_interest)#[b,1,h]\n",
      "    pos_score=tf.reduce_sum(tf.multiply(res,expanded_target_item),axis=-1)#[b,1]\n",
      "\n",
      "    neg_emb=self.item_embedding(inputs['neg_item'])#[b,neg_num,h]\n",
      "    alpha=tf.matmul(neg_emb,tf.transpose(mixed_interest,[0,2,1]))#[b,neg_num,3]\n",
      "    alpha=tf.nn.softmax(alpha)\n",
      "\n",
      "    #[b,t,1,3]*[b,1,3,h]=[b,t,1,h] squeeze=[b,t,h]\n",
      "    res = tf.matmul(alpha,mixed_interest)#[b,neg_num,h]\n",
      "    neg_score=tf.reduce_sum(tf.multiply(res,neg_emb),axis=-1)#[b,neg_num]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    pos_score = tf.tile(pos_score, [1, neg_score.shape[1]])\n",
      "\n",
      "    loss = tf.reduce_mean(- tf.math.log(tf.nn.sigmoid(pos_score)) - tf.math.log(1 - tf.nn.sigmoid(neg_score))) / 2\n",
      "    self.add_loss(loss)\n",
      "    logits = tf.concat([pos_score, neg_score], axis=-1)\n",
      "    return logits\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 11:10:09.940428: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_107\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2022-10-20 11:10:09.947932: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-10-20 11:10:09.948483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2500000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source code of <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>:\n",
      "\n",
      "def call(self, inputs):\n",
      "    seq_embed=self.item_embedding(inputs['click_seq'])\n",
      "    pos_encoding = tf.expand_dims(self.pos_embedding(tf.range(self.len_seq)), axis=0)  # (1, seq_len, embed_dim)\n",
      "    seq_embed += pos_encoding\n",
      "\n",
      "    seq_embed = self.dropout(seq_embed)\n",
      "\n",
      "    interest_1,interest_2,interest_3=self.hiera(seq_embed)\n",
      "\n",
      "    att_outputs_1=interest_1\n",
      "    for block in self.encoder_layer_1:\n",
      "        att_outputs_1 = block([att_outputs_1,None])  # (None, seq_len, embed_dim)\n",
      "\n",
      "    att_outputs_2=interest_2\n",
      "    for block in self.encoder_layer_2:\n",
      "        att_outputs_2= block([att_outputs_2,None])  # (None, seq_len, embed_dim)\n",
      "\n",
      "    att_outputs_3=interest_3\n",
      "    for block in self.encoder_layer_3:\n",
      "        att_outputs_3 = block([att_outputs_3,None])  # (None, seq_len, embed_dim)\n",
      "\n",
      "\n",
      "    interest_1 = tf.slice(att_outputs_1, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n",
      "    interest_2 = tf.slice(att_outputs_2, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n",
      "    interest_3 = tf.slice(att_outputs_3, begin=[0, self.len_seq-1, 0], size=[-1, 1, -1])\n",
      "\n",
      "    mixed_interest=tf.concat([interest_1,interest_2,interest_3],axis=-2)#[b,3,h]\n",
      "    #mixed_interest=interest_emb_3\n",
      "    #mixed_interest=tf.squeeze(mixed_interest,axis=1)\n",
      "\n",
      "    self.mixed_interest=mixed_interest\n",
      "\n",
      "    if self.use_l2norm:\n",
      "        pos_emb = tf.math.l2_normalize(pos_emb, axis=-1)\n",
      "        neg_emb = tf.math.l2_normalize(neg_emb, axis=-1)\n",
      "        user_info = tf.math.l2_normalize(user_info, axis=-1)\n",
      "\n",
      "    pos_emb=self.item_embedding(tf.reshape(inputs['pos_item'], [-1, ]))#[b,h]\n",
      "    expanded_target_item=tf.expand_dims(pos_emb,axis=1)#[b,1,h]\n",
      "\n",
      "    alpha=tf.matmul(expanded_target_item,tf.transpose(mixed_interest,[0,2,1]))#[b,1,3]\n",
      "    alpha=tf.nn.softmax(alpha)\n",
      "\n",
      "\n",
      "    res = tf.matmul(alpha,mixed_interest)#[b,1,h]\n",
      "    pos_score=tf.reduce_sum(tf.multiply(res,expanded_target_item),axis=-1)#[b,1]\n",
      "\n",
      "    neg_emb=self.item_embedding(inputs['neg_item'])#[b,neg_num,h]\n",
      "    alpha=tf.matmul(neg_emb,tf.transpose(mixed_interest,[0,2,1]))#[b,neg_num,3]\n",
      "    alpha=tf.nn.softmax(alpha)\n",
      "\n",
      "    #[b,t,1,3]*[b,1,3,h]=[b,t,1,h] squeeze=[b,t,h]\n",
      "    res = tf.matmul(alpha,mixed_interest)#[b,neg_num,h]\n",
      "    neg_score=tf.reduce_sum(tf.multiply(res,neg_emb),axis=-1)#[b,neg_num]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    pos_score = tf.tile(pos_score, [1, neg_score.shape[1]])\n",
      "\n",
      "    loss = tf.reduce_mean(- tf.math.log(tf.nn.sigmoid(pos_score)) - tf.math.log(1 - tf.nn.sigmoid(neg_score))) / 2\n",
      "    self.add_loss(loss)\n",
      "    logits = tf.concat([pos_score, neg_score], axis=-1)\n",
      "    return logits\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__call(self, inputs):\n",
      "    with ag__.FunctionScope('call', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        seq_embed = ag__.converted_call(ag__.ld(self).item_embedding, (ag__.ld(inputs)['click_seq'],), None, fscope)\n",
      "        pos_encoding = ag__.converted_call(ag__.ld(tf).expand_dims, (ag__.converted_call(ag__.ld(self).pos_embedding, (ag__.converted_call(ag__.ld(tf).range, (ag__.ld(self).len_seq,), None, fscope),), None, fscope),), dict(axis=0), fscope)\n",
      "        seq_embed = ag__.ld(seq_embed)\n",
      "        seq_embed += pos_encoding\n",
      "        seq_embed = ag__.converted_call(ag__.ld(self).dropout, (ag__.ld(seq_embed),), None, fscope)\n",
      "        (interest_1, interest_2, interest_3) = ag__.converted_call(ag__.ld(self).hiera, (ag__.ld(seq_embed),), None, fscope)\n",
      "        att_outputs_1 = ag__.ld(interest_1)\n",
      "\n",
      "        def get_state():\n",
      "            return (att_outputs_1,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal att_outputs_1\n",
      "            (att_outputs_1,) = vars_\n",
      "\n",
      "        def loop_body(itr):\n",
      "            nonlocal att_outputs_1\n",
      "            block = itr\n",
      "            att_outputs_1 = ag__.converted_call(ag__.ld(block), ([ag__.ld(att_outputs_1), None],), None, fscope)\n",
      "        block = ag__.Undefined('block')\n",
      "        ag__.for_stmt(ag__.ld(self).encoder_layer_1, None, loop_body, get_state, set_state, ('att_outputs_1',), {'iterate_names': 'block'})\n",
      "        att_outputs_2 = ag__.ld(interest_2)\n",
      "\n",
      "        def get_state_1():\n",
      "            return (att_outputs_2,)\n",
      "\n",
      "        def set_state_1(vars_):\n",
      "            nonlocal att_outputs_2\n",
      "            (att_outputs_2,) = vars_\n",
      "\n",
      "        def loop_body_1(itr_1):\n",
      "            nonlocal att_outputs_2\n",
      "            block = itr_1\n",
      "            att_outputs_2 = ag__.converted_call(ag__.ld(block), ([ag__.ld(att_outputs_2), None],), None, fscope)\n",
      "        ag__.for_stmt(ag__.ld(self).encoder_layer_2, None, loop_body_1, get_state_1, set_state_1, ('att_outputs_2',), {'iterate_names': 'block'})\n",
      "        att_outputs_3 = ag__.ld(interest_3)\n",
      "\n",
      "        def get_state_2():\n",
      "            return (att_outputs_3,)\n",
      "\n",
      "        def set_state_2(vars_):\n",
      "            nonlocal att_outputs_3\n",
      "            (att_outputs_3,) = vars_\n",
      "\n",
      "        def loop_body_2(itr_2):\n",
      "            nonlocal att_outputs_3\n",
      "            block = itr_2\n",
      "            att_outputs_3 = ag__.converted_call(ag__.ld(block), ([ag__.ld(att_outputs_3), None],), None, fscope)\n",
      "        ag__.for_stmt(ag__.ld(self).encoder_layer_3, None, loop_body_2, get_state_2, set_state_2, ('att_outputs_3',), {'iterate_names': 'block'})\n",
      "        interest_1 = ag__.converted_call(ag__.ld(tf).slice, (ag__.ld(att_outputs_1),), dict(begin=[0, (ag__.ld(self).len_seq - 1), 0], size=[(- 1), 1, (- 1)]), fscope)\n",
      "        interest_2 = ag__.converted_call(ag__.ld(tf).slice, (ag__.ld(att_outputs_2),), dict(begin=[0, (ag__.ld(self).len_seq - 1), 0], size=[(- 1), 1, (- 1)]), fscope)\n",
      "        interest_3 = ag__.converted_call(ag__.ld(tf).slice, (ag__.ld(att_outputs_3),), dict(begin=[0, (ag__.ld(self).len_seq - 1), 0], size=[(- 1), 1, (- 1)]), fscope)\n",
      "        mixed_interest = ag__.converted_call(ag__.ld(tf).concat, ([ag__.ld(interest_1), ag__.ld(interest_2), ag__.ld(interest_3)],), dict(axis=(- 2)), fscope)\n",
      "        ag__.ld(self).mixed_interest = ag__.ld(mixed_interest)\n",
      "\n",
      "        def get_state_3():\n",
      "            return (neg_emb, pos_emb, user_info)\n",
      "\n",
      "        def set_state_3(vars_):\n",
      "            nonlocal pos_emb, user_info, neg_emb\n",
      "            (neg_emb, pos_emb, user_info) = vars_\n",
      "\n",
      "        def if_body():\n",
      "            nonlocal pos_emb, user_info, neg_emb\n",
      "            pos_emb = ag__.converted_call(ag__.ld(tf).math.l2_normalize, (ag__.ld(pos_emb),), dict(axis=(- 1)), fscope)\n",
      "            neg_emb = ag__.converted_call(ag__.ld(tf).math.l2_normalize, (ag__.ld(neg_emb),), dict(axis=(- 1)), fscope)\n",
      "            user_info = ag__.converted_call(ag__.ld(tf).math.l2_normalize, (ag__.ld(user_info),), dict(axis=(- 1)), fscope)\n",
      "\n",
      "        def else_body():\n",
      "            nonlocal pos_emb, user_info, neg_emb\n",
      "            pass\n",
      "        pos_emb = ag__.Undefined('pos_emb')\n",
      "        user_info = ag__.Undefined('user_info')\n",
      "        neg_emb = ag__.Undefined('neg_emb')\n",
      "        ag__.if_stmt(ag__.ld(self).use_l2norm, if_body, else_body, get_state_3, set_state_3, ('neg_emb', 'pos_emb', 'user_info'), 0)\n",
      "        pos_emb = ag__.converted_call(ag__.ld(self).item_embedding, (ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(inputs)['pos_item'], [(- 1)]), None, fscope),), None, fscope)\n",
      "        expanded_target_item = ag__.converted_call(ag__.ld(tf).expand_dims, (ag__.ld(pos_emb),), dict(axis=1), fscope)\n",
      "        alpha = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(expanded_target_item), ag__.converted_call(ag__.ld(tf).transpose, (ag__.ld(mixed_interest), [0, 2, 1]), None, fscope)), None, fscope)\n",
      "        alpha = ag__.converted_call(ag__.ld(tf).nn.softmax, (ag__.ld(alpha),), None, fscope)\n",
      "        res = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(alpha), ag__.ld(mixed_interest)), None, fscope)\n",
      "        pos_score = ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.converted_call(ag__.ld(tf).multiply, (ag__.ld(res), ag__.ld(expanded_target_item)), None, fscope),), dict(axis=(- 1)), fscope)\n",
      "        neg_emb = ag__.converted_call(ag__.ld(self).item_embedding, (ag__.ld(inputs)['neg_item'],), None, fscope)\n",
      "        alpha = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(neg_emb), ag__.converted_call(ag__.ld(tf).transpose, (ag__.ld(mixed_interest), [0, 2, 1]), None, fscope)), None, fscope)\n",
      "        alpha = ag__.converted_call(ag__.ld(tf).nn.softmax, (ag__.ld(alpha),), None, fscope)\n",
      "        res = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(alpha), ag__.ld(mixed_interest)), None, fscope)\n",
      "        neg_score = ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.converted_call(ag__.ld(tf).multiply, (ag__.ld(res), ag__.ld(neg_emb)), None, fscope),), dict(axis=(- 1)), fscope)\n",
      "        pos_score = ag__.converted_call(ag__.ld(tf).tile, (ag__.ld(pos_score), [1, ag__.ld(neg_score).shape[1]]), None, fscope)\n",
      "        loss = (ag__.converted_call(ag__.ld(tf).reduce_mean, (((- ag__.converted_call(ag__.ld(tf).math.log, (ag__.converted_call(ag__.ld(tf).nn.sigmoid, (ag__.ld(pos_score),), None, fscope),), None, fscope)) - ag__.converted_call(ag__.ld(tf).math.log, ((1 - ag__.converted_call(ag__.ld(tf).nn.sigmoid, (ag__.ld(neg_score),), None, fscope)),), None, fscope)),), None, fscope) / 2)\n",
      "        ag__.converted_call(ag__.ld(self).add_loss, (ag__.ld(loss),), None, fscope)\n",
      "        logits = ag__.converted_call(ag__.ld(tf).concat, ([ag__.ld(pos_score), ag__.ld(neg_score)],), dict(axis=(- 1)), fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(logits)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "Transformed <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__call(self, inputs):\n",
      "    with ag__.FunctionScope('call', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        seq_embed = ag__.converted_call(ag__.ld(self).item_embedding, (ag__.ld(inputs)['click_seq'],), None, fscope)\n",
      "        pos_encoding = ag__.converted_call(ag__.ld(tf).expand_dims, (ag__.converted_call(ag__.ld(self).pos_embedding, (ag__.converted_call(ag__.ld(tf).range, (ag__.ld(self).len_seq,), None, fscope),), None, fscope),), dict(axis=0), fscope)\n",
      "        seq_embed = ag__.ld(seq_embed)\n",
      "        seq_embed += pos_encoding\n",
      "        seq_embed = ag__.converted_call(ag__.ld(self).dropout, (ag__.ld(seq_embed),), None, fscope)\n",
      "        (interest_1, interest_2, interest_3) = ag__.converted_call(ag__.ld(self).hiera, (ag__.ld(seq_embed),), None, fscope)\n",
      "        att_outputs_1 = ag__.ld(interest_1)\n",
      "\n",
      "        def get_state():\n",
      "            return (att_outputs_1,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal att_outputs_1\n",
      "            (att_outputs_1,) = vars_\n",
      "\n",
      "        def loop_body(itr):\n",
      "            nonlocal att_outputs_1\n",
      "            block = itr\n",
      "            att_outputs_1 = ag__.converted_call(ag__.ld(block), ([ag__.ld(att_outputs_1), None],), None, fscope)\n",
      "        block = ag__.Undefined('block')\n",
      "        ag__.for_stmt(ag__.ld(self).encoder_layer_1, None, loop_body, get_state, set_state, ('att_outputs_1',), {'iterate_names': 'block'})\n",
      "        att_outputs_2 = ag__.ld(interest_2)\n",
      "\n",
      "        def get_state_1():\n",
      "            return (att_outputs_2,)\n",
      "\n",
      "        def set_state_1(vars_):\n",
      "            nonlocal att_outputs_2\n",
      "            (att_outputs_2,) = vars_\n",
      "\n",
      "        def loop_body_1(itr_1):\n",
      "            nonlocal att_outputs_2\n",
      "            block = itr_1\n",
      "            att_outputs_2 = ag__.converted_call(ag__.ld(block), ([ag__.ld(att_outputs_2), None],), None, fscope)\n",
      "        ag__.for_stmt(ag__.ld(self).encoder_layer_2, None, loop_body_1, get_state_1, set_state_1, ('att_outputs_2',), {'iterate_names': 'block'})\n",
      "        att_outputs_3 = ag__.ld(interest_3)\n",
      "\n",
      "        def get_state_2():\n",
      "            return (att_outputs_3,)\n",
      "\n",
      "        def set_state_2(vars_):\n",
      "            nonlocal att_outputs_3\n",
      "            (att_outputs_3,) = vars_\n",
      "\n",
      "        def loop_body_2(itr_2):\n",
      "            nonlocal att_outputs_3\n",
      "            block = itr_2\n",
      "            att_outputs_3 = ag__.converted_call(ag__.ld(block), ([ag__.ld(att_outputs_3), None],), None, fscope)\n",
      "        ag__.for_stmt(ag__.ld(self).encoder_layer_3, None, loop_body_2, get_state_2, set_state_2, ('att_outputs_3',), {'iterate_names': 'block'})\n",
      "        interest_1 = ag__.converted_call(ag__.ld(tf).slice, (ag__.ld(att_outputs_1),), dict(begin=[0, (ag__.ld(self).len_seq - 1), 0], size=[(- 1), 1, (- 1)]), fscope)\n",
      "        interest_2 = ag__.converted_call(ag__.ld(tf).slice, (ag__.ld(att_outputs_2),), dict(begin=[0, (ag__.ld(self).len_seq - 1), 0], size=[(- 1), 1, (- 1)]), fscope)\n",
      "        interest_3 = ag__.converted_call(ag__.ld(tf).slice, (ag__.ld(att_outputs_3),), dict(begin=[0, (ag__.ld(self).len_seq - 1), 0], size=[(- 1), 1, (- 1)]), fscope)\n",
      "        mixed_interest = ag__.converted_call(ag__.ld(tf).concat, ([ag__.ld(interest_1), ag__.ld(interest_2), ag__.ld(interest_3)],), dict(axis=(- 2)), fscope)\n",
      "        ag__.ld(self).mixed_interest = ag__.ld(mixed_interest)\n",
      "\n",
      "        def get_state_3():\n",
      "            return (neg_emb, pos_emb, user_info)\n",
      "\n",
      "        def set_state_3(vars_):\n",
      "            nonlocal pos_emb, user_info, neg_emb\n",
      "            (neg_emb, pos_emb, user_info) = vars_\n",
      "\n",
      "        def if_body():\n",
      "            nonlocal pos_emb, user_info, neg_emb\n",
      "            pos_emb = ag__.converted_call(ag__.ld(tf).math.l2_normalize, (ag__.ld(pos_emb),), dict(axis=(- 1)), fscope)\n",
      "            neg_emb = ag__.converted_call(ag__.ld(tf).math.l2_normalize, (ag__.ld(neg_emb),), dict(axis=(- 1)), fscope)\n",
      "            user_info = ag__.converted_call(ag__.ld(tf).math.l2_normalize, (ag__.ld(user_info),), dict(axis=(- 1)), fscope)\n",
      "\n",
      "        def else_body():\n",
      "            nonlocal pos_emb, user_info, neg_emb\n",
      "            pass\n",
      "        pos_emb = ag__.Undefined('pos_emb')\n",
      "        user_info = ag__.Undefined('user_info')\n",
      "        neg_emb = ag__.Undefined('neg_emb')\n",
      "        ag__.if_stmt(ag__.ld(self).use_l2norm, if_body, else_body, get_state_3, set_state_3, ('neg_emb', 'pos_emb', 'user_info'), 0)\n",
      "        pos_emb = ag__.converted_call(ag__.ld(self).item_embedding, (ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(inputs)['pos_item'], [(- 1)]), None, fscope),), None, fscope)\n",
      "        expanded_target_item = ag__.converted_call(ag__.ld(tf).expand_dims, (ag__.ld(pos_emb),), dict(axis=1), fscope)\n",
      "        alpha = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(expanded_target_item), ag__.converted_call(ag__.ld(tf).transpose, (ag__.ld(mixed_interest), [0, 2, 1]), None, fscope)), None, fscope)\n",
      "        alpha = ag__.converted_call(ag__.ld(tf).nn.softmax, (ag__.ld(alpha),), None, fscope)\n",
      "        res = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(alpha), ag__.ld(mixed_interest)), None, fscope)\n",
      "        pos_score = ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.converted_call(ag__.ld(tf).multiply, (ag__.ld(res), ag__.ld(expanded_target_item)), None, fscope),), dict(axis=(- 1)), fscope)\n",
      "        neg_emb = ag__.converted_call(ag__.ld(self).item_embedding, (ag__.ld(inputs)['neg_item'],), None, fscope)\n",
      "        alpha = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(neg_emb), ag__.converted_call(ag__.ld(tf).transpose, (ag__.ld(mixed_interest), [0, 2, 1]), None, fscope)), None, fscope)\n",
      "        alpha = ag__.converted_call(ag__.ld(tf).nn.softmax, (ag__.ld(alpha),), None, fscope)\n",
      "        res = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(alpha), ag__.ld(mixed_interest)), None, fscope)\n",
      "        neg_score = ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.converted_call(ag__.ld(tf).multiply, (ag__.ld(res), ag__.ld(neg_emb)), None, fscope),), dict(axis=(- 1)), fscope)\n",
      "        pos_score = ag__.converted_call(ag__.ld(tf).tile, (ag__.ld(pos_score), [1, ag__.ld(neg_score).shape[1]]), None, fscope)\n",
      "        loss = (ag__.converted_call(ag__.ld(tf).reduce_mean, (((- ag__.converted_call(ag__.ld(tf).math.log, (ag__.converted_call(ag__.ld(tf).nn.sigmoid, (ag__.ld(pos_score),), None, fscope),), None, fscope)) - ag__.converted_call(ag__.ld(tf).math.log, ((1 - ag__.converted_call(ag__.ld(tf).nn.sigmoid, (ag__.ld(neg_score),), None, fscope)),), None, fscope)),), None, fscope) / 2)\n",
      "        ag__.converted_call(ag__.ld(self).add_loss, (ag__.ld(loss),), None, fscope)\n",
      "        logits = ag__.converted_call(ag__.ld(tf).concat, ([ag__.ld(pos_score), ag__.ld(neg_score)],), dict(axis=(- 1)), fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(logits)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c36dca0> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c36dca0> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c36dca0> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c36dca0> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c36dca0> with\n",
      "    self: <__main__.sasrec object at 0x7f4247a02b80>\n",
      "    inputs: {'click_seq': <tf.Tensor 'cond_2/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'neg_item': <tf.Tensor 'cond_2/Identity_1:0' shape=(None, 4) dtype=int64>}\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c36dca0> with\n",
      "    self: <__main__.sasrec object at 0x7f4247a02b80>\n",
      "    inputs: {'click_seq': <tf.Tensor 'cond_2/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'neg_item': <tf.Tensor 'cond_2/Identity_1:0' shape=(None, 4) dtype=int64>}\n",
      "\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>\n",
      "    args: (<tf.Tensor 'cond_2/Identity:0' shape=(None, 50) dtype=int64>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>\n",
      "    args: (<tf.Tensor 'cond_2/Identity:0' shape=(None, 50) dtype=int64>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function range at 0x7f4251eb99d0>\n",
      "    args: (50,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function range at 0x7f4251eb99d0>\n",
      "    args: (50,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function range at 0x7f4251eb99d0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function range at 0x7f4251eb99d0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982fc10>\n",
      "    args: (<tf.Tensor 'sasrec/range:0' shape=(50,) dtype=int32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982fc10>\n",
      "    args: (<tf.Tensor 'sasrec/range:0' shape=(50,) dtype=int32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982fc10>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982fc10>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function expand_dims_v2 at 0x7f4252549ee0>\n",
      "    args: (<tf.Tensor 'sasrec/embedding_1/embedding_lookup/Identity:0' shape=(50, 64) dtype=float32>,)\n",
      "    kwargs: {'axis': 0}\n",
      "\n",
      "Converted call: <function expand_dims_v2 at 0x7f4252549ee0>\n",
      "    args: (<tf.Tensor 'sasrec/embedding_1/embedding_lookup/Identity:0' shape=(50, 64) dtype=float32>,)\n",
      "    kwargs: {'axis': 0}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function expand_dims_v2 at 0x7f4252549ee0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function expand_dims_v2 at 0x7f4252549ee0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f423982ffa0>\n",
      "    args: (<tf.Tensor 'sasrec/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f423982ffa0>\n",
      "    args: (<tf.Tensor 'sasrec/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f423982ffa0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f423982ffa0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <__main__.hierarchical_interest object at 0x7f423982f0a0>\n",
      "    args: (<tf.Tensor 'sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.hierarchical_interest object at 0x7f423982f0a0>\n",
      "    args: (<tf.Tensor 'sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Layer.__call__ of <__main__.hierarchical_interest object at 0x7f423982f0a0>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Layer.__call__ of <__main__.hierarchical_interest object at 0x7f423982f0a0>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Allowlisted: <__main__.hierarchical_interest object at 0x7f423982f0a0>: object __call__ allowed\n",
      "Allowlisted: <__main__.hierarchical_interest object at 0x7f423982f0a0>: object __call__ allowed\n",
      "INFO:tensorflow:Converted call: <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>\n",
      "    args: (<tf.Tensor 'sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>\n",
      "    args: (<tf.Tensor 'sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c39af80>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c39af80>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.hierarchical_interest'>: default rule\n",
      "Not allowed: <class '__main__.hierarchical_interest'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>: default rule\n",
      "Not allowed: <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>: default rule\n",
      "INFO:tensorflow:<bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>> is not cached for subkey ConversionOptions[{}]\n",
      "<bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>:\n",
      "\n",
      "def call(self, x):\n",
      "    interest_1 = self.layer_1(x)  # (None, len_seq, d_model)\n",
      "    interest_2 = self.layer_2(x)  # (None, len_seq, d_model)\n",
      "    interest_3 = self.layer_3(x)  # (None, len_seq, d_model)\n",
      "    interest_2=interest_2-scaled_dot_product_attention(interest_2,interest_1,interest_1,None)\n",
      "    interest_3=interest_3-scaled_dot_product_attention(interest_3,interest_1,interest_1,None)\n",
      "    interest_1=self.layernorm(interest_1)\n",
      "    interest_2=self.layernorm(interest_2)\n",
      "    interest_3=self.layernorm(interest_3)\n",
      "    return interest_1,interest_2,interest_3\n",
      "\n",
      "\n",
      "Source code of <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>:\n",
      "\n",
      "def call(self, x):\n",
      "    interest_1 = self.layer_1(x)  # (None, len_seq, d_model)\n",
      "    interest_2 = self.layer_2(x)  # (None, len_seq, d_model)\n",
      "    interest_3 = self.layer_3(x)  # (None, len_seq, d_model)\n",
      "    interest_2=interest_2-scaled_dot_product_attention(interest_2,interest_1,interest_1,None)\n",
      "    interest_3=interest_3-scaled_dot_product_attention(interest_3,interest_1,interest_1,None)\n",
      "    interest_1=self.layernorm(interest_1)\n",
      "    interest_2=self.layernorm(interest_2)\n",
      "    interest_3=self.layernorm(interest_3)\n",
      "    return interest_1,interest_2,interest_3\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__call(self, x):\n",
      "    with ag__.FunctionScope('call', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        interest_1 = ag__.converted_call(ag__.ld(self).layer_1, (ag__.ld(x),), None, fscope)\n",
      "        interest_2 = ag__.converted_call(ag__.ld(self).layer_2, (ag__.ld(x),), None, fscope)\n",
      "        interest_3 = ag__.converted_call(ag__.ld(self).layer_3, (ag__.ld(x),), None, fscope)\n",
      "        interest_2 = (ag__.ld(interest_2) - ag__.converted_call(ag__.ld(scaled_dot_product_attention), (ag__.ld(interest_2), ag__.ld(interest_1), ag__.ld(interest_1), None), None, fscope))\n",
      "        interest_3 = (ag__.ld(interest_3) - ag__.converted_call(ag__.ld(scaled_dot_product_attention), (ag__.ld(interest_3), ag__.ld(interest_1), ag__.ld(interest_1), None), None, fscope))\n",
      "        interest_1 = ag__.converted_call(ag__.ld(self).layernorm, (ag__.ld(interest_1),), None, fscope)\n",
      "        interest_2 = ag__.converted_call(ag__.ld(self).layernorm, (ag__.ld(interest_2),), None, fscope)\n",
      "        interest_3 = ag__.converted_call(ag__.ld(self).layernorm, (ag__.ld(interest_3),), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = (ag__.ld(interest_1), ag__.ld(interest_2), ag__.ld(interest_3))\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "Transformed <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__call(self, x):\n",
      "    with ag__.FunctionScope('call', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        interest_1 = ag__.converted_call(ag__.ld(self).layer_1, (ag__.ld(x),), None, fscope)\n",
      "        interest_2 = ag__.converted_call(ag__.ld(self).layer_2, (ag__.ld(x),), None, fscope)\n",
      "        interest_3 = ag__.converted_call(ag__.ld(self).layer_3, (ag__.ld(x),), None, fscope)\n",
      "        interest_2 = (ag__.ld(interest_2) - ag__.converted_call(ag__.ld(scaled_dot_product_attention), (ag__.ld(interest_2), ag__.ld(interest_1), ag__.ld(interest_1), None), None, fscope))\n",
      "        interest_3 = (ag__.ld(interest_3) - ag__.converted_call(ag__.ld(scaled_dot_product_attention), (ag__.ld(interest_3), ag__.ld(interest_1), ag__.ld(interest_1), None), None, fscope))\n",
      "        interest_1 = ag__.converted_call(ag__.ld(self).layernorm, (ag__.ld(interest_1),), None, fscope)\n",
      "        interest_2 = ag__.converted_call(ag__.ld(self).layernorm, (ag__.ld(interest_2),), None, fscope)\n",
      "        interest_3 = ag__.converted_call(ag__.ld(self).layernorm, (ag__.ld(interest_3),), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = (ag__.ld(interest_1), ag__.ld(interest_2), ag__.ld(interest_3))\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c229f70> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c229f70> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c229f70> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c229f70> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c229f70> with\n",
      "    self: <__main__.hierarchical_interest object at 0x7f423982f0a0>\n",
      "    x: Tensor(\"sasrec/dropout/dropout/Mul_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c229f70> with\n",
      "    self: <__main__.hierarchical_interest object at 0x7f423982f0a0>\n",
      "    x: Tensor(\"sasrec/dropout/dropout/Mul_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f423982fca0>\n",
      "    args: (<tf.Tensor 'sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f423982fca0>\n",
      "    args: (<tf.Tensor 'sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f423982fca0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f423982fca0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4230014130>\n",
      "    args: (<tf.Tensor 'sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4230014130>\n",
      "    args: (<tf.Tensor 'sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4230014130>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4230014130>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f42300145e0>\n",
      "    args: (<tf.Tensor 'sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f42300145e0>\n",
      "    args: (<tf.Tensor 'sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f42300145e0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f42300145e0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/conv1d_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/conv1d_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:<function scaled_dot_product_attention at 0x7f4239b11310> is not cached for subkey ConversionOptions[{}]\n",
      "<function scaled_dot_product_attention at 0x7f4239b11310> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function scaled_dot_product_attention at 0x7f4239b11310>:\n",
      "\n",
      "def scaled_dot_product_attention(q, k, v, mask):\n",
      "    \"\"\"Attention Mechanism Function.\n",
      "    Args:\n",
      "        :param q: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
      "        :param k: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
      "        :param v: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
      "        :param mask: A 3d/4d tensor with shape of (None, ..., seq_len, 1)\n",
      "    :return:\n",
      "    \"\"\"\n",
      "    mat_qk = tf.matmul(q, k, transpose_b=True)  # (None, seq_len, seq_len)\n",
      "    # Scaled\n",
      "    dk = tf.cast(k.shape[-1], dtype=tf.float32)\n",
      "    scaled_att_logits = mat_qk / tf.sqrt(dk)\n",
      "\n",
      "    paddings = tf.ones_like(scaled_att_logits) * (-2 ** 32 + 1)  # (None, seq_len, seq_len)\n",
      "    if mask!=None:\n",
      "        outputs = tf.where(tf.equal(mask, tf.zeros_like(mask)), paddings, scaled_att_logits)  # (None, seq_len, seq_len)\n",
      "    else:\n",
      "        outputs=scaled_att_logits\n",
      "    # softmax\n",
      "    outputs = tf.nn.softmax(logits=outputs)  # (None, seq_len, seq_len)\n",
      "    outputs = tf.matmul(outputs, v)  # (None, seq_len, dim)\n",
      "\n",
      "    return outputs\n",
      "\n",
      "\n",
      "Source code of <function scaled_dot_product_attention at 0x7f4239b11310>:\n",
      "\n",
      "def scaled_dot_product_attention(q, k, v, mask):\n",
      "    \"\"\"Attention Mechanism Function.\n",
      "    Args:\n",
      "        :param q: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
      "        :param k: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
      "        :param v: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
      "        :param mask: A 3d/4d tensor with shape of (None, ..., seq_len, 1)\n",
      "    :return:\n",
      "    \"\"\"\n",
      "    mat_qk = tf.matmul(q, k, transpose_b=True)  # (None, seq_len, seq_len)\n",
      "    # Scaled\n",
      "    dk = tf.cast(k.shape[-1], dtype=tf.float32)\n",
      "    scaled_att_logits = mat_qk / tf.sqrt(dk)\n",
      "\n",
      "    paddings = tf.ones_like(scaled_att_logits) * (-2 ** 32 + 1)  # (None, seq_len, seq_len)\n",
      "    if mask!=None:\n",
      "        outputs = tf.where(tf.equal(mask, tf.zeros_like(mask)), paddings, scaled_att_logits)  # (None, seq_len, seq_len)\n",
      "    else:\n",
      "        outputs=scaled_att_logits\n",
      "    # softmax\n",
      "    outputs = tf.nn.softmax(logits=outputs)  # (None, seq_len, seq_len)\n",
      "    outputs = tf.matmul(outputs, v)  # (None, seq_len, dim)\n",
      "\n",
      "    return outputs\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function scaled_dot_product_attention at 0x7f4239b11310>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__scaled_dot_product_attention(q, k, v, mask):\n",
      "    'Attention Mechanism Function.\\n    Args:\\n        :param q: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\\n        :param k: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\\n        :param v: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\\n        :param mask: A 3d/4d tensor with shape of (None, ..., seq_len, 1)\\n    :return:\\n    '\n",
      "    with ag__.FunctionScope('scaled_dot_product_attention', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        mat_qk = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(q), ag__.ld(k)), dict(transpose_b=True), fscope)\n",
      "        dk = ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(k).shape[(- 1)],), dict(dtype=ag__.ld(tf).float32), fscope)\n",
      "        scaled_att_logits = (ag__.ld(mat_qk) / ag__.converted_call(ag__.ld(tf).sqrt, (ag__.ld(dk),), None, fscope))\n",
      "        paddings = (ag__.converted_call(ag__.ld(tf).ones_like, (ag__.ld(scaled_att_logits),), None, fscope) * ((- (2 ** 32)) + 1))\n",
      "\n",
      "        def get_state():\n",
      "            return (outputs,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal outputs\n",
      "            (outputs,) = vars_\n",
      "\n",
      "        def if_body():\n",
      "            nonlocal outputs\n",
      "            outputs = ag__.converted_call(ag__.ld(tf).where, (ag__.converted_call(ag__.ld(tf).equal, (ag__.ld(mask), ag__.converted_call(ag__.ld(tf).zeros_like, (ag__.ld(mask),), None, fscope)), None, fscope), ag__.ld(paddings), ag__.ld(scaled_att_logits)), None, fscope)\n",
      "\n",
      "        def else_body():\n",
      "            nonlocal outputs\n",
      "            outputs = ag__.ld(scaled_att_logits)\n",
      "        outputs = ag__.Undefined('outputs')\n",
      "        ag__.if_stmt((ag__.ld(mask) != None), if_body, else_body, get_state, set_state, ('outputs',), 1)\n",
      "        outputs = ag__.converted_call(ag__.ld(tf).nn.softmax, (), dict(logits=ag__.ld(outputs)), fscope)\n",
      "        outputs = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(outputs), ag__.ld(v)), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(outputs)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "Transformed <function scaled_dot_product_attention at 0x7f4239b11310>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__scaled_dot_product_attention(q, k, v, mask):\n",
      "    'Attention Mechanism Function.\\n    Args:\\n        :param q: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\\n        :param k: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\\n        :param v: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\\n        :param mask: A 3d/4d tensor with shape of (None, ..., seq_len, 1)\\n    :return:\\n    '\n",
      "    with ag__.FunctionScope('scaled_dot_product_attention', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        mat_qk = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(q), ag__.ld(k)), dict(transpose_b=True), fscope)\n",
      "        dk = ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(k).shape[(- 1)],), dict(dtype=ag__.ld(tf).float32), fscope)\n",
      "        scaled_att_logits = (ag__.ld(mat_qk) / ag__.converted_call(ag__.ld(tf).sqrt, (ag__.ld(dk),), None, fscope))\n",
      "        paddings = (ag__.converted_call(ag__.ld(tf).ones_like, (ag__.ld(scaled_att_logits),), None, fscope) * ((- (2 ** 32)) + 1))\n",
      "\n",
      "        def get_state():\n",
      "            return (outputs,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal outputs\n",
      "            (outputs,) = vars_\n",
      "\n",
      "        def if_body():\n",
      "            nonlocal outputs\n",
      "            outputs = ag__.converted_call(ag__.ld(tf).where, (ag__.converted_call(ag__.ld(tf).equal, (ag__.ld(mask), ag__.converted_call(ag__.ld(tf).zeros_like, (ag__.ld(mask),), None, fscope)), None, fscope), ag__.ld(paddings), ag__.ld(scaled_att_logits)), None, fscope)\n",
      "\n",
      "        def else_body():\n",
      "            nonlocal outputs\n",
      "            outputs = ag__.ld(scaled_att_logits)\n",
      "        outputs = ag__.Undefined('outputs')\n",
      "        ag__.if_stmt((ag__.ld(mask) != None), if_body, else_body, get_state, set_state, ('outputs',), 1)\n",
      "        outputs = ag__.converted_call(ag__.ld(tf).nn.softmax, (), dict(logits=ag__.ld(outputs)), fscope)\n",
      "        outputs = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(outputs), ag__.ld(v)), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(outputs)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198790> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198790> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198790> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198790> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198790> with\n",
      "    q: Tensor(\"sasrec/hierarchical_interest/conv1d_1/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198790> with\n",
      "    q: Tensor(\"sasrec/hierarchical_interest/conv1d_1/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/conv1d_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/conv1d_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function matmul at 0x7f4251ec8280>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function matmul at 0x7f4251ec8280>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function cast at 0x7f4251f2d670>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function cast at 0x7f4251f2d670>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/Cast:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/Cast:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function sqrt at 0x7f4251e3b5e0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function sqrt at 0x7f4251e3b5e0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/truediv:0' shape=(None, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/truediv:0' shape=(None, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function ones_like_v2 at 0x7f4252553ee0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function ones_like_v2 at 0x7f4252553ee0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'sasrec/hierarchical_interest/truediv:0' shape=(None, 50, 50) dtype=float32>}\n",
      "\n",
      "Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'sasrec/hierarchical_interest/truediv:0' shape=(None, 50, 50) dtype=float32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function softmax_v2 at 0x7f424efc55e0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function softmax_v2 at 0x7f424efc55e0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/Softmax:0' shape=(None, 50, 50) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/Softmax:0' shape=(None, 50, 50) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/conv1d_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/conv1d_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198e50> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198e50> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198e50> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198e50> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198e50> with\n",
      "    q: Tensor(\"sasrec/hierarchical_interest/conv1d_2/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c198e50> with\n",
      "    q: Tensor(\"sasrec/hierarchical_interest/conv1d_2/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/conv1d_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/conv1d_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "INFO:tensorflow:Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/Cast_1:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/Cast_1:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/truediv_1:0' shape=(None, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/truediv_1:0' shape=(None, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "INFO:tensorflow:Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'sasrec/hierarchical_interest/truediv_1:0' shape=(None, 50, 50) dtype=float32>}\n",
      "\n",
      "Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'sasrec/hierarchical_interest/truediv_1:0' shape=(None, 50, 50) dtype=float32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/Softmax_1:0' shape=(None, 50, 50) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/Softmax_1:0' shape=(None, 50, 50) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/sub:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/sub:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/sub_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/sub_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: from cache\n",
      "INFO:tensorflow:Converted call: <__main__.TransformerEncoder object at 0x7f421c5c00d0>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.TransformerEncoder object at 0x7f421c5c00d0>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Layer.__call__ of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Layer.__call__ of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Allowlisted: <__main__.TransformerEncoder object at 0x7f421c5c00d0>: object __call__ allowed\n",
      "Allowlisted: <__main__.TransformerEncoder object at 0x7f421c5c00d0>: object __call__ allowed\n",
      "INFO:tensorflow:Converted call: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f4239b40600>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f4239b40600>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.TransformerEncoder'>: default rule\n",
      "Not allowed: <class '__main__.TransformerEncoder'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>: default rule\n",
      "Not allowed: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>: default rule\n",
      "INFO:tensorflow:<bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>> is not cached for subkey ConversionOptions[{}]\n",
      "<bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>:\n",
      "\n",
      "def call(self, inputs):\n",
      "    x, mask= inputs\n",
      "    # self-attention\n",
      "    att_out = self.mha(x, x, x, mask)  # (None, seq_len, d_model)\n",
      "    att_out = self.dropout1(att_out)\n",
      "    # residual add\n",
      "    out1 = self.layernorm1(x + att_out)  # (None, seq_len, d_model)\n",
      "    # ffn\n",
      "    ffn_out = self.ffn(out1)\n",
      "    ffn_out = self.dropout2(ffn_out)\n",
      "    # residual add\n",
      "    out2 = self.layernorm2(out1 + ffn_out)  # (None, seq_len, d_model)\n",
      "    return out2\n",
      "\n",
      "\n",
      "Source code of <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>:\n",
      "\n",
      "def call(self, inputs):\n",
      "    x, mask= inputs\n",
      "    # self-attention\n",
      "    att_out = self.mha(x, x, x, mask)  # (None, seq_len, d_model)\n",
      "    att_out = self.dropout1(att_out)\n",
      "    # residual add\n",
      "    out1 = self.layernorm1(x + att_out)  # (None, seq_len, d_model)\n",
      "    # ffn\n",
      "    ffn_out = self.ffn(out1)\n",
      "    ffn_out = self.dropout2(ffn_out)\n",
      "    # residual add\n",
      "    out2 = self.layernorm2(out1 + ffn_out)  # (None, seq_len, d_model)\n",
      "    return out2\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__call(self, inputs):\n",
      "    with ag__.FunctionScope('call', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        (x, mask) = ag__.ld(inputs)\n",
      "        att_out = ag__.converted_call(ag__.ld(self).mha, (ag__.ld(x), ag__.ld(x), ag__.ld(x), ag__.ld(mask)), None, fscope)\n",
      "        att_out = ag__.converted_call(ag__.ld(self).dropout1, (ag__.ld(att_out),), None, fscope)\n",
      "        out1 = ag__.converted_call(ag__.ld(self).layernorm1, ((ag__.ld(x) + ag__.ld(att_out)),), None, fscope)\n",
      "        ffn_out = ag__.converted_call(ag__.ld(self).ffn, (ag__.ld(out1),), None, fscope)\n",
      "        ffn_out = ag__.converted_call(ag__.ld(self).dropout2, (ag__.ld(ffn_out),), None, fscope)\n",
      "        out2 = ag__.converted_call(ag__.ld(self).layernorm2, ((ag__.ld(out1) + ag__.ld(ffn_out)),), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(out2)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "Transformed <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__call(self, inputs):\n",
      "    with ag__.FunctionScope('call', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        (x, mask) = ag__.ld(inputs)\n",
      "        att_out = ag__.converted_call(ag__.ld(self).mha, (ag__.ld(x), ag__.ld(x), ag__.ld(x), ag__.ld(mask)), None, fscope)\n",
      "        att_out = ag__.converted_call(ag__.ld(self).dropout1, (ag__.ld(att_out),), None, fscope)\n",
      "        out1 = ag__.converted_call(ag__.ld(self).layernorm1, ((ag__.ld(x) + ag__.ld(att_out)),), None, fscope)\n",
      "        ffn_out = ag__.converted_call(ag__.ld(self).ffn, (ag__.ld(out1),), None, fscope)\n",
      "        ffn_out = ag__.converted_call(ag__.ld(self).dropout2, (ag__.ld(ffn_out),), None, fscope)\n",
      "        out2 = ag__.converted_call(ag__.ld(self).layernorm2, ((ag__.ld(out1) + ag__.ld(ffn_out)),), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(out2)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15cf70> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15cf70> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15cf70> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15cf70> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15cf70> with\n",
      "    self: <__main__.TransformerEncoder object at 0x7f421c5c00d0>\n",
      "    inputs: [<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None]\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15cf70> with\n",
      "    self: <__main__.TransformerEncoder object at 0x7f421c5c00d0>\n",
      "    inputs: [<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None]\n",
      "\n",
      "INFO:tensorflow:Converted call: <__main__.MultiHeadAttention object at 0x7f421c5c01f0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.MultiHeadAttention object at 0x7f421c5c01f0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Layer.__call__ of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Layer.__call__ of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Allowlisted: <__main__.MultiHeadAttention object at 0x7f421c5c01f0>: object __call__ allowed\n",
      "Allowlisted: <__main__.MultiHeadAttention object at 0x7f421c5c01f0>: object __call__ allowed\n",
      "INFO:tensorflow:Converted call: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c1fca80>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c1fca80>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.MultiHeadAttention'>: default rule\n",
      "Not allowed: <class '__main__.MultiHeadAttention'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>: default rule\n",
      "Not allowed: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>: default rule\n",
      "INFO:tensorflow:<bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>> is not cached for subkey ConversionOptions[{}]\n",
      "<bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>:\n",
      "\n",
      "def call(self, q, k, v, mask):\n",
      "    q = self.wq(q)  # (None, seq_len, d_model)\n",
      "    k = self.wk(k)  # (None, seq_len, d_model)\n",
      "    v = self.wv(v)  # (None, seq_len, d_model)\n",
      "    # split d_model into num_heads * depth\n",
      "    seq_len, d_model = q.shape[1], q.shape[2]\n",
      "    q = split_heads(q, seq_len, self.num_heads, q.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
      "    k = split_heads(k, seq_len, self.num_heads, k.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
      "    v = split_heads(v, seq_len, self.num_heads, v.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
      "    # mask\n",
      "    if mask!=None:\n",
      "        mask = tf.tile(tf.expand_dims(mask, axis=1), [1, self.num_heads, 1, 1])  # (None, num_heads, seq_len, 1)\n",
      "        # attention\n",
      "    scaled_attention = scaled_dot_product_attention(q, k, v, mask)  # (None, num_heads, seq_len, d_model // num_heads)\n",
      "    # reshape\n",
      "    outputs = tf.reshape(tf.transpose(scaled_attention, [0, 2, 1, 3]), [-1, seq_len, d_model])  # (None, seq_len, d_model)\n",
      "    return outputs\n",
      "\n",
      "\n",
      "Source code of <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>:\n",
      "\n",
      "def call(self, q, k, v, mask):\n",
      "    q = self.wq(q)  # (None, seq_len, d_model)\n",
      "    k = self.wk(k)  # (None, seq_len, d_model)\n",
      "    v = self.wv(v)  # (None, seq_len, d_model)\n",
      "    # split d_model into num_heads * depth\n",
      "    seq_len, d_model = q.shape[1], q.shape[2]\n",
      "    q = split_heads(q, seq_len, self.num_heads, q.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
      "    k = split_heads(k, seq_len, self.num_heads, k.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
      "    v = split_heads(v, seq_len, self.num_heads, v.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
      "    # mask\n",
      "    if mask!=None:\n",
      "        mask = tf.tile(tf.expand_dims(mask, axis=1), [1, self.num_heads, 1, 1])  # (None, num_heads, seq_len, 1)\n",
      "        # attention\n",
      "    scaled_attention = scaled_dot_product_attention(q, k, v, mask)  # (None, num_heads, seq_len, d_model // num_heads)\n",
      "    # reshape\n",
      "    outputs = tf.reshape(tf.transpose(scaled_attention, [0, 2, 1, 3]), [-1, seq_len, d_model])  # (None, seq_len, d_model)\n",
      "    return outputs\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__call(self, q, k, v, mask):\n",
      "    with ag__.FunctionScope('call', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        q = ag__.converted_call(ag__.ld(self).wq, (ag__.ld(q),), None, fscope)\n",
      "        k = ag__.converted_call(ag__.ld(self).wk, (ag__.ld(k),), None, fscope)\n",
      "        v = ag__.converted_call(ag__.ld(self).wv, (ag__.ld(v),), None, fscope)\n",
      "        (seq_len, d_model) = (ag__.ld(q).shape[1], ag__.ld(q).shape[2])\n",
      "        q = ag__.converted_call(ag__.ld(split_heads), (ag__.ld(q), ag__.ld(seq_len), ag__.ld(self).num_heads, (ag__.ld(q).shape[2] // ag__.ld(self).num_heads)), None, fscope)\n",
      "        k = ag__.converted_call(ag__.ld(split_heads), (ag__.ld(k), ag__.ld(seq_len), ag__.ld(self).num_heads, (ag__.ld(k).shape[2] // ag__.ld(self).num_heads)), None, fscope)\n",
      "        v = ag__.converted_call(ag__.ld(split_heads), (ag__.ld(v), ag__.ld(seq_len), ag__.ld(self).num_heads, (ag__.ld(v).shape[2] // ag__.ld(self).num_heads)), None, fscope)\n",
      "\n",
      "        def get_state():\n",
      "            return (mask,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal mask\n",
      "            (mask,) = vars_\n",
      "\n",
      "        def if_body():\n",
      "            nonlocal mask\n",
      "            mask = ag__.converted_call(ag__.ld(tf).tile, (ag__.converted_call(ag__.ld(tf).expand_dims, (ag__.ld(mask),), dict(axis=1), fscope), [1, ag__.ld(self).num_heads, 1, 1]), None, fscope)\n",
      "\n",
      "        def else_body():\n",
      "            nonlocal mask\n",
      "            pass\n",
      "        ag__.if_stmt((ag__.ld(mask) != None), if_body, else_body, get_state, set_state, ('mask',), 1)\n",
      "        scaled_attention = ag__.converted_call(ag__.ld(scaled_dot_product_attention), (ag__.ld(q), ag__.ld(k), ag__.ld(v), ag__.ld(mask)), None, fscope)\n",
      "        outputs = ag__.converted_call(ag__.ld(tf).reshape, (ag__.converted_call(ag__.ld(tf).transpose, (ag__.ld(scaled_attention), [0, 2, 1, 3]), None, fscope), [(- 1), ag__.ld(seq_len), ag__.ld(d_model)]), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(outputs)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "Transformed <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__call(self, q, k, v, mask):\n",
      "    with ag__.FunctionScope('call', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        q = ag__.converted_call(ag__.ld(self).wq, (ag__.ld(q),), None, fscope)\n",
      "        k = ag__.converted_call(ag__.ld(self).wk, (ag__.ld(k),), None, fscope)\n",
      "        v = ag__.converted_call(ag__.ld(self).wv, (ag__.ld(v),), None, fscope)\n",
      "        (seq_len, d_model) = (ag__.ld(q).shape[1], ag__.ld(q).shape[2])\n",
      "        q = ag__.converted_call(ag__.ld(split_heads), (ag__.ld(q), ag__.ld(seq_len), ag__.ld(self).num_heads, (ag__.ld(q).shape[2] // ag__.ld(self).num_heads)), None, fscope)\n",
      "        k = ag__.converted_call(ag__.ld(split_heads), (ag__.ld(k), ag__.ld(seq_len), ag__.ld(self).num_heads, (ag__.ld(k).shape[2] // ag__.ld(self).num_heads)), None, fscope)\n",
      "        v = ag__.converted_call(ag__.ld(split_heads), (ag__.ld(v), ag__.ld(seq_len), ag__.ld(self).num_heads, (ag__.ld(v).shape[2] // ag__.ld(self).num_heads)), None, fscope)\n",
      "\n",
      "        def get_state():\n",
      "            return (mask,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal mask\n",
      "            (mask,) = vars_\n",
      "\n",
      "        def if_body():\n",
      "            nonlocal mask\n",
      "            mask = ag__.converted_call(ag__.ld(tf).tile, (ag__.converted_call(ag__.ld(tf).expand_dims, (ag__.ld(mask),), dict(axis=1), fscope), [1, ag__.ld(self).num_heads, 1, 1]), None, fscope)\n",
      "\n",
      "        def else_body():\n",
      "            nonlocal mask\n",
      "            pass\n",
      "        ag__.if_stmt((ag__.ld(mask) != None), if_body, else_body, get_state, set_state, ('mask',), 1)\n",
      "        scaled_attention = ag__.converted_call(ag__.ld(scaled_dot_product_attention), (ag__.ld(q), ag__.ld(k), ag__.ld(v), ag__.ld(mask)), None, fscope)\n",
      "        outputs = ag__.converted_call(ag__.ld(tf).reshape, (ag__.converted_call(ag__.ld(tf).transpose, (ag__.ld(scaled_attention), [0, 2, 1, 3]), None, fscope), [(- 1), ag__.ld(seq_len), ag__.ld(d_model)]), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(outputs)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15ce50> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15ce50> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15ce50> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15ce50> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15ce50> with\n",
      "    self: <__main__.MultiHeadAttention object at 0x7f421c5c01f0>\n",
      "    q: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c15ce50> with\n",
      "    self: <__main__.MultiHeadAttention object at 0x7f421c5c01f0>\n",
      "    q: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c02b0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c02b0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c02b0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c02b0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c06a0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c06a0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c06a0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c06a0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c09d0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c09d0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c09d0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c09d0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:<function split_heads at 0x7f4239b113a0> is not cached for subkey ConversionOptions[{}]\n",
      "<function split_heads at 0x7f4239b113a0> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function split_heads at 0x7f4239b113a0>:\n",
      "\n",
      "def split_heads(x, seq_len, num_heads, depth):\n",
      "    \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "    Args:\n",
      "        :param x: A Tensor with shape of [batch_size, seq_len, num_heads * depth]\n",
      "        :param seq_len: A scalar(int).\n",
      "        :param num_heads: A scalar(int).\n",
      "        :param depth: A scalar(int).\n",
      "    :return: A tensor with shape of [batch_size, num_heads, seq_len, depth]\n",
      "    \"\"\"\n",
      "    x = tf.reshape(x, (-1, seq_len, num_heads, depth))\n",
      "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "\n",
      "Source code of <function split_heads at 0x7f4239b113a0>:\n",
      "\n",
      "def split_heads(x, seq_len, num_heads, depth):\n",
      "    \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "    Args:\n",
      "        :param x: A Tensor with shape of [batch_size, seq_len, num_heads * depth]\n",
      "        :param seq_len: A scalar(int).\n",
      "        :param num_heads: A scalar(int).\n",
      "        :param depth: A scalar(int).\n",
      "    :return: A tensor with shape of [batch_size, num_heads, seq_len, depth]\n",
      "    \"\"\"\n",
      "    x = tf.reshape(x, (-1, seq_len, num_heads, depth))\n",
      "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function split_heads at 0x7f4239b113a0>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__split_heads(x, seq_len, num_heads, depth):\n",
      "    'Split the last dimension into (num_heads, depth).\\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\\n    Args:\\n        :param x: A Tensor with shape of [batch_size, seq_len, num_heads * depth]\\n        :param seq_len: A scalar(int).\\n        :param num_heads: A scalar(int).\\n        :param depth: A scalar(int).\\n    :return: A tensor with shape of [batch_size, num_heads, seq_len, depth]\\n    '\n",
      "    with ag__.FunctionScope('split_heads', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        x = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(x), ((- 1), ag__.ld(seq_len), ag__.ld(num_heads), ag__.ld(depth))), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.converted_call(ag__.ld(tf).transpose, (ag__.ld(x),), dict(perm=[0, 2, 1, 3]), fscope)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "Transformed <function split_heads at 0x7f4239b113a0>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__split_heads(x, seq_len, num_heads, depth):\n",
      "    'Split the last dimension into (num_heads, depth).\\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\\n    Args:\\n        :param x: A Tensor with shape of [batch_size, seq_len, num_heads * depth]\\n        :param seq_len: A scalar(int).\\n        :param num_heads: A scalar(int).\\n        :param depth: A scalar(int).\\n    :return: A tensor with shape of [batch_size, num_heads, seq_len, depth]\\n    '\n",
      "    with ag__.FunctionScope('split_heads', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        x = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(x), ((- 1), ag__.ld(seq_len), ag__.ld(num_heads), ag__.ld(depth))), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.converted_call(ag__.ld(tf).transpose, (ag__.ld(x),), dict(perm=[0, 2, 1, 3]), fscope)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c155ee0> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c155ee0> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c155ee0> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c155ee0> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c155ee0> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c155ee0> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function reshape at 0x7f4252549820>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function reshape at 0x7f4252549820>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Reshape:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Reshape:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function transpose_v2 at 0x7f4252550f70>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function transpose_v2 at 0x7f4252550f70>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Reshape_1:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Reshape_1:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c075040> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Reshape_2:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Reshape_2:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c075040> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c075040> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c075040> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c075040> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c075040> with\n",
      "    q: Tensor(\"sasrec/transformer_encoder/multi_head_attention/transpose:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/transformer_encoder/multi_head_attention/transpose_1:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/transformer_encoder/multi_head_attention/transpose_2:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c075040> with\n",
      "    q: Tensor(\"sasrec/transformer_encoder/multi_head_attention/transpose:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/transformer_encoder/multi_head_attention/transpose_1:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/transformer_encoder/multi_head_attention/transpose_2:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "INFO:tensorflow:Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Cast:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Cast:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/truediv:0' shape=(None, 1, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/truediv:0' shape=(None, 1, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "INFO:tensorflow:Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/truediv:0' shape=(None, 1, 50, 50) dtype=float32>}\n",
      "\n",
      "Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/truediv:0' shape=(None, 1, 50, 50) dtype=float32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Softmax:0' shape=(None, 1, 50, 50) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Softmax:0' shape=(None, 1, 50, 50) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/MatMul_1:0' shape=(None, 1, 50, 64) dtype=float32>, [0, 2, 1, 3])\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/MatMul_1:0' shape=(None, 1, 50, 64) dtype=float32>, [0, 2, 1, 3])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose_3:0' shape=(None, 50, 1, 64) dtype=float32>, [-1, 50, 64])\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/transpose_3:0' shape=(None, 50, 1, 64) dtype=float32>, [-1, 50, 64])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c5e2b20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Reshape_3:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c5e2b20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/multi_head_attention/Reshape_3:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c5e2b20>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c5e2b20>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5c0e20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5c0e20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5c0e20>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5c0e20>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <__main__.FFN object at 0x7f421c5c0be0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/layer_normalization_1/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.FFN object at 0x7f421c5c0be0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/layer_normalization_1/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Layer.__call__ of <__main__.FFN object at 0x7f421c5c0be0>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Layer.__call__ of <__main__.FFN object at 0x7f421c5c0be0>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Allowlisted: <__main__.FFN object at 0x7f421c5c0be0>: object __call__ allowed\n",
      "Allowlisted: <__main__.FFN object at 0x7f421c5c0be0>: object __call__ allowed\n",
      "INFO:tensorflow:Converted call: <bound method FFN.call of <__main__.FFN object at 0x7f421c5c0be0>>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/layer_normalization_1/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method FFN.call of <__main__.FFN object at 0x7f421c5c0be0>>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/layer_normalization_1/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c3209c0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c3209c0>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.FFN'>: default rule\n",
      "Not allowed: <class '__main__.FFN'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method FFN.call of <__main__.FFN object at 0x7f421c5c0be0>>: default rule\n",
      "Not allowed: <bound method FFN.call of <__main__.FFN object at 0x7f421c5c0be0>>: default rule\n",
      "INFO:tensorflow:<bound method FFN.call of <__main__.FFN object at 0x7f421c5c0be0>> is not cached for subkey ConversionOptions[{}]\n",
      "<bound method FFN.call of <__main__.FFN object at 0x7f421c5c0be0>> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <bound method FFN.call of <__main__.FFN object at 0x7f421c5c0be0>>:\n",
      "\n",
      "def call(self, inputs):\n",
      "    x = self.conv1(inputs)\n",
      "    output = self.conv2(x)\n",
      "    return output\n",
      "\n",
      "\n",
      "Source code of <bound method FFN.call of <__main__.FFN object at 0x7f421c5c0be0>>:\n",
      "\n",
      "def call(self, inputs):\n",
      "    x = self.conv1(inputs)\n",
      "    output = self.conv2(x)\n",
      "    return output\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <bound method FFN.call of <__main__.FFN object at 0x7f421c5c0be0>>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__call(self, inputs):\n",
      "    with ag__.FunctionScope('call', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        x = ag__.converted_call(ag__.ld(self).conv1, (ag__.ld(inputs),), None, fscope)\n",
      "        output = ag__.converted_call(ag__.ld(self).conv2, (ag__.ld(x),), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(output)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "Transformed <bound method FFN.call of <__main__.FFN object at 0x7f421c5c0be0>>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__call(self, inputs):\n",
      "    with ag__.FunctionScope('call', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        x = ag__.converted_call(ag__.ld(self).conv1, (ag__.ld(inputs),), None, fscope)\n",
      "        output = ag__.converted_call(ag__.ld(self).conv2, (ag__.ld(x),), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(output)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0653a0> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0653a0> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0653a0> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0653a0> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0653a0> with\n",
      "    self: <__main__.FFN object at 0x7f421c5c0be0>\n",
      "    inputs: Tensor(\"sasrec/transformer_encoder/layer_normalization_1/batchnorm/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0653a0> with\n",
      "    self: <__main__.FFN object at 0x7f421c5c0be0>\n",
      "    inputs: Tensor(\"sasrec/transformer_encoder/layer_normalization_1/batchnorm/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c5c0df0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/layer_normalization_1/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c5c0df0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/layer_normalization_1/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c5c0df0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c5c0df0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c5e2040>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/ffn/conv1d_3/Relu:0' shape=(None, 50, 128) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c5e2040>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/ffn/conv1d_3/Relu:0' shape=(None, 50, 128) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c5e2040>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c5e2040>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c5e2d90>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/ffn/conv1d_4/BiasAdd:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c5e2d90>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/ffn/conv1d_4/BiasAdd:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c5e2d90>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c5e2d90>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5e28b0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5e28b0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5e28b0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5e28b0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <__main__.TransformerEncoder object at 0x7f4239882100>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.TransformerEncoder object at 0x7f4239882100>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Layer.__call__ of <__main__.TransformerEncoder object at 0x7f4239882100>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Layer.__call__ of <__main__.TransformerEncoder object at 0x7f4239882100>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Allowlisted: <__main__.TransformerEncoder object at 0x7f4239882100>: object __call__ allowed\n",
      "Allowlisted: <__main__.TransformerEncoder object at 0x7f4239882100>: object __call__ allowed\n",
      "INFO:tensorflow:Converted call: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f4239882100>>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f4239882100>>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f4239b40600>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f4239b40600>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.TransformerEncoder'>: default rule\n",
      "Not allowed: <class '__main__.TransformerEncoder'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f4239882100>>: default rule\n",
      "Not allowed: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f4239882100>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f4239882100>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c1fd640>\n",
      "Cache hit for <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f4239882100>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c1fd640>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066310> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066310> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066310> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066310> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066310> with\n",
      "    self: <__main__.TransformerEncoder object at 0x7f4239882100>\n",
      "    inputs: [<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, None]\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066310> with\n",
      "    self: <__main__.TransformerEncoder object at 0x7f4239882100>\n",
      "    inputs: [<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, None]\n",
      "\n",
      "INFO:tensorflow:Converted call: <__main__.MultiHeadAttention object at 0x7f42398821f0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.MultiHeadAttention object at 0x7f42398821f0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Layer.__call__ of <__main__.MultiHeadAttention object at 0x7f42398821f0>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Layer.__call__ of <__main__.MultiHeadAttention object at 0x7f42398821f0>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Allowlisted: <__main__.MultiHeadAttention object at 0x7f42398821f0>: object __call__ allowed\n",
      "Allowlisted: <__main__.MultiHeadAttention object at 0x7f42398821f0>: object __call__ allowed\n",
      "INFO:tensorflow:Converted call: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f42398821f0>>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f42398821f0>>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c174f80>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c174f80>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.MultiHeadAttention'>: default rule\n",
      "Not allowed: <class '__main__.MultiHeadAttention'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f42398821f0>>: default rule\n",
      "Not allowed: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f42398821f0>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f42398821f0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c12a430>\n",
      "Cache hit for <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f42398821f0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c12a430>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066430> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066430> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066430> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066430> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066430> with\n",
      "    self: <__main__.MultiHeadAttention object at 0x7f42398821f0>\n",
      "    q: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c066430> with\n",
      "    self: <__main__.MultiHeadAttention object at 0x7f42398821f0>\n",
      "    q: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398822b0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398822b0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398822b0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398822b0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398826a0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398826a0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398826a0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398826a0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398829d0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398829d0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398829d0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f42398829d0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_3/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_3/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aa60> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aa60> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aa60> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aa60> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aa60> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/dense_3/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aa60> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/dense_3/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_3/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_3/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Reshape:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Reshape:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_4/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_4/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044a9d0> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044a9d0> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044a9d0> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044a9d0> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044a9d0> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/dense_4/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044a9d0> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/dense_4/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_4/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_4/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Reshape_1:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Reshape_1:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_5/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_5/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aca0> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aca0> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aca0> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aca0> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aca0> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/dense_5/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41d044aca0> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/dense_5/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_5/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/dense_5/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Reshape_2:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Reshape_2:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d044adc0> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d044adc0> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d044adc0> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d044adc0> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d044adc0> with\n",
      "    q: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/transpose:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/transpose_1:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/transpose_2:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d044adc0> with\n",
      "    q: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/transpose:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/transpose_1:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/transformer_encoder_1/multi_head_attention_1/transpose_2:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "INFO:tensorflow:Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Cast:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Cast:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/truediv:0' shape=(None, 1, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/truediv:0' shape=(None, 1, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "INFO:tensorflow:Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/truediv:0' shape=(None, 1, 50, 50) dtype=float32>}\n",
      "\n",
      "Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/truediv:0' shape=(None, 1, 50, 50) dtype=float32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Softmax:0' shape=(None, 1, 50, 50) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Softmax:0' shape=(None, 1, 50, 50) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/MatMul_1:0' shape=(None, 1, 50, 64) dtype=float32>, [0, 2, 1, 3])\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/MatMul_1:0' shape=(None, 1, 50, 64) dtype=float32>, [0, 2, 1, 3])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose_3:0' shape=(None, 50, 1, 64) dtype=float32>, [-1, 50, 64])\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/transpose_3:0' shape=(None, 50, 1, 64) dtype=float32>, [-1, 50, 64])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c583b20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Reshape_3:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c583b20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/multi_head_attention_1/Reshape_3:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c583b20>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c583b20>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f4239882e20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f4239882e20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f4239882e20>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f4239882e20>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <__main__.FFN object at 0x7f4239882be0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/layer_normalization_3/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.FFN object at 0x7f4239882be0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/layer_normalization_3/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Layer.__call__ of <__main__.FFN object at 0x7f4239882be0>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Layer.__call__ of <__main__.FFN object at 0x7f4239882be0>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Allowlisted: <__main__.FFN object at 0x7f4239882be0>: object __call__ allowed\n",
      "Allowlisted: <__main__.FFN object at 0x7f4239882be0>: object __call__ allowed\n",
      "INFO:tensorflow:Converted call: <bound method FFN.call of <__main__.FFN object at 0x7f4239882be0>>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/layer_normalization_3/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method FFN.call of <__main__.FFN object at 0x7f4239882be0>>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/layer_normalization_3/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c394b40>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c394b40>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.FFN'>: default rule\n",
      "Not allowed: <class '__main__.FFN'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method FFN.call of <__main__.FFN object at 0x7f4239882be0>>: default rule\n",
      "Not allowed: <bound method FFN.call of <__main__.FFN object at 0x7f4239882be0>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method FFN.call of <__main__.FFN object at 0x7f4239882be0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c14e490>\n",
      "Cache hit for <bound method FFN.call of <__main__.FFN object at 0x7f4239882be0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c14e490>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d0419280> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d0419280> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d0419280> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d0419280> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d0419280> with\n",
      "    self: <__main__.FFN object at 0x7f4239882be0>\n",
      "    inputs: Tensor(\"sasrec/transformer_encoder_1/layer_normalization_3/batchnorm/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d0419280> with\n",
      "    self: <__main__.FFN object at 0x7f4239882be0>\n",
      "    inputs: Tensor(\"sasrec/transformer_encoder_1/layer_normalization_3/batchnorm/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4239882df0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/layer_normalization_3/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4239882df0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/layer_normalization_3/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4239882df0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4239882df0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c583040>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/ffn_1/conv1d_5/Relu:0' shape=(None, 50, 128) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c583040>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/ffn_1/conv1d_5/Relu:0' shape=(None, 50, 128) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c583040>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c583040>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c583d90>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/ffn_1/conv1d_6/BiasAdd:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c583d90>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/ffn_1/conv1d_6/BiasAdd:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c583d90>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c583d90>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5838b0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5838b0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5838b0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c5838b0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <__main__.TransformerEncoder object at 0x7f421c587100>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.TransformerEncoder object at 0x7f421c587100>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Layer.__call__ of <__main__.TransformerEncoder object at 0x7f421c587100>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Layer.__call__ of <__main__.TransformerEncoder object at 0x7f421c587100>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Allowlisted: <__main__.TransformerEncoder object at 0x7f421c587100>: object __call__ allowed\n",
      "Allowlisted: <__main__.TransformerEncoder object at 0x7f421c587100>: object __call__ allowed\n",
      "INFO:tensorflow:Converted call: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c587100>>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c587100>>\n",
      "    args: ([<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f4239b40600>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f4239b40600>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.TransformerEncoder'>: default rule\n",
      "Not allowed: <class '__main__.TransformerEncoder'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c587100>>: default rule\n",
      "Not allowed: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c587100>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c587100>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c1fd640>\n",
      "Cache hit for <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c587100>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c1fd640>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0280> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0280> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0280> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0280> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0280> with\n",
      "    self: <__main__.TransformerEncoder object at 0x7f421c587100>\n",
      "    inputs: [<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, None]\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0280> with\n",
      "    self: <__main__.TransformerEncoder object at 0x7f421c587100>\n",
      "    inputs: [<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, None]\n",
      "\n",
      "INFO:tensorflow:Converted call: <__main__.MultiHeadAttention object at 0x7f421c5871f0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.MultiHeadAttention object at 0x7f421c5871f0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Layer.__call__ of <__main__.MultiHeadAttention object at 0x7f421c5871f0>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Layer.__call__ of <__main__.MultiHeadAttention object at 0x7f421c5871f0>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Allowlisted: <__main__.MultiHeadAttention object at 0x7f421c5871f0>: object __call__ allowed\n",
      "Allowlisted: <__main__.MultiHeadAttention object at 0x7f421c5871f0>: object __call__ allowed\n",
      "INFO:tensorflow:Converted call: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5871f0>>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5871f0>>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c07bd80>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c07bd80>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.MultiHeadAttention'>: default rule\n",
      "Not allowed: <class '__main__.MultiHeadAttention'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5871f0>>: default rule\n",
      "Not allowed: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5871f0>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5871f0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c12a430>\n",
      "Cache hit for <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5871f0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c12a430>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0430> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0430> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0430> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0430> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0430> with\n",
      "    self: <__main__.MultiHeadAttention object at 0x7f421c5871f0>\n",
      "    q: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_2:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_2:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_2:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0c0430> with\n",
      "    self: <__main__.MultiHeadAttention object at 0x7f421c5871f0>\n",
      "    q: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_2:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_2:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/hierarchical_interest/layer_normalization/add_2:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5872b0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5872b0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5872b0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5872b0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5876a0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5876a0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5876a0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5876a0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5879d0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5879d0>\n",
      "    args: (<tf.Tensor 'sasrec/hierarchical_interest/layer_normalization/add_2:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5879d0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5879d0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_6/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_6/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a45e0> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a45e0> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a45e0> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a45e0> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a45e0> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/dense_6/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a45e0> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/dense_6/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_6/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_6/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Reshape:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Reshape:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_7/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_7/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4820> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4820> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4820> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4820> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4820> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/dense_7/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4820> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/dense_7/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_7/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_7/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Reshape_1:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Reshape_1:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_8/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_8/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4700> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4700> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4700> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4700> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4700> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/dense_8/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f421c0a4700> with\n",
      "    x: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/dense_8/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_8/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/dense_8/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Reshape_2:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Reshape_2:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c0a4940> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c0a4940> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c0a4940> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c0a4940> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c0a4940> with\n",
      "    q: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/transpose:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/transpose_1:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/transpose_2:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f421c0a4940> with\n",
      "    q: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/transpose:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    k: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/transpose_1:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    v: Tensor(\"sasrec/transformer_encoder_2/multi_head_attention_2/transpose_2:0\", shape=(None, 1, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "    mask: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "INFO:tensorflow:Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Cast:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Cast:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/truediv:0' shape=(None, 1, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/truediv:0' shape=(None, 1, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "INFO:tensorflow:Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/truediv:0' shape=(None, 1, 50, 50) dtype=float32>}\n",
      "\n",
      "Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/truediv:0' shape=(None, 1, 50, 50) dtype=float32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Softmax:0' shape=(None, 1, 50, 50) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Softmax:0' shape=(None, 1, 50, 50) dtype=float32>, <tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/MatMul_1:0' shape=(None, 1, 50, 64) dtype=float32>, [0, 2, 1, 3])\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/MatMul_1:0' shape=(None, 1, 50, 64) dtype=float32>, [0, 2, 1, 3])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose_3:0' shape=(None, 50, 1, 64) dtype=float32>, [-1, 50, 64])\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/transpose_3:0' shape=(None, 50, 1, 64) dtype=float32>, [-1, 50, 64])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c58fb20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Reshape_3:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c58fb20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/multi_head_attention_2/Reshape_3:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c58fb20>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c58fb20>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c587e20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c587e20>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c587e20>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c587e20>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <__main__.FFN object at 0x7f421c587be0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/layer_normalization_5/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.FFN object at 0x7f421c587be0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/layer_normalization_5/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Layer.__call__ of <__main__.FFN object at 0x7f421c587be0>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Layer.__call__ of <__main__.FFN object at 0x7f421c587be0>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Allowlisted: <__main__.FFN object at 0x7f421c587be0>: object __call__ allowed\n",
      "Allowlisted: <__main__.FFN object at 0x7f421c587be0>: object __call__ allowed\n",
      "INFO:tensorflow:Converted call: <bound method FFN.call of <__main__.FFN object at 0x7f421c587be0>>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/layer_normalization_5/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method FFN.call of <__main__.FFN object at 0x7f421c587be0>>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/layer_normalization_5/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c102c40>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c102c40>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.FFN'>: default rule\n",
      "Not allowed: <class '__main__.FFN'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method FFN.call of <__main__.FFN object at 0x7f421c587be0>>: default rule\n",
      "Not allowed: <bound method FFN.call of <__main__.FFN object at 0x7f421c587be0>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method FFN.call of <__main__.FFN object at 0x7f421c587be0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c14e490>\n",
      "Cache hit for <bound method FFN.call of <__main__.FFN object at 0x7f421c587be0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c14e490>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0a44c0> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0a44c0> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0a44c0> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0a44c0> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0a44c0> with\n",
      "    self: <__main__.FFN object at 0x7f421c587be0>\n",
      "    inputs: Tensor(\"sasrec/transformer_encoder_2/layer_normalization_5/batchnorm/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f421c0a44c0> with\n",
      "    self: <__main__.FFN object at 0x7f421c587be0>\n",
      "    inputs: Tensor(\"sasrec/transformer_encoder_2/layer_normalization_5/batchnorm/add_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c587df0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/layer_normalization_5/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c587df0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/layer_normalization_5/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c587df0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c587df0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c58f040>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/ffn_2/conv1d_7/Relu:0' shape=(None, 50, 128) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c58f040>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/ffn_2/conv1d_7/Relu:0' shape=(None, 50, 128) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c58f040>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f421c58f040>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c58fd90>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/ffn_2/conv1d_8/BiasAdd:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c58fd90>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/ffn_2/conv1d_8/BiasAdd:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c58fd90>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.core.Dropout object at 0x7f421c58fd90>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c58f8b0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c58f8b0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c58f8b0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f421c58f8b0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function slice at 0x7f425254bee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/layer_normalization_2/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {'begin': [0, 49, 0], 'size': [-1, 1, -1]}\n",
      "\n",
      "Converted call: <function slice at 0x7f425254bee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder/layer_normalization_2/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {'begin': [0, 49, 0], 'size': [-1, 1, -1]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function slice at 0x7f425254bee0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function slice at 0x7f425254bee0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function slice at 0x7f425254bee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/layer_normalization_4/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {'begin': [0, 49, 0], 'size': [-1, 1, -1]}\n",
      "\n",
      "Converted call: <function slice at 0x7f425254bee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_1/layer_normalization_4/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {'begin': [0, 49, 0], 'size': [-1, 1, -1]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function slice at 0x7f425254bee0>: from cache\n",
      "Allowlisted <function slice at 0x7f425254bee0>: from cache\n",
      "INFO:tensorflow:Converted call: <function slice at 0x7f425254bee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/layer_normalization_6/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {'begin': [0, 49, 0], 'size': [-1, 1, -1]}\n",
      "\n",
      "Converted call: <function slice at 0x7f425254bee0>\n",
      "    args: (<tf.Tensor 'sasrec/transformer_encoder_2/layer_normalization_6/batchnorm/add_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {'begin': [0, 49, 0], 'size': [-1, 1, -1]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function slice at 0x7f425254bee0>: from cache\n",
      "Allowlisted <function slice at 0x7f425254bee0>: from cache\n",
      "INFO:tensorflow:Converted call: <function concat at 0x7f4252550820>\n",
      "    args: ([<tf.Tensor 'sasrec/Slice:0' shape=(None, 1, 64) dtype=float32>, <tf.Tensor 'sasrec/Slice_1:0' shape=(None, 1, 64) dtype=float32>, <tf.Tensor 'sasrec/Slice_2:0' shape=(None, 1, 64) dtype=float32>],)\n",
      "    kwargs: {'axis': -2}\n",
      "\n",
      "Converted call: <function concat at 0x7f4252550820>\n",
      "    args: ([<tf.Tensor 'sasrec/Slice:0' shape=(None, 1, 64) dtype=float32>, <tf.Tensor 'sasrec/Slice_1:0' shape=(None, 1, 64) dtype=float32>, <tf.Tensor 'sasrec/Slice_2:0' shape=(None, 1, 64) dtype=float32>],)\n",
      "    kwargs: {'axis': -2}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function concat at 0x7f4252550820>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function concat at 0x7f4252550820>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, [-1])\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, [-1])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>\n",
      "    args: (<tf.Tensor 'sasrec/Reshape:0' shape=(None,) dtype=int64>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>\n",
      "    args: (<tf.Tensor 'sasrec/Reshape:0' shape=(None,) dtype=int64>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function expand_dims_v2 at 0x7f4252549ee0>\n",
      "    args: (<tf.Tensor 'sasrec/embedding/embedding_lookup_1/Identity:0' shape=(None, 64) dtype=float32>,)\n",
      "    kwargs: {'axis': 1}\n",
      "\n",
      "Converted call: <function expand_dims_v2 at 0x7f4252549ee0>\n",
      "    args: (<tf.Tensor 'sasrec/embedding/embedding_lookup_1/Identity:0' shape=(None, 64) dtype=float32>,)\n",
      "    kwargs: {'axis': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function expand_dims_v2 at 0x7f4252549ee0>: from cache\n",
      "Allowlisted <function expand_dims_v2 at 0x7f4252549ee0>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/concat:0' shape=(None, 3, 64) dtype=float32>, [0, 2, 1])\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/concat:0' shape=(None, 3, 64) dtype=float32>, [0, 2, 1])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/ExpandDims_1:0' shape=(None, 1, 64) dtype=float32>, <tf.Tensor 'sasrec/transpose:0' shape=(None, 64, 3) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/ExpandDims_1:0' shape=(None, 1, 64) dtype=float32>, <tf.Tensor 'sasrec/transpose:0' shape=(None, 64, 3) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: (<tf.Tensor 'sasrec/MatMul:0' shape=(None, 1, 3) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: (<tf.Tensor 'sasrec/MatMul:0' shape=(None, 1, 3) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/Softmax:0' shape=(None, 1, 3) dtype=float32>, <tf.Tensor 'sasrec/concat:0' shape=(None, 3, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/Softmax:0' shape=(None, 1, 3) dtype=float32>, <tf.Tensor 'sasrec/concat:0' shape=(None, 3, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function multiply at 0x7f4251f283a0>\n",
      "    args: (<tf.Tensor 'sasrec/MatMul_1:0' shape=(None, 1, 64) dtype=float32>, <tf.Tensor 'sasrec/ExpandDims_1:0' shape=(None, 1, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function multiply at 0x7f4251f283a0>\n",
      "    args: (<tf.Tensor 'sasrec/MatMul_1:0' shape=(None, 1, 64) dtype=float32>, <tf.Tensor 'sasrec/ExpandDims_1:0' shape=(None, 1, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function multiply at 0x7f4251f283a0>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function multiply at 0x7f4251f283a0>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function reduce_sum at 0x7f4251eb9f70>\n",
      "    args: (<tf.Tensor 'sasrec/Mul:0' shape=(None, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'axis': -1}\n",
      "\n",
      "Converted call: <function reduce_sum at 0x7f4251eb9f70>\n",
      "    args: (<tf.Tensor 'sasrec/Mul:0' shape=(None, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'axis': -1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function reduce_sum at 0x7f4251eb9f70>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function reduce_sum at 0x7f4251eb9f70>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>\n",
      "    args: (<tf.Tensor 'cond_2/Identity_1:0' shape=(None, 4) dtype=int64>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>\n",
      "    args: (<tf.Tensor 'cond_2/Identity_1:0' shape=(None, 4) dtype=int64>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/concat:0' shape=(None, 3, 64) dtype=float32>, [0, 2, 1])\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'sasrec/concat:0' shape=(None, 3, 64) dtype=float32>, [0, 2, 1])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/embedding/embedding_lookup_2/Identity:0' shape=(None, 4, 64) dtype=float32>, <tf.Tensor 'sasrec/transpose_1:0' shape=(None, 64, 3) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/embedding/embedding_lookup_2/Identity:0' shape=(None, 4, 64) dtype=float32>, <tf.Tensor 'sasrec/transpose_1:0' shape=(None, 64, 3) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: (<tf.Tensor 'sasrec/MatMul_2:0' shape=(None, 4, 3) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: (<tf.Tensor 'sasrec/MatMul_2:0' shape=(None, 4, 3) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/Softmax_1:0' shape=(None, 4, 3) dtype=float32>, <tf.Tensor 'sasrec/concat:0' shape=(None, 3, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'sasrec/Softmax_1:0' shape=(None, 4, 3) dtype=float32>, <tf.Tensor 'sasrec/concat:0' shape=(None, 3, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function multiply at 0x7f4251f283a0>\n",
      "    args: (<tf.Tensor 'sasrec/MatMul_3:0' shape=(None, 4, 64) dtype=float32>, <tf.Tensor 'sasrec/embedding/embedding_lookup_2/Identity:0' shape=(None, 4, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function multiply at 0x7f4251f283a0>\n",
      "    args: (<tf.Tensor 'sasrec/MatMul_3:0' shape=(None, 4, 64) dtype=float32>, <tf.Tensor 'sasrec/embedding/embedding_lookup_2/Identity:0' shape=(None, 4, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function multiply at 0x7f4251f283a0>: from cache\n",
      "Allowlisted <function multiply at 0x7f4251f283a0>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_sum at 0x7f4251eb9f70>\n",
      "    args: (<tf.Tensor 'sasrec/Mul_1:0' shape=(None, 4, 64) dtype=float32>,)\n",
      "    kwargs: {'axis': -1}\n",
      "\n",
      "Converted call: <function reduce_sum at 0x7f4251eb9f70>\n",
      "    args: (<tf.Tensor 'sasrec/Mul_1:0' shape=(None, 4, 64) dtype=float32>,)\n",
      "    kwargs: {'axis': -1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_sum at 0x7f4251eb9f70>: from cache\n",
      "Allowlisted <function reduce_sum at 0x7f4251eb9f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function tile at 0x7f4252617280>\n",
      "    args: (<tf.Tensor 'sasrec/Sum:0' shape=(None, 1) dtype=float32>, [1, 4])\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function tile at 0x7f4252617280>\n",
      "    args: (<tf.Tensor 'sasrec/Sum:0' shape=(None, 1) dtype=float32>, [1, 4])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function tile at 0x7f4252617280>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function tile at 0x7f4252617280>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function sigmoid at 0x7f4251eaad30>\n",
      "    args: (<tf.Tensor 'sasrec/Tile:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function sigmoid at 0x7f4251eaad30>\n",
      "    args: (<tf.Tensor 'sasrec/Tile:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function sigmoid at 0x7f4251eaad30>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function sigmoid at 0x7f4251eaad30>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function log at 0x7f4252599160>\n",
      "    args: (<tf.Tensor 'sasrec/Sigmoid:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function log at 0x7f4252599160>\n",
      "    args: (<tf.Tensor 'sasrec/Sigmoid:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function log at 0x7f4252599160>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function log at 0x7f4252599160>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function sigmoid at 0x7f4251eaad30>\n",
      "    args: (<tf.Tensor 'sasrec/Sum_1:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function sigmoid at 0x7f4251eaad30>\n",
      "    args: (<tf.Tensor 'sasrec/Sum_1:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function sigmoid at 0x7f4251eaad30>: from cache\n",
      "Allowlisted <function sigmoid at 0x7f4251eaad30>: from cache\n",
      "INFO:tensorflow:Converted call: <function log at 0x7f4252599160>\n",
      "    args: (<tf.Tensor 'sasrec/sub:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function log at 0x7f4252599160>\n",
      "    args: (<tf.Tensor 'sasrec/sub:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function log at 0x7f4252599160>: from cache\n",
      "Allowlisted <function log at 0x7f4252599160>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x7f4251ebb820>\n",
      "    args: (<tf.Tensor 'sasrec/sub_1:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reduce_mean at 0x7f4251ebb820>\n",
      "    args: (<tf.Tensor 'sasrec/sub_1:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function reduce_mean at 0x7f4251ebb820>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <function reduce_mean at 0x7f4251ebb820>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <bound method Layer.add_loss of <__main__.sasrec object at 0x7f4247a02b80>>\n",
      "    args: (<tf.Tensor 'sasrec/truediv:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <bound method Layer.add_loss of <__main__.sasrec object at 0x7f4247a02b80>>\n",
      "    args: (<tf.Tensor 'sasrec/truediv:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Layer.add_loss of <__main__.sasrec object at 0x7f4247a02b80>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Layer.add_loss of <__main__.sasrec object at 0x7f4247a02b80>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function concat at 0x7f4252550820>\n",
      "    args: ([<tf.Tensor 'sasrec/Tile:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'sasrec/Sum_1:0' shape=(None, 4) dtype=float32>],)\n",
      "    kwargs: {'axis': -1}\n",
      "\n",
      "Converted call: <function concat at 0x7f4252550820>\n",
      "    args: ([<tf.Tensor 'sasrec/Tile:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'sasrec/Sum_1:0' shape=(None, 4) dtype=float32>],)\n",
      "    kwargs: {'axis': -1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function concat at 0x7f4252550820>: from cache\n",
      "Allowlisted <function concat at 0x7f4252550820>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.update_state of <tensorflow.python.keras.metrics.Mean object at 0x7f421c594340>>\n",
      "    args: (<tf.Tensor 'AddN:0' shape=() dtype=float32>,)\n",
      "    kwargs: {'sample_weight': None}\n",
      "\n",
      "Converted call: <bound method Reduce.update_state of <tensorflow.python.keras.metrics.Mean object at 0x7f421c594340>>\n",
      "    args: (<tf.Tensor 'AddN:0' shape=() dtype=float32>,)\n",
      "    kwargs: {'sample_weight': None}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Reduce.update_state of <tensorflow.python.keras.metrics.Mean object at 0x7f421c594340>>: DoNotConvert rule for tensorflow\n",
      "Allowlisted: <bound method Reduce.update_state of <tensorflow.python.keras.metrics.Mean object at 0x7f421c594340>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x7f421c3f09d0>\n",
      "    args: (({'click_seq': <tf.Tensor 'cond_3/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'cond_3/Identity_2:0' shape=(None,) dtype=int64>, 'neg_item': <tf.Tensor 'cond_3/Identity_1:0' shape=(None, 4) dtype=int64>},),)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x7f421c3f09d0>\n",
      "    args: (({'click_seq': <tf.Tensor 'cond_3/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'cond_3/Identity_2:0' shape=(None,) dtype=int64>, 'neg_item': <tf.Tensor 'cond_3/Identity_1:0' shape=(None, 4) dtype=int64>},),)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x7f421c3f09d0>: from cache\n",
      "Allowlisted <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x7f421c3f09d0>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>\n",
      "    args: ({'click_seq': <tf.Tensor 'cond_3/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'replica_1/ExpandDims:0' shape=(None, 1) dtype=int64>, 'neg_item': <tf.Tensor 'cond_3/Identity_1:0' shape=(None, 4) dtype=int64>},)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>\n",
      "    args: ({'click_seq': <tf.Tensor 'cond_3/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'replica_1/ExpandDims:0' shape=(None, 1) dtype=int64>, 'neg_item': <tf.Tensor 'cond_3/Identity_1:0' shape=(None, 4) dtype=int64>},)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c2ce7c0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c2ce7c0>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.sasrec'>: default rule\n",
      "Not allowed: <class '__main__.sasrec'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>: default rule\n",
      "Not allowed: <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c16a070>\n",
      "Cache hit for <bound method sasrec.call of <__main__.sasrec object at 0x7f4247a02b80>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c16a070>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d017de50> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d017de50> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d017de50> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d017de50> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d017de50> with\n",
      "    self: <__main__.sasrec object at 0x7f4247a02b80>\n",
      "    inputs: {'click_seq': <tf.Tensor 'cond_3/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'replica_1/ExpandDims:0' shape=(None, 1) dtype=int64>, 'neg_item': <tf.Tensor 'cond_3/Identity_1:0' shape=(None, 4) dtype=int64>}\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d017de50> with\n",
      "    self: <__main__.sasrec object at 0x7f4247a02b80>\n",
      "    inputs: {'click_seq': <tf.Tensor 'cond_3/Identity:0' shape=(None, 50) dtype=int64>, 'pos_item': <tf.Tensor 'replica_1/ExpandDims:0' shape=(None, 1) dtype=int64>, 'neg_item': <tf.Tensor 'cond_3/Identity_1:0' shape=(None, 4) dtype=int64>}\n",
      "\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>\n",
      "    args: (<tf.Tensor 'cond_3/Identity:0' shape=(None, 50) dtype=int64>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>\n",
      "    args: (<tf.Tensor 'cond_3/Identity:0' shape=(None, 50) dtype=int64>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982f2e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function range at 0x7f4251eb99d0>\n",
      "    args: (50,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function range at 0x7f4251eb99d0>\n",
      "    args: (50,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function range at 0x7f4251eb99d0>: from cache\n",
      "Allowlisted <function range at 0x7f4251eb99d0>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982fc10>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/range:0' shape=(50,) dtype=int32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982fc10>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/range:0' shape=(50,) dtype=int32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982fc10>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f423982fc10>: from cache\n",
      "INFO:tensorflow:Converted call: <function expand_dims_v2 at 0x7f4252549ee0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/embedding_1/embedding_lookup/Identity:0' shape=(50, 64) dtype=float32>,)\n",
      "    kwargs: {'axis': 0}\n",
      "\n",
      "Converted call: <function expand_dims_v2 at 0x7f4252549ee0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/embedding_1/embedding_lookup/Identity:0' shape=(50, 64) dtype=float32>,)\n",
      "    kwargs: {'axis': 0}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function expand_dims_v2 at 0x7f4252549ee0>: from cache\n",
      "Allowlisted <function expand_dims_v2 at 0x7f4252549ee0>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f423982ffa0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dropout object at 0x7f423982ffa0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.core.Dropout object at 0x7f423982ffa0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.core.Dropout object at 0x7f423982ffa0>: from cache\n",
      "INFO:tensorflow:Converted call: <__main__.hierarchical_interest object at 0x7f423982f0a0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.hierarchical_interest object at 0x7f423982f0a0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <__main__.hierarchical_interest object at 0x7f423982f0a0>: from cache\n",
      "Allowlisted <__main__.hierarchical_interest object at 0x7f423982f0a0>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c0a5400>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c0a5400>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.hierarchical_interest'>: default rule\n",
      "Not allowed: <class '__main__.hierarchical_interest'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>: default rule\n",
      "Not allowed: <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c3225b0>\n",
      "Cache hit for <bound method hierarchical_interest.call of <__main__.hierarchical_interest object at 0x7f423982f0a0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c3225b0>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d00e8310> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d00e8310> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d00e8310> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d00e8310> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d00e8310> with\n",
      "    self: <__main__.hierarchical_interest object at 0x7f423982f0a0>\n",
      "    x: Tensor(\"replica_1/sasrec/dropout/dropout/Mul_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41d00e8310> with\n",
      "    self: <__main__.hierarchical_interest object at 0x7f423982f0a0>\n",
      "    x: Tensor(\"replica_1/sasrec/dropout/dropout/Mul_1:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f423982fca0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f423982fca0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f423982fca0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f423982fca0>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4230014130>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4230014130>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4230014130>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f4230014130>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f42300145e0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f42300145e0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/dropout/dropout/Mul_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f42300145e0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f42300145e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0310> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0310> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0310> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0310> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0310> with\n",
      "    q: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d_1/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    k: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    v: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    mask: None\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0310> with\n",
      "    q: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d_1/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    k: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    v: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    mask: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "INFO:tensorflow:Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/Cast:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/Cast:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/truediv:0' shape=(None, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/truediv:0' shape=(None, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "INFO:tensorflow:Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'replica_1/sasrec/hierarchical_interest/truediv:0' shape=(None, 50, 50) dtype=float32>}\n",
      "\n",
      "Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'replica_1/sasrec/hierarchical_interest/truediv:0' shape=(None, 50, 50) dtype=float32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/Softmax:0' shape=(None, 50, 50) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/Softmax:0' shape=(None, 50, 50) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0280> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0280> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0280> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0280> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0280> with\n",
      "    q: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d_2/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    k: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    v: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    mask: None\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41d00b0280> with\n",
      "    q: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d_2/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    k: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    v: Tensor(\"replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    mask: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: {'transpose_b': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "Converted call: <function cast at 0x7f4251f2d670>\n",
      "    args: (64,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "Allowlisted <function cast at 0x7f4251f2d670>: from cache\n",
      "INFO:tensorflow:Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/Cast_1:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function sqrt at 0x7f4251e3b5e0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/Cast_1:0' shape=() dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "Allowlisted <function sqrt at 0x7f4251e3b5e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/truediv_1:0' shape=(None, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function ones_like_v2 at 0x7f4252553ee0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/truediv_1:0' shape=(None, 50, 50) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "Allowlisted <function ones_like_v2 at 0x7f4252553ee0>: from cache\n",
      "INFO:tensorflow:Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'replica_1/sasrec/hierarchical_interest/truediv_1:0' shape=(None, 50, 50) dtype=float32>}\n",
      "\n",
      "Converted call: <function softmax_v2 at 0x7f424efc55e0>\n",
      "    args: ()\n",
      "    kwargs: {'logits': <tf.Tensor 'replica_1/sasrec/hierarchical_interest/truediv_1:0' shape=(None, 50, 50) dtype=float32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "Allowlisted <function softmax_v2 at 0x7f424efc55e0>: from cache\n",
      "INFO:tensorflow:Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/Softmax_1:0' shape=(None, 50, 50) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function matmul at 0x7f4251ec8280>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/Softmax_1:0' shape=(None, 50, 50) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "Allowlisted <function matmul at 0x7f4251ec8280>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/conv1d/BiasAdd:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/sub:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/sub:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/sub_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/sub_1:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.normalization.LayerNormalization object at 0x7f423982fdc0>: from cache\n",
      "INFO:tensorflow:Converted call: <__main__.TransformerEncoder object at 0x7f421c5c00d0>\n",
      "    args: ([<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.TransformerEncoder object at 0x7f421c5c00d0>\n",
      "    args: ([<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <__main__.TransformerEncoder object at 0x7f421c5c00d0>: from cache\n",
      "Allowlisted <__main__.TransformerEncoder object at 0x7f421c5c00d0>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>\n",
      "    args: ([<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>\n",
      "    args: ([<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None],)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c2fc440>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c2fc440>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.TransformerEncoder'>: default rule\n",
      "Not allowed: <class '__main__.TransformerEncoder'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>: default rule\n",
      "Not allowed: <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c1fd640>\n",
      "Cache hit for <bound method TransformerEncoder.call of <__main__.TransformerEncoder object at 0x7f421c5c00d0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c1fd640>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba550> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba550> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba550> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba550> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba550> with\n",
      "    self: <__main__.TransformerEncoder object at 0x7f421c5c00d0>\n",
      "    inputs: [<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None]\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba550> with\n",
      "    self: <__main__.TransformerEncoder object at 0x7f421c5c00d0>\n",
      "    inputs: [<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None]\n",
      "\n",
      "INFO:tensorflow:Converted call: <__main__.MultiHeadAttention object at 0x7f421c5c01f0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <__main__.MultiHeadAttention object at 0x7f421c5c01f0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <__main__.MultiHeadAttention object at 0x7f421c5c01f0>: from cache\n",
      "Allowlisted <__main__.MultiHeadAttention object at 0x7f421c5c01f0>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: {}\n",
      "\n",
      "Converted call: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>, None)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f421c370f80>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of method object at 0x7f421c370f80>: default rule\n",
      "INFO:tensorflow:Not allowed: <class '__main__.MultiHeadAttention'>: default rule\n",
      "Not allowed: <class '__main__.MultiHeadAttention'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>: default rule\n",
      "Not allowed: <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c12a430>\n",
      "Cache hit for <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f421c5c01f0>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c12a430>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba310> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba310> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba310> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba310> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba310> with\n",
      "    self: <__main__.MultiHeadAttention object at 0x7f421c5c01f0>\n",
      "    q: Tensor(\"replica_1/sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    k: Tensor(\"replica_1/sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    v: Tensor(\"replica_1/sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    mask: None\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x7f41b07ba310> with\n",
      "    self: <__main__.MultiHeadAttention object at 0x7f421c5c01f0>\n",
      "    q: Tensor(\"replica_1/sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    k: Tensor(\"replica_1/sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    v: Tensor(\"replica_1/sasrec/hierarchical_interest/layer_normalization/add:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    mask: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c02b0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c02b0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c02b0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c02b0>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c06a0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c06a0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c06a0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c06a0>: from cache\n",
      "INFO:tensorflow:Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c09d0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c09d0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/hierarchical_interest/layer_normalization/add:0' shape=(None, 50, 64) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c09d0>: from cache\n",
      "Allowlisted <tensorflow.python.keras.layers.core.Dense object at 0x7f421c5c09d0>: from cache\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8160> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8160> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8160> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8160> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8160> with\n",
      "    x: Tensor(\"replica_1/sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8160> with\n",
      "    x: Tensor(\"replica_1/sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/Reshape:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/Reshape:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8280> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8280> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8280> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8280> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8280> with\n",
      "    x: Tensor(\"replica_1/sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e8280> with\n",
      "    x: Tensor(\"replica_1/sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense_1/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/Reshape_1:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/Reshape_1:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function split_heads at 0x7f4239b113a0>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, 50, 1, 64)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "Not allowed: <function split_heads at 0x7f4239b113a0>: default rule\n",
      "INFO:tensorflow:Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "Cache hit for <function split_heads at 0x7f4239b113a0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c067220>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e81f0> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e81f0> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e81f0> : None\n",
      "KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e81f0> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e81f0> with\n",
      "    x: Tensor(\"replica_1/sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__split_heads at 0x7f41b07e81f0> with\n",
      "    x: Tensor(\"replica_1/sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0\", shape=(None, 50, 64), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "    seq_len: 50\n",
      "    num_heads: 1\n",
      "    depth: 64\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function reshape at 0x7f4252549820>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/dense_2/BiasAdd:0' shape=(None, 50, 64) dtype=float32>, (-1, 50, 1, 64))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "Allowlisted <function reshape at 0x7f4252549820>: from cache\n",
      "INFO:tensorflow:Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/Reshape_2:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "Converted call: <function transpose_v2 at 0x7f4252550f70>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/Reshape_2:0' shape=(None, 50, 1, 64) dtype=float32>,)\n",
      "    kwargs: {'perm': [0, 2, 1, 3]}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "Allowlisted <function transpose_v2 at 0x7f4252550f70>: from cache\n",
      "INFO:tensorflow:Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "Converted call: <function scaled_dot_product_attention at 0x7f4239b11310>\n",
      "    args: (<tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/transpose:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/transpose_1:0' shape=(None, 1, 50, 64) dtype=float32>, <tf.Tensor 'replica_1/sasrec/transformer_encoder/multi_head_attention/transpose_2:0' shape=(None, 1, 50, 64) dtype=float32>, None)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "Not allowed: <method-wrapper '__call__' of function object at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "Not allowed: <function scaled_dot_product_attention at 0x7f4239b11310>: default rule\n",
      "INFO:tensorflow:Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "Cache hit for <function scaled_dot_product_attention at 0x7f4239b11310> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7f421c059520>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41b07e83a0> : None\n",
      "Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41b07e83a0> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__scaled_dot_product_attention at 0x7f41b07e83a0> : None\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = sasrec()\n",
    "    #model.summary()\n",
    "    optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    model.compile(optimizer=optimizer)\n",
    "\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    t1 = time()\n",
    "    model.fit(\n",
    "            x=train_data,\n",
    "            epochs=1,\n",
    "            batch_size=batch_size)\n",
    "    t2 = time()\n",
    "    eval_dict = eval_pos_neg(model, test_data, ['hr', 'mrr', 'ndcg'], 10, batch_size)\n",
    "    print('Iteration %d Fit [%.1f s], Evaluate [%.1f s]: HR = %.4f, MRR = %.4f, NDCG = %.4f'\n",
    "              % (epoch, t2 - t1, time() - t2, eval_dict['hr'], eval_dict['mrr'], eval_dict['ndcg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e033e-5ee2-4932-b9a9-4787df1733ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39a04b-df32-46da-93f0-7f2749da22d5",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1666147143886,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "ab39a04b-df32-46da-93f0-7f2749da22d5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0933a7-b926-4371-a008-0e992c04d096",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d0933a7-b926-4371-a008-0e992c04d096",
    "outputId": "b91891a5-5d41-462b-fa17-174a05dafd46"
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1, epoch_num + 1):\n",
    "    t1 = time()\n",
    "    model.fit(\n",
    "            x=train_data,\n",
    "            epochs=1,\n",
    "            batch_size=batch_size)\n",
    "    t2 = time()\n",
    "    eval_dict = eval_pos_neg(model, test_data, ['hr', 'mrr', 'ndcg'], 10, batch_size)\n",
    "    print('Iteration %d Fit [%.1f s], Evaluate [%.1f s]: HR = %.4f, MRR = %.4f, NDCG = %.4f'\n",
    "              % (epoch, t2 - t1, time() - t2, eval_dict['hr'], eval_dict['mrr'], eval_dict['ndcg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9725cf-0012-418f-bd02-be06cc7ab1ad",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "aborted",
     "timestamp": 1666147060876,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "1a9725cf-0012-418f-bd02-be06cc7ab1ad"
   },
   "outputs": [],
   "source": [
    "a=tf.constant([[2558, 2559, 2582, 2605, 2608, 2669, 2801, 2835, 2913, 3052]])\n",
    "tf.print(tf.argmax(model(a)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79481e70-93d3-41d1-8e5b-558125f5da11",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "aborted",
     "timestamp": 1666147060876,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "79481e70-93d3-41d1-8e5b-558125f5da11"
   },
   "outputs": [],
   "source": [
    "'''def train_model(model,ds_train,ds_valid,epoches):\n",
    "\n",
    "    for epoch in tf.range(1,epoches+1):\n",
    "        model.reset_metrics()\n",
    "        \n",
    "        # 在后期降低学习率\n",
    "        if epoch == 5000:\n",
    "            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n",
    "            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n",
    "        \n",
    "        for x, y in ds_train:\n",
    "            train_result = model.train_on_batch(x, y)\n",
    "\n",
    "        for x, y in ds_valid:\n",
    "            valid_result = model.test_on_batch(x, y,reset_metrics=False)\n",
    "            \n",
    "        if epoch%100 ==0:\n",
    "            tf.print(\"epoch = \",epoch)\n",
    "            print(\"train:\",dict(zip(model.metrics_names,train_result)))\n",
    "            print(\"valid:\",dict(zip(model.metrics_names,valid_result)))\n",
    "            print(\"\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a286ada-d7d0-4faf-a82c-43c935a51ff9",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "aborted",
     "timestamp": 1666147060876,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "6a286ada-d7d0-4faf-a82c-43c935a51ff9"
   },
   "outputs": [],
   "source": [
    "'''train_model(model,ds_train,ds_test,800)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e66217-f349-4dc4-85fe-a8d250b20e79",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "aborted",
     "timestamp": 1666147060877,
     "user": {
      "displayName": "yijun sheng",
      "userId": "07090048668815296857"
     },
     "user_tz": -480
    },
    "id": "f8e66217-f349-4dc4-85fe-a8d250b20e79"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "items_num=3416\n",
    "maxlen=100\n",
    "len_seq=50\n",
    "batch_size=512\n",
    "epoch_num=20\n",
    "hidden_size=64\n",
    "keep_rate=0.9\n",
    "layers_num=2\n",
    "num_interest=1\n",
    "neg_num=4\n",
    "test_neg_num=100\n",
    "567/567 [==============================] - 32s 36ms/step - loss: 0.3581\n",
    "Iteration 1 Fit [32.5 s], Evaluate [1.5 s]: HR = 0.1518, MRR = 0.0580, NDCG = 0.0800\n",
    "567/567 [==============================] - 21s 36ms/step - loss: 0.1707\n",
    "Iteration 2 Fit [20.9 s], Evaluate [0.4 s]: HR = 0.1877, MRR = 0.0754, NDCG = 0.1018\n",
    "567/567 [==============================] - 20s 35ms/step - loss: 0.1296\n",
    "Iteration 3 Fit [20.2 s], Evaluate [0.3 s]: HR = 0.2270, MRR = 0.0938, NDCG = 0.1251\n",
    "567/567 [==============================] - 17s 31ms/step - loss: 0.1101\n",
    "Iteration 4 Fit [17.6 s], Evaluate [0.4 s]: HR = 0.2409, MRR = 0.0962, NDCG = 0.1303\n",
    "567/567 [==============================] - 19s 33ms/step - loss: 0.0978\n",
    "Iteration 5 Fit [18.9 s], Evaluate [0.3 s]: HR = 0.2594, MRR = 0.1063, NDCG = 0.1424\n",
    "567/567 [==============================] - 17s 31ms/step - loss: 0.0889\n",
    "Iteration 6 Fit [17.6 s], Evaluate [0.3 s]: HR = 0.2757, MRR = 0.1099, NDCG = 0.1490\n",
    "567/567 [==============================] - 20s 36ms/step - loss: 0.0813\n",
    "Iteration 7 Fit [20.6 s], Evaluate [0.3 s]: HR = 0.2846, MRR = 0.1158, NDCG = 0.1557\n",
    "567/567 [==============================] - 18s 31ms/step - loss: 0.0754\n",
    "Iteration 8 Fit [17.8 s], Evaluate [0.3 s]: HR = 0.2791, MRR = 0.1131, NDCG = 0.1523\n",
    "567/567 [==============================] - 17s 30ms/step - loss: 0.0704\n",
    "Iteration 9 Fit [17.3 s], Evaluate [0.3 s]: HR = 0.2899, MRR = 0.1168, NDCG = 0.1578\n",
    "567/567 [==============================] - 17s 30ms/step - loss: 0.0658\n",
    "Iteration 10 Fit [17.1 s], Evaluate [0.3 s]: HR = 0.2937, MRR = 0.1186, NDCG = 0.1600\n",
    "567/567 [==============================] - 17s 30ms/step - loss: 0.0621\n",
    "Iteration 11 Fit [17.3 s], Evaluate [0.3 s]: HR = 0.2874, MRR = 0.1159, NDCG = 0.1565\n",
    "567/567 [==============================] - 17s 30ms/step - loss: 0.0585\n",
    "Iteration 12 Fit [17.4 s], Evaluate [0.3 s]: HR = 0.2896, MRR = 0.1153, NDCG = 0.1565\n",
    "567/567 [==============================] - 21s 38ms/step - loss: 0.0556\n",
    "Iteration 13 Fit [21.6 s], Evaluate [0.4 s]: HR = 0.2851, MRR = 0.1175, NDCG = 0.1572\n",
    "567/567 [==============================] - 22s 39ms/step - loss: 0.0529\n",
    "Iteration 14 Fit [22.6 s], Evaluate [0.3 s]: HR = 0.2831, MRR = 0.1156, NDCG = 0.1553\n",
    "567/567 [==============================] - 19s 34ms/step - loss: 0.0503\n",
    "Iteration 15 Fit [19.4 s], Evaluate [0.3 s]: HR = 0.2841, MRR = 0.1139, NDCG = 0.1542\n",
    "567/567 [==============================] - 19s 33ms/step - loss: 0.0478\n",
    "Iteration 16 Fit [18.9 s], Evaluate [0.3 s]: HR = 0.2834, MRR = 0.1141, NDCG = 0.1541\n",
    "567/567 [==============================] - 20s 35ms/step - loss: 0.0462\n",
    "Iteration 17 Fit [19.8 s], Evaluate [0.3 s]: HR = 0.2765, MRR = 0.1114, NDCG = 0.1504\n",
    "567/567 [==============================] - 21s 37ms/step - loss: 0.0441\n",
    "Iteration 18 Fit [21.2 s], Evaluate [0.4 s]: HR = 0.2858, MRR = 0.1165, NDCG = 0.1566\n",
    "567/567 [==============================] - 19s 33ms/step - loss: 0.0424\n",
    "Iteration 19 Fit [18.8 s], Evaluate [0.3 s]: HR = 0.2879, MRR = 0.1180, NDCG = 0.1582\n",
    "567/567 [==============================] - 17s 30ms/step - loss: 0.0406\n",
    "Iteration 20 Fit [17.2 s], Evaluate [0.3 s]: HR = 0.2902, MRR = 0.1177, NDCG = 0.1584\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
